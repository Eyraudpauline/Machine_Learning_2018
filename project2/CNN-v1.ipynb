{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little help from\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "and my solutions from the class \n",
    "https://edu.epfl.ch/coursebook/en/data-and-artificial-intelligence-for-transportation-CIVIL-459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "\n",
    "path_embeddings = 'pretrained_glove/embeddings200_pretrained_reduced.npy'\n",
    "path_vocab = 'pretrained_glove/vocab_pretrained_reduced.pkl'\n",
    "path_train_pos = 'pos_train.txt'\n",
    "path_train_neg = 'neg_train.txt'\n",
    "path_test = 'test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(np.zeros((1, embeddings.shape[1])), embeddings, axis=0)\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet has 64 words\n"
     ]
    }
   ],
   "source": [
    "# find maximal tweet length (number of words)\n",
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length\n",
    "            \n",
    "print(\"Longest tweet has {:d} words\".format(longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = np.zeros((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        y.append(1)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "        \n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = np.zeros((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        y.append(0)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = np.zeros((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "x_submission = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_channels = 16\n",
    "        filter_size = 3\n",
    "        \n",
    "        embedding_dim = embeddings.shape[1]        \n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings)\n",
    "        self.conv = torch.nn.Conv2d(1, n_channels, kernel_size=(filter_size, embedding_dim))\n",
    "        self.fc = nn.Linear(n_channels, 1)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x = self.embeddings(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze(3)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, x.shape[2])\n",
    "        x = x.squeeze(2)\n",
    "        x = F.dropout(x, 0)\n",
    "        x = self.fc(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x.squeeze(1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pred = torch.round(self.forward(x))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20\t Loss 0.7001\t Test accuracy 0.5191\n",
      "Epoch 1 / 20\t Loss 0.6862\t Test accuracy 0.5280\n",
      "Epoch 1 / 20\t Loss 0.6777\t Test accuracy 0.5600\n",
      "Epoch 1 / 20\t Loss 0.6685\t Test accuracy 0.6101\n",
      "Epoch 1 / 20\t Loss 0.6613\t Test accuracy 0.6469\n",
      "Epoch 1 / 20\t Loss 0.6519\t Test accuracy 0.6764\n",
      "Epoch 1 / 20\t Loss 0.6476\t Test accuracy 0.6703\n",
      "Epoch 1 / 20\t Loss 0.6390\t Test accuracy 0.7190\n",
      "Epoch 1 / 20\t Loss 0.6473\t Test accuracy 0.7025\n",
      "Epoch 1 / 20\t Loss 0.6384\t Test accuracy 0.7303\n",
      "Epoch 1 / 20\t Loss 0.6284\t Test accuracy 0.7343\n",
      "Epoch 1 / 20\t Loss 0.6323\t Test accuracy 0.7188\n",
      "Epoch 1 / 20\t Loss 0.6340\t Test accuracy 0.7371\n",
      "Epoch 1 / 20\t Loss 0.6327\t Test accuracy 0.7527\n",
      "Epoch 1 / 20\t Loss 0.6309\t Test accuracy 0.7490\n",
      "Epoch 1 / 20\t Loss 0.6068\t Test accuracy 0.7666\n",
      "Epoch 1 / 20\t Loss 0.6166\t Test accuracy 0.7433\n",
      "Epoch 1 / 20\t Loss 0.6288\t Test accuracy 0.7447\n",
      "Epoch 1 / 20\t Loss 0.6212\t Test accuracy 0.7608\n",
      "Epoch 1 / 20\t Loss 0.6162\t Test accuracy 0.7572\n",
      "Epoch 1 / 20\t Loss 0.6141\t Test accuracy 0.7652\n",
      "Epoch 1 / 20\t Loss 0.6284\t Test accuracy 0.7564\n",
      "Epoch 1 / 20\t Loss 0.6141\t Test accuracy 0.7717\n",
      "Epoch 1 / 20\t Loss 0.6058\t Test accuracy 0.7867\n",
      "Epoch 1 / 20\t Loss 0.6054\t Test accuracy 0.7685\n",
      "Epoch 1 / 20\t Loss 0.6145\t Test accuracy 0.7846\n",
      "Epoch 1 / 20\t Loss 0.6203\t Test accuracy 0.7680\n",
      "Epoch 1 / 20\t Loss 0.6038\t Test accuracy 0.7979\n",
      "Epoch 1 / 20\t Loss 0.6041\t Test accuracy 0.7869\n",
      "Epoch 1 / 20\t Loss 0.6061\t Test accuracy 0.7906\n",
      "Epoch 1 / 20\t Loss 0.5960\t Test accuracy 0.7970\n",
      "Epoch 1 / 20\t Loss 0.6174\t Test accuracy 0.7767\n",
      "Epoch 1 / 20\t Loss 0.6016\t Test accuracy 0.7844\n",
      "Epoch 1 / 20\t Loss 0.6010\t Test accuracy 0.8002\n",
      "Epoch 1 / 20\t Loss 0.6002\t Test accuracy 0.7946\n",
      "Epoch 1 / 20\t Loss 0.6028\t Test accuracy 0.7880\n",
      "Epoch 1 / 20\t Loss 0.6017\t Test accuracy 0.7950\n",
      "Epoch 1 / 20\t Loss 0.5952\t Test accuracy 0.7949\n",
      "Epoch 1 / 20\t Loss 0.6064\t Test accuracy 0.7999\n",
      "Epoch 1 / 20\t Loss 0.5932\t Test accuracy 0.7973\n",
      "Epoch 1 / 20\t Loss 0.6035\t Test accuracy 0.7956\n",
      "Epoch 1 / 20\t Loss 0.6020\t Test accuracy 0.7926\n",
      "Epoch 1 / 20\t Loss 0.6152\t Test accuracy 0.7681\n",
      "Epoch 1 / 20\t Loss 0.5949\t Test accuracy 0.7987\n",
      "Epoch 1 / 20\t Loss 0.5976\t Test accuracy 0.8011\n",
      "Epoch 1 / 20\t Loss 0.5894\t Test accuracy 0.8033\n",
      "Epoch 1 / 20\t Loss 0.5963\t Test accuracy 0.7982\n",
      "Epoch 1 / 20\t Loss 0.5990\t Test accuracy 0.8004\n",
      "Epoch 1 / 20\t Loss 0.5873\t Test accuracy 0.8069\n",
      "Epoch 1 / 20\t Loss 0.5982\t Test accuracy 0.7980\n",
      "Epoch 1 / 20\t Loss 0.5981\t Test accuracy 0.8080\n",
      "Epoch 1 / 20\t Loss 0.6002\t Test accuracy 0.8079\n",
      "Epoch 1 / 20\t Loss 0.5975\t Test accuracy 0.7962\n",
      "Epoch 1 / 20\t Loss 0.5882\t Test accuracy 0.8082\n",
      "Epoch 1 / 20\t Loss 0.5923\t Test accuracy 0.8019\n",
      "Epoch 1 / 20\t Loss 0.6017\t Test accuracy 0.8049\n",
      "Epoch 1 / 20\t Loss 0.6088\t Test accuracy 0.8024\n",
      "Epoch 1 / 20\t Loss 0.5996\t Test accuracy 0.8052\n",
      "Epoch 1 / 20\t Loss 0.5966\t Test accuracy 0.8052\n",
      "Epoch 1 / 20\t Loss 0.6029\t Test accuracy 0.8079\n",
      "Epoch 1 / 20\t Loss 0.6100\t Test accuracy 0.7920\n",
      "Epoch 1 / 20\t Loss 0.5965\t Test accuracy 0.8056\n",
      "Epoch 1 / 20\t Loss 0.6011\t Test accuracy 0.7951\n",
      "Epoch 1 / 20\t Loss 0.5853\t Test accuracy 0.8083\n",
      "Epoch 1 / 20\t Loss 0.5846\t Test accuracy 0.8057\n",
      "Epoch 1 / 20\t Loss 0.5953\t Test accuracy 0.7984\n",
      "Epoch 1 / 20\t Loss 0.6073\t Test accuracy 0.7993\n",
      "Epoch 1 / 20\t Loss 0.5952\t Test accuracy 0.8075\n",
      "Epoch 1 / 20\t Loss 0.5987\t Test accuracy 0.7934\n",
      "Epoch 1 / 20\t Loss 0.5936\t Test accuracy 0.8106\n",
      "Epoch 1 / 20\t Loss 0.5892\t Test accuracy 0.8109\n",
      "Epoch 1 / 20\t Loss 0.5962\t Test accuracy 0.8065\n",
      "Epoch 1 / 20\t Loss 0.5927\t Test accuracy 0.8112\n",
      "Epoch 1 / 20\t Loss 0.5964\t Test accuracy 0.7999\n",
      "Epoch 1 / 20\t Loss 0.5899\t Test accuracy 0.8140\n",
      "Epoch 1 / 20\t Loss 0.5999\t Test accuracy 0.8025\n",
      "Epoch 1 / 20\t Loss 0.5968\t Test accuracy 0.8072\n",
      "Epoch 1 / 20\t Loss 0.6145\t Test accuracy 0.7897\n",
      "Epoch 1 / 20\t Loss 0.5936\t Test accuracy 0.8110\n",
      "Epoch 1 / 20\t Loss 0.5973\t Test accuracy 0.8016\n",
      "Epoch 1 / 20\t Loss 0.6002\t Test accuracy 0.8078\n",
      "Epoch 1 / 20\t Loss 0.5979\t Test accuracy 0.8078\n",
      "Epoch 1 / 20\t Loss 0.6005\t Test accuracy 0.8117\n",
      "Epoch 1 / 20\t Loss 0.5935\t Test accuracy 0.8105\n",
      "Epoch 1 / 20\t Loss 0.6007\t Test accuracy 0.7971\n",
      "Epoch 1 / 20\t Loss 0.5964\t Test accuracy 0.8095\n",
      "Epoch 1 / 20\t Loss 0.5941\t Test accuracy 0.8108\n",
      "Epoch 1 / 20\t Loss 0.6022\t Test accuracy 0.7999\n",
      "Epoch 1 / 20\t Loss 0.5841\t Test accuracy 0.8143\n",
      "Epoch 1 / 20\t Loss 0.5910\t Test accuracy 0.8069\n",
      "Epoch 1 / 20\t Loss 0.5977\t Test accuracy 0.8154\n",
      "Epoch 1 / 20\t Loss 0.5911\t Test accuracy 0.8127\n",
      "Epoch 1 / 20\t Loss 0.5823\t Test accuracy 0.8112\n",
      "Epoch 1 / 20\t Loss 0.5915\t Test accuracy 0.8068\n",
      "Epoch 1 / 20\t Loss 0.5991\t Test accuracy 0.8078\n",
      "Epoch 1 / 20\t Loss 0.5957\t Test accuracy 0.8060\n",
      "Epoch 1 / 20\t Loss 0.5922\t Test accuracy 0.8180\n",
      "Epoch 1 / 20\t Loss 0.5892\t Test accuracy 0.8116\n",
      "Epoch 1 / 20\t Loss 0.5822\t Test accuracy 0.8159\n",
      "Epoch 1 / 20\t Loss 0.5940\t Test accuracy 0.8151\n",
      "Epoch 1 / 20\t Loss 0.5977\t Test accuracy 0.8161\n",
      "Epoch 1 / 20\t Loss 0.6092\t Test accuracy 0.8025\n",
      "Epoch 1 / 20\t Loss 0.5971\t Test accuracy 0.8121\n",
      "Epoch 1 / 20\t Loss 0.5906\t Test accuracy 0.8158\n",
      "Epoch 1 / 20\t Loss 0.5895\t Test accuracy 0.8117\n",
      "Epoch 1 / 20\t Loss 0.5991\t Test accuracy 0.8126\n",
      "Epoch 1 / 20\t Loss 0.6005\t Test accuracy 0.8071\n",
      "Epoch 1 / 20\t Loss 0.6023\t Test accuracy 0.8097\n",
      "Epoch 1 / 20\t Loss 0.5894\t Test accuracy 0.8136\n",
      "Epoch 1 / 20\t Loss 0.5931\t Test accuracy 0.8131\n",
      "Epoch 1 / 20\t Loss 0.5949\t Test accuracy 0.8130\n",
      "Epoch 1 / 20\t Loss 0.5918\t Test accuracy 0.8199\n",
      "Epoch 1 / 20\t Loss 0.6008\t Test accuracy 0.8037\n",
      "Epoch 1 / 20\t Loss 0.5994\t Test accuracy 0.8060\n",
      "Epoch 1 / 20\t Loss 0.6043\t Test accuracy 0.8128\n",
      "Epoch 1 / 20\t Loss 0.5920\t Test accuracy 0.8090\n",
      "Epoch 1 / 20\t Loss 0.5846\t Test accuracy 0.8104\n",
      "Epoch 1 / 20\t Loss 0.5853\t Test accuracy 0.8199\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-01e7293bd4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-32b854509aae>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-32b854509aae>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = ConvNet(torch.from_numpy(embeddings).float())\n",
    "valset = 10000\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "print_every = 20\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "x_val_torch = torch.from_numpy(x_train[0:valset, :])\n",
    "y_val_torch = torch.from_numpy(y_train[0:valset]).float()\n",
    "x_train_torch = torch.from_numpy(x_train[valset + 1:, :])\n",
    "y_train_torch = torch.from_numpy(y_train[valset + 1:]).float()\n",
    "\n",
    "train_set = utils.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = utils.DataLoader(train_set, batch_size, shuffle=False)\n",
    "\n",
    "val_set = utils.TensorDataset(x_val_torch, y_val_torch)\n",
    "val_loader = utils.DataLoader(val_set, batch_size, shuffle=False)\n",
    "\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    for tweets, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        inputs = Variable(tweets)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            accuracy = 0\n",
    "            for n, (tweets, labels) in enumerate(val_loader):\n",
    "                predictions = net.predict(tweets)\n",
    "                accuracy += sum(predictions.data.numpy() == labels.data.numpy()) / labels.data.numpy().size\n",
    "            \n",
    "            print(\"Epoch {} / {}\\t\".format(e+1, epochs),\n",
    "                  \"Loss {:.4f}\\t\".format(running_loss / print_every),\n",
    "                  \"Test accuracy {:.4f}\".format(accuracy / n))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
