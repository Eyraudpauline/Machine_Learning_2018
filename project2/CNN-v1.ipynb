{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "'''\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "\n",
    "path_embeddings = 'pretrained_glove/embeddings200_pretrained_reduced.npy'\n",
    "path_vocab = 'pretrained_glove/vocab_pretrained_reduced.pkl'\n",
    "path_train_pos = 'pos_train.txt'\n",
    "path_train_neg = 'neg_train.txt'\n",
    "path_test = 'test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(embeddings, np.zeros((1, embeddings.shape[1])), axis=0)\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet has 64 words\n"
     ]
    }
   ],
   "source": [
    "# find maximal tweet length (number of words)\n",
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length\n",
    "            \n",
    "print(\"Longest tweet has {:d} words\".format(longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = embeddings.shape[0] * np.ones((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        y.append(np.asarray([1, 0]).astype(int))\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "        \n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = embeddings.shape[0] * np.ones((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        y.append(np.asarray([0, 1]).astype(int))\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = embeddings.shape[0] * np.ones((longest)).astype(int)\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "x_submission = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_input, embeddings):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.embeddings = torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t 1 \tloss:  80.41898758565523\n",
      "epoch\t 1 \tloss:  72.63990867432156\n",
      "epoch\t 1 \tloss:  72.27247042275462\n",
      "epoch\t 2 \tloss:  71.36846290841906\n",
      "epoch\t 2 \tloss:  70.9661952624341\n",
      "epoch\t 2 \tloss:  70.56315594898248\n",
      "epoch\t 3 \tloss:  70.12605256449098\n",
      "epoch\t 3 \tloss:  69.6703350456351\n",
      "epoch\t 3 \tloss:  69.63211465423336\n",
      "epoch\t 4 \tloss:  69.31261762711519\n",
      "epoch\t 4 \tloss:  69.08028647668529\n",
      "epoch\t 4 \tloss:  68.99231228631497\n",
      "epoch\t 5 \tloss:  68.82432824619028\n",
      "epoch\t 5 \tloss:  68.72600853653859\n",
      "epoch\t 5 \tloss:  68.63533220042841\n",
      "epoch\t 6 \tloss:  68.48845100318616\n",
      "epoch\t 6 \tloss:  68.6223296969754\n",
      "epoch\t 6 \tloss:  68.3797046782826\n",
      "epoch\t 7 \tloss:  68.36875349583184\n",
      "epoch\t 7 \tloss:  68.43275071749726\n",
      "epoch\t 7 \tloss:  68.34038315851258\n",
      "epoch\t 8 \tloss:  68.28139345077676\n",
      "epoch\t 8 \tloss:  68.31877622957936\n",
      "epoch\t 8 \tloss:  68.2911010593855\n",
      "epoch\t 9 \tloss:  68.20385412113002\n",
      "epoch\t 9 \tloss:  68.29297923386456\n",
      "epoch\t 9 \tloss:  68.1807471177709\n",
      "epoch\t 10 \tloss:  68.21568128259491\n",
      "epoch\t 10 \tloss:  68.21918128948933\n",
      "epoch\t 10 \tloss:  68.24327442396574\n",
      "epoch\t 11 \tloss:  68.23751127051482\n",
      "epoch\t 11 \tloss:  68.16189244741668\n",
      "epoch\t 11 \tloss:  68.1983487821522\n",
      "epoch\t 12 \tloss:  68.15952697561742\n",
      "epoch\t 12 \tloss:  68.20030921085288\n",
      "epoch\t 12 \tloss:  68.22363725183929\n",
      "epoch\t 13 \tloss:  68.25122515774294\n",
      "epoch\t 13 \tloss:  68.22583751039522\n",
      "epoch\t 13 \tloss:  68.07140657641973\n",
      "epoch\t 14 \tloss:  68.137390902048\n",
      "epoch\t 14 \tloss:  68.1582744055859\n",
      "epoch\t 14 \tloss:  68.22744948836093\n",
      "epoch\t 15 \tloss:  68.16237514937316\n",
      "epoch\t 15 \tloss:  68.15168948868012\n",
      "epoch\t 15 \tloss:  68.11765218752386\n",
      "epoch\t 16 \tloss:  68.12336168813367\n",
      "epoch\t 16 \tloss:  68.17683856603573\n",
      "epoch\t 16 \tloss:  68.23398117965237\n",
      "epoch\t 17 \tloss:  68.15547058776582\n",
      "epoch\t 17 \tloss:  68.14625153142786\n",
      "epoch\t 17 \tloss:  68.21438835205915\n",
      "epoch\t 18 \tloss:  68.1538714253665\n",
      "epoch\t 18 \tloss:  68.09742617043044\n",
      "epoch\t 18 \tloss:  68.22913101353042\n",
      "epoch\t 19 \tloss:  68.09862305303128\n",
      "epoch\t 19 \tloss:  68.21515986422699\n",
      "epoch\t 19 \tloss:  68.24538163636637\n",
      "epoch\t 20 \tloss:  68.0909674321416\n",
      "epoch\t 20 \tloss:  68.22899856885005\n",
      "epoch\t 20 \tloss:  68.13192736111833\n",
      "epoch\t 21 \tloss:  68.25164487790256\n",
      "epoch\t 21 \tloss:  68.22185447697912\n",
      "epoch\t 21 \tloss:  68.07064997532375\n",
      "epoch\t 22 \tloss:  68.16037886825697\n",
      "epoch\t 22 \tloss:  68.34043400419313\n",
      "epoch\t 22 \tloss:  68.10176835914446\n",
      "epoch\t 23 \tloss:  68.09789688195782\n",
      "epoch\t 23 \tloss:  68.18623823631945\n",
      "epoch\t 23 \tloss:  68.2028874166614\n",
      "epoch\t 24 \tloss:  68.213828231813\n",
      "epoch\t 24 \tloss:  68.08084772209298\n",
      "epoch\t 24 \tloss:  68.18491689022444\n",
      "epoch\t 25 \tloss:  68.08582083044699\n",
      "epoch\t 25 \tloss:  68.23194544131522\n",
      "epoch\t 25 \tloss:  68.16049678788171\n",
      "epoch\t 26 \tloss:  68.09551863769421\n",
      "epoch\t 26 \tloss:  68.29612225148976\n",
      "epoch\t 26 \tloss:  68.127272569203\n",
      "epoch\t 27 \tloss:  68.1865122092055\n",
      "epoch\t 27 \tloss:  68.19099833415116\n",
      "epoch\t 27 \tloss:  68.13385638599885\n",
      "epoch\t 28 \tloss:  68.13417394054848\n",
      "epoch\t 28 \tloss:  68.19701966184567\n",
      "epoch\t 28 \tloss:  68.20002803189992\n",
      "epoch\t 29 \tloss:  68.05036323624199\n",
      "epoch\t 29 \tloss:  68.25996293155504\n",
      "epoch\t 29 \tloss:  68.16975353766196\n",
      "epoch\t 30 \tloss:  68.17455673632678\n",
      "epoch\t 30 \tloss:  68.13179672522347\n",
      "epoch\t 30 \tloss:  68.1898361193937\n",
      "epoch\t 31 \tloss:  68.15617898105744\n",
      "epoch\t 31 \tloss:  68.14491956370708\n",
      "epoch\t 31 \tloss:  68.20720860333138\n",
      "epoch\t 32 \tloss:  68.17165119062635\n",
      "epoch\t 32 \tloss:  68.1356617034465\n",
      "epoch\t 32 \tloss:  68.16488858283968\n",
      "epoch\t 33 \tloss:  68.16177381180134\n",
      "epoch\t 33 \tloss:  68.1927334728085\n",
      "epoch\t 33 \tloss:  68.15165368156268\n",
      "epoch\t 34 \tloss:  68.16768589840808\n",
      "epoch\t 34 \tloss:  68.08901337172468\n",
      "epoch\t 34 \tloss:  68.27737735949925\n",
      "epoch\t 35 \tloss:  68.27023843424813\n",
      "epoch\t 35 \tloss:  68.14553365457928\n",
      "epoch\t 35 \tloss:  68.14563778675475\n",
      "epoch\t 36 \tloss:  68.12502057579724\n",
      "epoch\t 36 \tloss:  68.08950029514436\n",
      "epoch\t 36 \tloss:  68.31263068935719\n",
      "epoch\t 37 \tloss:  68.1779097344521\n",
      "epoch\t 37 \tloss:  68.15160041922785\n",
      "epoch\t 37 \tloss:  68.20792151702935\n",
      "epoch\t 38 \tloss:  68.14879956888795\n",
      "epoch\t 38 \tloss:  68.20719593081635\n",
      "epoch\t 38 \tloss:  68.19852387721086\n",
      "epoch\t 39 \tloss:  68.24199414025239\n",
      "epoch\t 39 \tloss:  68.14994961512173\n",
      "epoch\t 39 \tloss:  68.09860111753656\n",
      "epoch\t 40 \tloss:  68.1391569340729\n",
      "epoch\t 40 \tloss:  68.11781756273581\n",
      "epoch\t 40 \tloss:  68.22275542895508\n"
     ]
    }
   ],
   "source": [
    "# set aside a small portion for validation\n",
    "testset = 1000\n",
    "\n",
    "x_test = x_train[0:testset, :]\n",
    "y_test = y_train[0:testset]\n",
    "x_train_log = x_train[testset + 1:, :]\n",
    "y_train_log = y_train[testset + 1:]\n",
    "\n",
    "# standardize\n",
    "#x_train_log, mean, std = standardize(x_train_log)\n",
    "\n",
    "# add offset\n",
    "x_train_log = add_offset(x_train_log)\n",
    "\n",
    "# train using logistic regression (SGD)\n",
    "initial_w = np.random.rand(x_train_log.shape[1])\n",
    "epochs = 40\n",
    "batch_size = 100\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.01\n",
    "print_every = int(50000 / batch_size)\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train_log, x_train_log, initial_w, epochs, batch_size, gamma, lambda_, print_every)\n",
    "\n",
    "# free up memory\n",
    "del x_train_log\n",
    "del y_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593\n"
     ]
    }
   ],
   "source": [
    "# standardize\n",
    "#x_test_log = standardize_test(x_test, mean, std)\n",
    "\n",
    "# add offset\n",
    "x_test_log = add_offset(x_test)\n",
    "\n",
    "y_pred = predict_logistic_labels(weights, x_test_log)\n",
    "accuracy = get_accuracy(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logreg_preprocessing12_reduced.csv'\n",
    "\n",
    "# standardize\n",
    "#x_submission_log = standardize_test(x_submission_log, mean, std)\n",
    "\n",
    "# add offset\n",
    "x_submission_log = add_offset(x_submission)\n",
    "\n",
    "y_submission = predict_logistic_labels(weights, x_submission_log)\n",
    "ids = np.arange(len(y_submission)) + 1\n",
    "\n",
    "create_csv_submission(ids, y_submission, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
