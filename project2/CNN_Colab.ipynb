{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "x0vKgfHdXL9b",
    "outputId": "3661a0ab-2e98-4622-db7f-d3b36422467b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 29kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x5a0cc000 @  0x7f31d7a422a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 13.8MB/s \n",
      "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 4.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Installing collected packages: torch, pillow, torchvision\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "eKHbCoXoqNzy",
    "outputId": "50537d25-68fc-4ba3-a9d6-b09adfbb7e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdYXS5_zqiQL"
   },
   "outputs": [],
   "source": [
    "# Specify path to files on Google Drive\n",
    "\n",
    "ml_path = '/content/gdrive/My Drive/ML2018/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-GgFgNZrTsw"
   },
   "outputs": [],
   "source": [
    "# add Google Drive path to system path for imports\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, ml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "gYBIrnWdtcmq",
    "outputId": "a3abdcb2-b737-48ad-d2cc-c2b88e7e9ee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg_train.txt',\n",
       " 'pos_train.txt',\n",
       " 'vocab_pretrained_reduced.pkl',\n",
       " 'embeddings200_pretrained_reduced.npy',\n",
       " 'helpers.py',\n",
       " '__pycache__',\n",
       " 'test_data.txt',\n",
       " 'train_neg_full.txt',\n",
       " 'train_pos_full.txt',\n",
       " 'cnn1_reduced_32.csv',\n",
       " 'cnns.py',\n",
       " 'vocab_pretrained.pkl',\n",
       " 'embeddings200.npy',\n",
       " 'network.pt',\n",
       " 'cnn1_reduced_128.csv',\n",
       " 'tweets_pos_wstp.txt',\n",
       " 'tweets_neg_wstp.txt',\n",
       " 'test_data_wstp.txt',\n",
       " 'vocab_pretrained_wstp_reduced.pkl',\n",
       " 'embeddings200_wstp_pretrained_reduced.npy',\n",
       " 'network_1_128_full.pt',\n",
       " 'cnn1_128_reduced_wstp.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in Google Drive\n",
    "\n",
    "import os\n",
    "os.listdir(ml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "colab_type": "code",
    "id": "G9I867jzo3Dn",
    "outputId": "d6098229-a645-46a4-9446-c8c23b3dd24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet has 64 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:165: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\t Loss 0.5241\t Validation accuracy 0.7768\t 0.0284 s/batch\n",
      "Epoch 1 / 10\t Loss 0.4165\t Validation accuracy 0.8066\t 0.0261 s/batch\n",
      "Epoch 1 / 10\t Loss 0.3864\t Validation accuracy 0.8122\t 0.0264 s/batch\n",
      "Epoch 2 / 10\t Loss 0.3779\t Validation accuracy 0.8190\t 0.0259 s/batch\n",
      "Epoch 2 / 10\t Loss 0.3638\t Validation accuracy 0.8223\t 0.0265 s/batch\n",
      "Epoch 2 / 10\t Loss 0.3597\t Validation accuracy 0.8266\t 0.0264 s/batch\n",
      "Epoch 2 / 10\t Loss 0.3519\t Validation accuracy 0.8294\t 0.0282 s/batch\n",
      "Epoch 3 / 10\t Loss 0.3497\t Validation accuracy 0.8306\t 0.0262 s/batch\n",
      "Epoch 3 / 10\t Loss 0.3403\t Validation accuracy 0.8298\t 0.0262 s/batch\n",
      "Epoch 3 / 10\t Loss 0.3369\t Validation accuracy 0.8302\t 0.0262 s/batch\n",
      "Epoch 3 / 10\t Loss 0.3322\t Validation accuracy 0.8344\t 0.0265 s/batch\n",
      "Epoch 4 / 10\t Loss 0.3312\t Validation accuracy 0.8325\t 0.0260 s/batch\n",
      "Epoch 4 / 10\t Loss 0.3239\t Validation accuracy 0.8338\t 0.0264 s/batch\n",
      "Epoch 4 / 10\t Loss 0.3211\t Validation accuracy 0.8349\t 0.0262 s/batch\n",
      "Epoch 5 / 10\t Loss 0.3182\t Validation accuracy 0.8355\t 0.0263 s/batch\n",
      "Epoch 5 / 10\t Loss 0.3127\t Validation accuracy 0.8361\t 0.0264 s/batch\n",
      "Epoch 5 / 10\t Loss 0.3108\t Validation accuracy 0.8374\t 0.0263 s/batch\n",
      "Epoch 5 / 10\t Loss 0.3055\t Validation accuracy 0.8360\t 0.0261 s/batch\n",
      "Epoch 6 / 10\t Loss 0.3041\t Validation accuracy 0.8347\t 0.0262 s/batch\n",
      "Epoch 6 / 10\t Loss 0.2982\t Validation accuracy 0.8365\t 0.0262 s/batch\n",
      "Epoch 6 / 10\t Loss 0.2955\t Validation accuracy 0.8388\t 0.0262 s/batch\n",
      "Epoch 6 / 10\t Loss 0.2941\t Validation accuracy 0.8358\t 0.0264 s/batch\n",
      "Epoch 7 / 10\t Loss 0.2924\t Validation accuracy 0.8392\t 0.0264 s/batch\n",
      "Epoch 7 / 10\t Loss 0.2854\t Validation accuracy 0.8387\t 0.0262 s/batch\n",
      "Epoch 7 / 10\t Loss 0.2840\t Validation accuracy 0.8394\t 0.0264 s/batch\n",
      "Epoch 7 / 10\t Loss 0.2811\t Validation accuracy 0.8361\t 0.0262 s/batch\n",
      "Epoch 8 / 10\t Loss 0.2788\t Validation accuracy 0.8374\t 0.0249 s/batch\n",
      "Epoch 8 / 10\t Loss 0.2760\t Validation accuracy 0.8365\t 0.0264 s/batch\n",
      "Epoch 8 / 10\t Loss 0.2724\t Validation accuracy 0.8349\t 0.0262 s/batch\n",
      "Epoch 9 / 10\t Loss 0.2713\t Validation accuracy 0.8371\t 0.0261 s/batch\n",
      "Epoch 9 / 10\t Loss 0.2664\t Validation accuracy 0.8377\t 0.0264 s/batch\n",
      "Epoch 9 / 10\t Loss 0.2654\t Validation accuracy 0.8374\t 0.0264 s/batch\n",
      "Epoch 9 / 10\t Loss 0.2629\t Validation accuracy 0.8359\t 0.0266 s/batch\n",
      "Epoch 10 / 10\t Loss 0.2610\t Validation accuracy 0.8365\t 0.0264 s/batch\n",
      "Epoch 10 / 10\t Loss 0.2572\t Validation accuracy 0.8331\t 0.0265 s/batch\n",
      "Epoch 10 / 10\t Loss 0.2536\t Validation accuracy 0.8371\t 0.0262 s/batch\n",
      "Epoch 10 / 10\t Loss 0.2537\t Validation accuracy 0.8375\t 0.0262 s/batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SimpleConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.8368\n",
      "Submissions saved as /content/gdrive/My Drive/ML2018/cnn1_128_reduced_dropout.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from helpers import *\n",
    "from cnns import *\n",
    "\n",
    "\n",
    "# define file paths\n",
    "path_embeddings = ml_path + 'embeddings200_pretrained_reduced.npy'\n",
    "path_vocab = ml_path + 'vocab_pretrained_reduced.pkl'\n",
    "path_train_pos = ml_path + 'pos_train.txt'\n",
    "path_train_neg = ml_path + 'neg_train.txt'\n",
    "path_test = ml_path + 'test_data.txt'\n",
    "\n",
    "# define file names\n",
    "submission_file = ml_path + 'cnn1_128_reduced_dropout.csv'\n",
    "network_file = ml_path + 'network.pt'\n",
    "\n",
    "\n",
    "# load word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(np.zeros((1, embeddings.shape[1])), embeddings, axis=0)\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "\n",
    "# find maximal tweet length (number of words)\n",
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length          \n",
    "print(\"Longest tweet has {:d} words\".format(longest))\n",
    "\n",
    "\n",
    "'''\n",
    "For each tweet we create an array containing the indexes of the words in\n",
    "the word embedding matrix. All of these vectors have equal length equivalent\n",
    "to the number of words in the longest tweet. If a tweet doesn't fill everything,\n",
    "we pad with 0. this index corresponds to the embedding [0, 0, 0, ... 0, 0].\n",
    "\n",
    "Data type is int32 in order to minimize memory usage.\n",
    "\n",
    "For the labels, we use the value 0 for negative tweets and 1 for positive tweets.\n",
    "'''\n",
    "\n",
    "#initiate empty feature and label lists\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# process positive tweets\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(1)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# process negative tweets\n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(0)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# transform to numpy array\n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "\n",
    "# We proceed process test tweets in the same way as the training tweets\n",
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        # filter out the IDs and first comma\n",
    "        line_bare = line[(line.index(\",\")+1):]\n",
    "        for word in line_bare.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "        # convert to numpy array\n",
    "x_test = np.asarray(x)\n",
    "\n",
    "\n",
    "# specify the network you want to use\n",
    "net = SimpleConvNet(torch.from_numpy(embeddings).float()).cuda()\n",
    "# decide how many training tweets to use for validation\n",
    "val_prop = 10000\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "# defines after how many batches loss and accuracy are displayed\n",
    "print_every = 50\n",
    "# choose loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "# choose optimizer (Adam does fine most of the time)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "# cut validation data from training data.\n",
    "# convert numpy arrays to torch tensors\n",
    "x_val_torch = torch.from_numpy(x_train[0:val_prop, :])\n",
    "y_val_torch = torch.from_numpy(y_train[0:val_prop]).float()\n",
    "x_train_torch = torch.from_numpy(x_train[val_prop + 1:, :])\n",
    "y_train_torch = torch.from_numpy(y_train[val_prop + 1:]).float()\n",
    "\n",
    "# create batch loaders\n",
    "train_set = utils.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = utils.DataLoader(train_set, batch_size, shuffle=False)\n",
    "val_set = utils.TensorDataset(x_val_torch, y_val_torch)\n",
    "val_loader = utils.DataLoader(val_set, batch_size, shuffle=False)\n",
    "\n",
    "# Run training\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    for tweets, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        # converting to Variable is necessary in order to compute the gradient later\n",
    "        inputs = Variable(tweets).cuda()\n",
    "        targets = Variable(labels).cuda()\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # forward inputs through the net\n",
    "        outputs = net.forward(inputs.long())\n",
    "        # compute loss and gradient\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            accuracy = 0\n",
    "            n = 0\n",
    "            # compute accuracy on validation set\n",
    "            net.eval()\n",
    "            for tweets, labels in iter(val_loader):\n",
    "                predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "                accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "                n += labels.data.numpy().size\n",
    "            net.train()\n",
    "            \n",
    "            print(\"Epoch {} / {}\\t\".format(e+1, epochs),\n",
    "                  \"Loss {:.4f}\\t\".format(running_loss / print_every),\n",
    "                  \"Validation accuracy {:.4f}\\t\".format(accuracy / n),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every))\n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "\n",
    "\n",
    "# Save the trained network\n",
    "torch.save(net.cpu(), network_file)\n",
    "\n",
    "# Load a saved network\n",
    "net = torch.load(network_file).cuda()\n",
    "\n",
    "\n",
    "# compute accuracy based on a part of the training data\n",
    "net.eval()\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))\n",
    "\n",
    "\n",
    "# Compute preditions and transform the labels to (-1, 1)\n",
    "net.eval()\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# save predicitons on the test set to csv file.\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_file)\n",
    "print(\"Submissions saved as\", submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "xTiSKqLD2x6b",
    "outputId": "30154fc7-187b-4f72-d10f-31f2d0cdab1e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86e310197842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compute accuracy based on a part of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a saved network\n",
    "net = torch.load(network_file).cuda()\n",
    "\n",
    "\n",
    "# compute accuracy based on a part of the training data\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))\n",
    "\n",
    "\n",
    "# Compute preditions and transform the labels to (-1, 1)\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# save predicitons on the test set to csv file.\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_file)\n",
    "print(\"Submissions saved as\", submission_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1y2UUyXpdwx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
