{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "x0vKgfHdXL9b",
    "outputId": "895dfcb6-ed0a-41d0-d1de-6ce472bc2fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eKHbCoXoqNzy",
    "outputId": "9acfeb84-bcb4-4ab2-ce62-d4d697bf6d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdYXS5_zqiQL"
   },
   "outputs": [],
   "source": [
    "# Specify path to files on Google Drive\n",
    "\n",
    "ml_path = '/content/gdrive/My Drive/ML2018/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-GgFgNZrTsw"
   },
   "outputs": [],
   "source": [
    "# add Google Drive path to system path for imports\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, ml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "colab_type": "code",
    "id": "gYBIrnWdtcmq",
    "outputId": "5a9c31b5-aeeb-4fb1-ee49-ef6d86eb2a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg_train.txt',\n",
       " 'pos_train.txt',\n",
       " 'vocab_pretrained_reduced.pkl',\n",
       " 'embeddings200_pretrained_reduced.npy',\n",
       " 'helpers.py',\n",
       " '__pycache__',\n",
       " 'test_data.txt',\n",
       " 'train_neg_full.txt',\n",
       " 'train_pos_full.txt',\n",
       " 'cnn1_reduced_32.csv',\n",
       " 'vocab_pretrained.pkl',\n",
       " 'embeddings200.npy',\n",
       " 'cnn1_reduced_128.csv',\n",
       " 'embeddings200_wstp_pretrained_reduced.npy',\n",
       " 'vocab_pretrained_wstp_reduced.pkl',\n",
       " 'network_1_128_full.pt',\n",
       " 'cnn1_128_reduced_wstp.csv',\n",
       " 'cnn1_128_reduced_wstp_dropout.csv',\n",
       " 'cnn1_128_reduced_dropout.csv',\n",
       " 'test_data_wstp.txt',\n",
       " 'tweets_pos_wstp.txt',\n",
       " 'tweets_neg_wstp.txt',\n",
       " 'cnn1_256_wstp_reduced_dropout.csv',\n",
       " 'preprocessing_train_not_test.csv',\n",
       " 'vocab_pretrained_wstp_full.pkl',\n",
       " 'tweets_neg_full_wstp.txt',\n",
       " 'tweets_pos_full_wstp.txt',\n",
       " 'embeddings_pretrained_wstp_full.npy',\n",
       " 'test_data_wstp_nofrequent.txt',\n",
       " 'tweets_neg_wstp_nofrequent.txt',\n",
       " 'tweets_pos_wstp_nofrequent.txt',\n",
       " 'vocab_wstp_reduced_nofrequent',\n",
       " 'embeddings200_wstp_reduced_nofrequent.npy',\n",
       " 'network.pt',\n",
       " 'preprocessing_nofrequency.csv',\n",
       " 'classification.pt',\n",
       " 'classifier_reduced.csv',\n",
       " 'regression.pt',\n",
       " 'regression_reduced.csv',\n",
       " 'cnns.py',\n",
       " 'net.pt',\n",
       " 'simple_16.csv',\n",
       " 'simple_64.csv',\n",
       " 'simple_128.csv',\n",
       " 'simple_256.csv',\n",
       " 'simple_512.csv',\n",
       " 'simple_1024.csv',\n",
       " 'simple_noblock.csv',\n",
       " 'simple_dropout_025.csv',\n",
       " 'simple_dropout_05.csv',\n",
       " 'simple_dropout_075.csv',\n",
       " 'classification.csv',\n",
       " 'n_grams.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in Google Drive\n",
    "\n",
    "import os\n",
    "os.listdir(ml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "id": "G9I867jzo3Dn",
    "outputId": "5407a6eb-51b9-455f-8c38-1a105667813b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet has 64 words\n",
      "Epoch 1 / 5\t Loss 0.5712\t Validation accuracy 0.7294\t 0.0647 s/batch\n",
      "Epoch 1 / 5\t Loss 0.4731\t Validation accuracy 0.7759\t 0.0573 s/batch\n",
      "Epoch 1 / 5\t Loss 0.4255\t Validation accuracy 0.7943\t 0.0571 s/batch\n",
      "Epoch 1 / 5\t Loss 0.4026\t Validation accuracy 0.8095\t 0.0591 s/batch\n",
      "Epoch 1 / 5\t Loss 0.3950\t Validation accuracy 0.8120\t 0.0567 s/batch\n",
      "Epoch 1 / 5\t Loss 0.3797\t Validation accuracy 0.8186\t 0.0569 s/batch\n",
      "Epoch 1 / 5\t Loss 0.3799\t Validation accuracy 0.8153\t 0.0590 s/batch\n",
      "Epoch 1 / 5\t Loss 0.3732\t Validation accuracy 0.8163\t 0.0568 s/batch\n",
      "Epoch 1 / 5\t Loss 0.3684\t Validation accuracy 0.8241\t 0.0590 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3759\t Validation accuracy 0.8225\t 0.0558 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3604\t Validation accuracy 0.8259\t 0.0567 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3562\t Validation accuracy 0.8253\t 0.0589 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3515\t Validation accuracy 0.8288\t 0.0569 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3506\t Validation accuracy 0.8297\t 0.0590 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3507\t Validation accuracy 0.8298\t 0.0567 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3449\t Validation accuracy 0.8295\t 0.0569 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3456\t Validation accuracy 0.8291\t 0.0591 s/batch\n",
      "Epoch 2 / 5\t Loss 0.3392\t Validation accuracy 0.8333\t 0.0567 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3459\t Validation accuracy 0.8303\t 0.0556 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3397\t Validation accuracy 0.8312\t 0.0595 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3282\t Validation accuracy 0.8347\t 0.0569 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3309\t Validation accuracy 0.8309\t 0.0590 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3302\t Validation accuracy 0.8353\t 0.0566 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3292\t Validation accuracy 0.8345\t 0.0568 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3222\t Validation accuracy 0.8349\t 0.0591 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3217\t Validation accuracy 0.8343\t 0.0570 s/batch\n",
      "Epoch 3 / 5\t Loss 0.3227\t Validation accuracy 0.8353\t 0.0591 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3208\t Validation accuracy 0.8376\t 0.0555 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3203\t Validation accuracy 0.8349\t 0.0566 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3128\t Validation accuracy 0.8370\t 0.0590 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3104\t Validation accuracy 0.8367\t 0.0568 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3093\t Validation accuracy 0.8375\t 0.0568 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3109\t Validation accuracy 0.8363\t 0.0589 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3007\t Validation accuracy 0.8370\t 0.0567 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3067\t Validation accuracy 0.8374\t 0.0592 s/batch\n",
      "Epoch 4 / 5\t Loss 0.3032\t Validation accuracy 0.8380\t 0.0573 s/batch\n",
      "Epoch 4 / 5\t Loss 0.2995\t Validation accuracy 0.8394\t 0.0572 s/batch\n",
      "Epoch 5 / 5\t Loss 0.3051\t Validation accuracy 0.8382\t 0.0586 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2978\t Validation accuracy 0.8380\t 0.0571 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2918\t Validation accuracy 0.8388\t 0.0589 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2940\t Validation accuracy 0.8374\t 0.0569 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2874\t Validation accuracy 0.8377\t 0.0567 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2910\t Validation accuracy 0.8390\t 0.0593 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2856\t Validation accuracy 0.8374\t 0.0576 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2855\t Validation accuracy 0.8397\t 0.0574 s/batch\n",
      "Epoch 5 / 5\t Loss 0.2816\t Validation accuracy 0.8401\t 0.0591 s/batch\n",
      "Accuracy on validation set: 0.8416\n",
      "Submissions saved as /content/gdrive/My Drive/ML2018/submission1.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from helpers import *\n",
    "from cnns import *\n",
    "\n",
    "\n",
    "# define file paths\n",
    "path_embeddings = ml_path + 'embeddings200_pretrained_reduced.npy'\n",
    "path_vocab = ml_path + 'vocab_pretrained_reduced.pkl'\n",
    "path_train_pos = ml_path + 'pos_train.txt'\n",
    "path_train_neg = ml_path + 'neg_train.txt'\n",
    "path_test = ml_path + 'test_data.txt'\n",
    "\n",
    "# define file names\n",
    "submission_file = ml_path + 'submission1.csv'\n",
    "network_file = ml_path + 'net.pt'\n",
    "\n",
    "# loads word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(np.zeros((1, embeddings.shape[1])), embeddings, axis=0)\n",
    "\n",
    "# specify seed for random weight initialisation\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# specify the network you want to use\n",
    "net = NGrams(torch.from_numpy(embeddings).float()).cuda()\n",
    "\n",
    "# decide how many training tweets to use for validation\n",
    "val_prop = 10000\n",
    "batch_size = 1024\n",
    "epochs = 5\n",
    "\n",
    "# defines after how many batches loss and accuracy are displayed\n",
    "print_every = 20\n",
    "\n",
    "# choose optimizer (Adam does fine most of the time)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "\n",
    "# find maximal tweet length (number of words)\n",
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length          \n",
    "print(\"Longest tweet has {:d} words\".format(longest))\n",
    "\n",
    "\n",
    "'''\n",
    "For each tweet we create an array containing the indexes of the words in\n",
    "the word embedding matrix. All of these vectors have equal length equivalent\n",
    "to the number of words in the longest tweet. If a tweet doesn't fill everything,\n",
    "we pad with 0. this index corresponds to the embedding [0, 0, 0, ... 0, 0].\n",
    "\n",
    "Data type is int32 in order to minimize memory usage.\n",
    "\n",
    "For the labels, we use the value 0 for negative tweets and 1 for positive tweets.\n",
    "'''\n",
    "\n",
    "#initiate empty feature and label lists\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# process positive tweets\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(1)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# process negative tweets\n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(0)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# transform to numpy array\n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "# We proceed process test tweets in the same way as the training tweets\n",
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        # filter out the IDs and first comma\n",
    "        line_bare = line[(line.index(\",\")+1):]\n",
    "        for word in line_bare.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# convert to numpy array\n",
    "x_test = np.asarray(x)\n",
    "\n",
    "# get loss criterion of the network and the target type it requires\n",
    "criterion, target_type = net.get_criterion()\n",
    "\n",
    "# cut validation data from training data.\n",
    "# convert numpy arrays to torch tensors\n",
    "x_val_torch = torch.from_numpy(x_train[0:val_prop, :])\n",
    "y_val_torch = torch.from_numpy(y_train[0:val_prop]).float()\n",
    "x_train_torch = torch.from_numpy(x_train[val_prop + 1:, :])\n",
    "y_train_torch = torch.from_numpy(y_train[val_prop + 1:]).float()\n",
    "\n",
    "# create batch loaders\n",
    "train_set = utils.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = utils.DataLoader(train_set, batch_size, shuffle=False)\n",
    "val_set = utils.TensorDataset(x_val_torch, y_val_torch)\n",
    "val_loader = utils.DataLoader(val_set, batch_size, shuffle=False)\n",
    "\n",
    "# Run training\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    for tweets, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        # converting to Variable is necessary in order to compute the gradient later\n",
    "        inputs = Variable(tweets).cuda()\n",
    "        targets = Variable(labels.to(target_type)).cuda()\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # forward inputs through the net\n",
    "        outputs = net.forward(inputs.long())\n",
    "        # compute loss and gradient\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            accuracy = 0\n",
    "            n = 0\n",
    "            # compute accuracy on validation set\n",
    "            net.eval()\n",
    "            for tweets, labels in iter(val_loader):\n",
    "                predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "                accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "                n += labels.data.numpy().size\n",
    "            net.train()            \n",
    "\n",
    "            print(\"Epoch {} / {}\\t\".format(e+1, epochs),\n",
    "                  \"Loss {:.4f}\\t\".format(running_loss / print_every),\n",
    "                  \"Validation accuracy {:.4f}\\t\".format(accuracy / n),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every))\n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "\n",
    "# Save the trained network\n",
    "torch.save(net.cpu(), network_file)\n",
    "\n",
    "# compute accuracy based on a part of the training data\n",
    "net.cuda()\n",
    "net.eval()\n",
    "accuracy = 0\n",
    "n = 0\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))\n",
    "\n",
    "# Compute preditions and transform the labels to (-1, 1)\n",
    "net.eval()\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# save predicitons on the test set to csv file.\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_file)\n",
    "print(\"Submissions saved as\", submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "xTiSKqLD2x6b",
    "outputId": "30154fc7-187b-4f72-d10f-31f2d0cdab1e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86e310197842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compute accuracy based on a part of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a saved network\n",
    "net = torch.load(network_file).cuda()\n",
    "\n",
    "\n",
    "# compute accuracy based on a part of the training data\n",
    "net.cuda()\n",
    "net.eval()\n",
    "accuracy = 0\n",
    "n = 0\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))\n",
    "\n",
    "# Compute preditions and transform the labels to (-1, 1)\n",
    "net.eval()\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# save predicitons on the test set to csv file.\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_file)\n",
    "print(\"Submissions saved as\", submission_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1y2UUyXpdwx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
