{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by Convolutional Neural Networks (CNN)\n",
    "Inspired by [Bentrevett - ConvolutionalSentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb) and the exercises of the class [Data and Artificial Intelligence for Transportation](https://edu.epfl.ch/coursebook/en/data-and-artificial-intelligence-for-transportation-CIVIL-459)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from helpers import *\n",
    "from cnns import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Load Data and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Specify file paths here:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "\n",
    "path_embeddings = 'pretrained_glove/embeddings200_pretrained_reduced.npy'\n",
    "path_vocab = 'pretrained_glove/vocab_pretrained_reduced.pkl'\n",
    "path_train_pos = 'train_pos.txt'\n",
    "path_train_neg = 'train_neg.txt'\n",
    "path_test = 'test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(np.zeros((1, embeddings.shape[1])), embeddings, axis=0)\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet has 64 words\n"
     ]
    }
   ],
   "source": [
    "# find maximal tweet length (number of words)\n",
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length          \n",
    "print(\"Longest tweet has {:d} words\".format(longest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each tweet we create an array containing the indexes of the words in\n",
    "the word embedding matrix. All of these vectors have equal length equivalent\n",
    "to the number of words in the longest tweet. If a tweet doesn't fill everything,\n",
    "we pad with 0. this index corresponds to the embedding [0, 0, 0, ... 0, 0].\n",
    "\n",
    "Data type is int32 in order to minimize memory usage.\n",
    "\n",
    "For the labels, we use the value 0 for negative tweets and 1 for positive tweets.\n",
    "'''\n",
    "\n",
    "#initiate empty feature and label lists\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# process positive tweets\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(1)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# process negative tweets\n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(0)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# transform to numpy array\n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We proceed process test tweets in the same way as the training tweets\n",
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        # filter out the IDs and first comma\n",
    "        line_bare = line[(line.index(\",\")+1):]\n",
    "        for word in line_bare.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "        # convert to numpy array\n",
    "x_test = np.asarray(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Specify Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify seed for random weight initialisation\n",
    "torch.manual_seed(1234)\n",
    "# specify the network you want to use\n",
    "net = NGrams(torch.from_numpy(embeddings).float())\n",
    "# decide how many training tweets to use for validation\n",
    "val_prop = 10000\n",
    "batch_size = 1024\n",
    "epochs = 1\n",
    "# defines after how many batches loss and accuracy are displayed\n",
    "print_every = 5\n",
    "# choose optimizer (Adam does fine most of the time)\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\t Loss 0.6468\t Validation accuracy 0.6565\t 0.8850 s/batch\n",
      "Epoch 1 / 1\t Loss 0.5715\t Validation accuracy 0.6786\t 0.7211 s/batch\n",
      "Epoch 1 / 1\t Loss 0.5500\t Validation accuracy 0.7014\t 0.8805 s/batch\n",
      "Epoch 1 / 1\t Loss 0.5162\t Validation accuracy 0.7294\t 0.7436 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4945\t Validation accuracy 0.7483\t 0.9216 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4815\t Validation accuracy 0.7630\t 0.8740 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4695\t Validation accuracy 0.7705\t 0.8147 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4470\t Validation accuracy 0.7761\t 0.8678 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4391\t Validation accuracy 0.7831\t 1.1238 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4197\t Validation accuracy 0.7893\t 0.7269 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4313\t Validation accuracy 0.7930\t 1.0116 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4118\t Validation accuracy 0.7944\t 0.7085 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4136\t Validation accuracy 0.8024\t 0.7490 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3979\t Validation accuracy 0.8016\t 0.7405 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4090\t Validation accuracy 0.8081\t 0.7687 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3900\t Validation accuracy 0.8092\t 0.8422 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3969\t Validation accuracy 0.8086\t 0.7472 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3958\t Validation accuracy 0.8093\t 0.7878 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3841\t Validation accuracy 0.8094\t 0.7121 s/batch\n",
      "Epoch 1 / 1\t Loss 0.4029\t Validation accuracy 0.8129\t 0.7518 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3900\t Validation accuracy 0.8132\t 0.7865 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3837\t Validation accuracy 0.8140\t 0.7646 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3753\t Validation accuracy 0.8145\t 0.8617 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3699\t Validation accuracy 0.8187\t 0.9883 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3787\t Validation accuracy 0.8159\t 0.7645 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3836\t Validation accuracy 0.8179\t 0.6405 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3813\t Validation accuracy 0.8171\t 0.6355 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3756\t Validation accuracy 0.8152\t 0.7425 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3779\t Validation accuracy 0.8198\t 0.6380 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3745\t Validation accuracy 0.8157\t 0.6363 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3721\t Validation accuracy 0.8211\t 0.6704 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3686\t Validation accuracy 0.8163\t 0.7506 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3727\t Validation accuracy 0.8228\t 0.7475 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3656\t Validation accuracy 0.8220\t 0.6740 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3691\t Validation accuracy 0.8234\t 0.7253 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3659\t Validation accuracy 0.8234\t 0.7305 s/batch\n",
      "Epoch 1 / 1\t Loss 0.3740\t Validation accuracy 0.8199\t 0.7417 s/batch\n"
     ]
    }
   ],
   "source": [
    "# get loss criterion of the network and the target type it requires\n",
    "criterion, target_type = net.get_criterion()\n",
    "\n",
    "# cut validation data from training data.\n",
    "# convert numpy arrays to torch tensors\n",
    "x_val_torch = torch.from_numpy(x_train[0:val_prop, :])\n",
    "y_val_torch = torch.from_numpy(y_train[0:val_prop]).float()\n",
    "x_train_torch = torch.from_numpy(x_train[val_prop + 1:, :])\n",
    "y_train_torch = torch.from_numpy(y_train[val_prop + 1:]).float()\n",
    "\n",
    "# create batch loaders\n",
    "train_set = utils.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = utils.DataLoader(train_set, batch_size, shuffle=False)\n",
    "val_set = utils.TensorDataset(x_val_torch, y_val_torch)\n",
    "val_loader = utils.DataLoader(val_set, batch_size, shuffle=False)\n",
    "\n",
    "# Run training\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    for tweets, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        # converting to Variable is necessary in order to compute the gradient later\n",
    "        inputs = Variable(tweets)\n",
    "        targets = Variable(labels.to(target_type))\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # forward inputs through the net\n",
    "        outputs = net.forward(inputs.long())\n",
    "        # compute loss and gradient\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            accuracy = 0\n",
    "            n = 0\n",
    "            # compute accuracy on validation set\n",
    "            net.eval()\n",
    "            for tweets, labels in iter(val_loader):\n",
    "                # predictions are the index of the class wiht the highest probability\n",
    "                predictions = net.predict(tweets.long())\n",
    "                accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "                n += labels.data.numpy().size\n",
    "            net.train()\n",
    "            \n",
    "            print(\"Epoch {} / {}\\t\".format(e+1, epochs),\n",
    "                  \"Loss {:.4f}\\t\".format(running_loss / print_every),\n",
    "                  \"Validation accuracy {:.4f}\\t\".format(accuracy / n),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every))\n",
    "            running_loss = 0\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained network\n",
    "torch.save(net, 'firstnet.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved network\n",
    "net = torch.load('firstnet.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy based on a part of the training data\n",
    "net.eval()\n",
    "accuracy = 0\n",
    "n = 0\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long())\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create submission file for challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = 'submission1.csv'\n",
    "\n",
    "# Compute preditions and transform the labels to (-1, 1)\n",
    "net.eval()\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long())\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# save predicitons on the test set to csv file.\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
