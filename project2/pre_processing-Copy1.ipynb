{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = False\n",
    "cleaned_neg = False\n",
    "cleaned_neg_full = False\n",
    "cleaned_pos_full = False\n",
    "cleaned_test = False\n",
    "\n",
    "freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "freq_word = list(freq_word.text)\n",
    "# freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n"
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    dung austin read my mention or not only austin...\n",
       "2    because your logic is so dumb will not even cr...\n",
       "3     just put caper in box loved the battle crakbitch\n",
       "4    thanks sir do not trip ll mamma just kept down...\n",
       "5    visiting my brother mr is the beset birthday g...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    # interpret emoji\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/pre_processed/tweets_pos_removewords.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg_full :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg_full.index = tweets_neg_full.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg_full.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"split numbers\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastag\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "\n",
    "    print(\"expand contractions\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' your ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' ')\n",
    "    print(\"punctuaction\")\n",
    "        #remove punctuaction\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"remove words\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"haaapy to haapy\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #correct words with textblob\n",
    "    print(\"Correcting with textblob\")\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    print(\"lemmatizing\")\n",
    "    #lemmatize words\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "   \n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg_full))\n",
    "    \n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_removewords.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"rb\")\n",
    "tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e92c6c6541fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#correct words with textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT BLOB!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtweets_pos_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_pos_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m# tweets_neg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e92c6c6541fb>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#correct words with textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT BLOB!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtweets_pos_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_pos_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m# tweets_neg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;31m# regex matches: word or punctuation or whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         '''\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         '''\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\en\\__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__iter__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__contains__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not cleaned_pos_full:\n",
    "    #read data .txt file\n",
    "    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' you ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    sea do pro sea sister sports with the portable...\n",
       "2    sucks we work al we so now can not come cher y...\n",
       "3            i can not stay away from bug that my baby\n",
       "4    no madam laugh in perfectly fine and not conta...\n",
       "5    whenever far asleep watching the to always wak...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 => test \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    test_data['text'] = test_data['text'].str.replace(' ur ', ' your ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' u ', ' you ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' yourl ', ' your ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/test_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/test_textblob.pickle\",\"rb\")\n",
    "test_data_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data .txt file\n",
    "# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos_full.drop_duplicates(inplace=True)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"Interpreting emojis!!\")\n",
    "#  # interpret emoji\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"Separating Numbers!!\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "# #     print(tweets_pos_full['text'].count())\n",
    "# print(\"Remove Hashtags!!\")\n",
    "# #remove hashtag #\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "# print(\"Replace ur by your!!\")\n",
    "# #replace ur by your\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions!!\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n",
    "# #remove punctuaction\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words!!\")\n",
    "# # remove words user, url, number\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"Replace more leeters!!\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data from .txt to preprocess\n",
    "# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "\n",
    "# #increase index to start at 1\n",
    "# tweets_neg_full.index = tweets_neg_full.index +1\n",
    "# #remove duplicates\n",
    "# tweets_neg_full.drop_duplicates(inplace=True)\n",
    "# test = tweets_neg_full.head(50)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"emoji\")\n",
    "# # interpret emoji\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"split numbers\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "# print(\"hastag\")\n",
    "# #remove hashtag #\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "# print(\"ur your\")\n",
    "# #replace ur by your\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n",
    "# print(\"punctuaction\")\n",
    "#     #remove punctuaction\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words\")\n",
    "# # remove words user, url, number\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"haaapy to haapy\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n",
    "\n",
    "# #remove stop words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n",
    "# combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove most frequent words \n",
    "# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "# freq_max_total = list(freq_max_total.index)\n",
    "\n",
    "# #remove least frequent words\n",
    "# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "# freq_min_total = list(freq_min_total.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_min_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words = freq_max_total+ freq_min_total\n",
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the file to pickle\n",
    "# print(\"Saving most frequent and lest frequent words\")\n",
    "# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n",
    "# pickle.dump(freq_words, pickle_out)\n",
    "# pickle_out.close()\n",
    "# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n",
    "# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n",
    "# f.close()\n",
    "# print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n",
    "# xxxxxx = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos.index = tweets_pos.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_full_pickle.str.contains('like').sum()\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_pos.replace(freq_words_20,' ', inplace=True)\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_pos.text.str.contains(' like ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n",
    "# tweets_neg_freqwords20 = pickle.load(pickle_in)\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                              dtype=tweets_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos['text'] = tweets_pos['text'].str.replace('a', 'aaaaaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin reaaaaaaaaad my mention ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>becaaaaaaaaause your logic is so dumb , i won'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put caaaaaaaaasper in aaaaaaaaa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thaaaaaaaaanks sir &gt; &gt; don't tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; yaaaaaaaaay ! ! #lifecompleted . tweet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; #1dnextaaaaaaaaalbumtitle : feel for yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>workin haaaaaaaaard or haaaaaaaaardly workin r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;user&gt; i saaaaaaaaaw . i'll be replying in aaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is were i belong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <user> i dunno justin reaaaaaaaaad my mention ...\n",
       "1  becaaaaaaaaause your logic is so dumb , i won'...\n",
       "2  \" <user> just put caaaaaaaaasper in aaaaaaaaa ...\n",
       "3  <user> <user> thaaaaaaaaanks sir > > don't tri...\n",
       "4  visiting my brother tmr is the bestest birthda...\n",
       "5  <user> yaaaaaaaaay ! ! #lifecompleted . tweet ...\n",
       "6  <user> #1dnextaaaaaaaaalbumtitle : feel for yo...\n",
       "7  workin haaaaaaaaard or haaaaaaaaardly workin r...\n",
       "8  <user> i saaaaaaaaaw . i'll be replying in aaa...\n",
       "9                              this is were i belong"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweets_pos.head(10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andres Montero\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i funny austin reaad my mention or not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>becaause your logic is so dumb , i won't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put caasper in aa box ! \" looked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thaanks sir &gt; &gt; don't trip ll ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother mr is the beset birthdaay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; yaay ! ! #lifecompleted . sweet / faace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; #1dnextaalbumtitle : feel for you / rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>working haard or haardly working it &lt;user&gt; aat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;user&gt; i saaw . i'll be replying in aa bit .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is were i belong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <user> i funny austin reaad my mention or not ...\n",
       "1  becaause your logic is so dumb , i won't even ...\n",
       "2  \" <user> just put caasper in aa box ! \" looked...\n",
       "3  <user> <user> thaanks sir > > don't trip ll ma...\n",
       "4  visiting my brother mr is the beset birthdaay ...\n",
       "5  <user> yaay ! ! #lifecompleted . sweet / faace...\n",
       "6  <user> #1dnextaalbumtitle : feel for you / rol...\n",
       "7  working haard or haardly working it <user> aat...\n",
       "8       <user> i saaw . i'll be replying in aa bit .\n",
       "9                              this is were i belong"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace more letters haaaaaaaaaappy => happy\n",
    "test['text'] = test['text'].apply(lambda x: replace_moreletters(x))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andres Montero\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i funny austin reaad my mention or not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>becaause your logic is so dumb , i won't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put caasper in aa box ! \" looked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thaanks sir &gt; &gt; don't trip ll ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother mr is the beset birthdaay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; yaay ! ! #lifecompleted . sweet / faace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; #1dnextaalbumtitle : feel for you / rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>working haard or haardly working it &lt;user&gt; aat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;user&gt; i saaw . i'll be replying in aa bit .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is were i belong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <user> i funny austin reaad my mention or not ...\n",
       "1  becaause your logic is so dumb , i won't even ...\n",
       "2  \" <user> just put caasper in aa box ! \" looked...\n",
       "3  <user> <user> thaanks sir > > don't trip ll ma...\n",
       "4  visiting my brother mr is the beset birthdaay ...\n",
       "5  <user> yaay ! ! #lifecompleted . sweet / faace...\n",
       "6  <user> #1dnextaalbumtitle : feel for you / rol...\n",
       "7  working haard or haardly working it <user> aat...\n",
       "8       <user> i saaw . i'll be replying in aa bit .\n",
       "9                              this is were i belong"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'] = test['text'].apply(lambda x: replace_moreletters(x))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andres Montero\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i funny austin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put clasped in a box ! \" looked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip ll mam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother mr is the beset birthday g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; may ! ! #lifecompleted . sweet / faaceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; #1dnextaalbumtitle : feel for you / rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>working heard or hardly working it &lt;user&gt; at h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;user&gt; i saw . i'll be replying in a bit .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is were i belong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <user> i funny austin read my mention or not ....\n",
       "1  because your logic is so dumb , i won't even c...\n",
       "2  \" <user> just put clasped in a box ! \" looked ...\n",
       "3  <user> <user> thanks sir > > don't trip ll mam...\n",
       "4  visiting my brother mr is the beset birthday g...\n",
       "5  <user> may ! ! #lifecompleted . sweet / faaceb...\n",
       "6  <user> #1dnextaalbumtitle : feel for you / rol...\n",
       "7  working heard or hardly working it <user> at h...\n",
       "8         <user> i saw . i'll be replying in a bit .\n",
       "9                              this is were i belong"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
