{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (using GloVe)\n",
    "The idea behind this approach is to average the word vectors over every tweet, and use this average vectors to train logistic regression.\n",
    "\n",
    "Before running this notebook, make sure to have saved a embedding matrix named \"embeddings.npy\" beforehand. To do this, follow the instructions in Readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from logreg import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our GloVe word embeddings from file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load('embeddings200.npy')\n",
    "# load vocabulary\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... ord load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word vectors over tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000  tweets processed\n",
      "200000  tweets processed\n",
      "300000  tweets processed\n",
      "400000  tweets processed\n",
      "500000  tweets processed\n",
      "600000  tweets processed\n",
      "700000  tweets processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9a937471d375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mwordcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# skip words for which we have no embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Average vectors for training tweets\n",
    "\n",
    "'''\n",
    "Appending a row for each tweet is impracticable slow. \n",
    "However, we can not know in advance the number of tweets we will\n",
    "be appended (this is because we skip tweets fr which we have no embeddings).\n",
    "therefore we allocate a too big array fr x_train and cut wht's too much\n",
    "in the end.\n",
    "'''\n",
    "allocate_columns = 3000000\n",
    "x_train = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "y_train = np.zeros(allocate_columns)\n",
    "counter = 0\n",
    "\n",
    "with open('pos_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = 1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "with open('neg_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = -1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "# cut zero rows in x_train and y_train\n",
    "y_train = y_train[np.nonzero(y_train)]\n",
    "x_train = x_train[np.nonzero(y_train)]\n",
    "                               \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "\n",
    "np.save('x_train_own_200', x_train)\n",
    "np.save('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "\n",
    "x_train = np.load('x_train_own_200.npy')\n",
    "y_train = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average words for testing data\n",
    "\n",
    "allocate_columns = 300000\n",
    "x_submission = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "embeddings_mean = np.expand_dims(np.mean(embeddings, axis=0), axis=0)\n",
    "counter = 0\n",
    "\n",
    "with open('test_data.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        # TODO: filter out the IDs\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_submission[counter, :] = mean\n",
    "        else:\n",
    "            # in case that we have no embedding for any word of the tweet\n",
    "            # just use the overall mean of the embeddings\n",
    "            x_submission[counter, :] = embeddings_mean\n",
    "        counter += 1\n",
    "        if counter % 5000 == 0:\n",
    "            print(str(counter), \" tweets processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "\n",
    "np.save('x_submission_own_200', x_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "\n",
    "x_submission = np.load('x_submission_own_200.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aside a small portion for validation\n",
    "\n",
    "x_test = x_train[0:100000, :]\n",
    "y_test = y_train[0:100000]\n",
    "x_train_log = x_train[100001:, :]\n",
    "y_train_log = y_train[100001:]\n",
    "\n",
    "# standardize\n",
    "#x_train_log, mean, std = standardize(x_train_log)\n",
    "\n",
    "# add offset\n",
    "x_train_log = add_offset(x_train_log)\n",
    "\n",
    "# train using logistic regression (SGD)\n",
    "initial_w = np.random.rand(x_train_log.shape[1])\n",
    "epochs = 3\n",
    "batch_size = 10\n",
    "gamma = 0.00005\n",
    "lambda_ = 0.1\n",
    "print_every = int(500000 / batch_size)\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train_log, x_train_log, initial_w, epochs, batch_size, gamma, lambda_, print_every)\n",
    "\n",
    "# free up memory\n",
    "del x_train_log\n",
    "del y_train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on a local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "#x_test_log = standardize_test(x_test, mean, std)\n",
    "\n",
    "# add offset\n",
    "x_test_log = add_offset(x_test)\n",
    "\n",
    "y_pred = predict_logistic_labels(weights, x_test_log)\n",
    "accuracy = get_accuracy(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test dataset, prepare submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logreg_own_200.csv'\n",
    "\n",
    "# add offset\n",
    "x_submission_log = add_offset(x_submission)\n",
    "\n",
    "# standardize\n",
    "#x_submission_log = standardize_test(x_submission_log, mean, std)\n",
    "\n",
    "y_submission = predict_logistic_labels(weights, x_submission_log)\n",
    "ids = np.arange(len(y_submission)) + 1\n",
    "\n",
    "create_csv_submission(ids, y_submission, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
