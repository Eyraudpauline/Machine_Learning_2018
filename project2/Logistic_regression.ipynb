{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (using GloVe)\n",
    "The idea behind this approach is to average the word vectors over every tweet, and use this average vectors to train logistic regression.\n",
    "\n",
    "Before running this notebook, make sure to have saved a embedding matrix named \"embeddings.npy\" beforehand. To do this, follow the instructions in Readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from logreg import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our GloVe word embeddings from file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load('pretrained_glove/embeddings25.npy')\n",
    "# load vocabulary\n",
    "with open('pretrained_glove/vocab_pretrained.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word vectors over tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000  tweets processed\n",
      "200000  tweets processed\n",
      "300000  tweets processed\n",
      "400000  tweets processed\n",
      "500000  tweets processed\n",
      "600000  tweets processed\n",
      "700000  tweets processed\n",
      "800000  tweets processed\n",
      "900000  tweets processed\n",
      "1000000  tweets processed\n",
      "1100000  tweets processed\n",
      "1200000  tweets processed\n",
      "1300000  tweets processed\n",
      "1400000  tweets processed\n",
      "1500000  tweets processed\n",
      "1600000  tweets processed\n",
      "1700000  tweets processed\n",
      "1800000  tweets processed\n",
      "1900000  tweets processed\n",
      "2000000  tweets processed\n",
      "2100000  tweets processed\n",
      "2200000  tweets processed\n",
      "2300000  tweets processed\n",
      "2400000  tweets processed\n"
     ]
    }
   ],
   "source": [
    "# Average vectors for training tweets\n",
    "\n",
    "'''\n",
    "Appending a row for each tweet is impracticable slow. \n",
    "However, we can not know in advance the number of tweets we will\n",
    "be appended (this is because we skip tweets fr which we have no embeddings).\n",
    "therefore we allocate a too big array fr x_train and cut wht's too much\n",
    "in the end.\n",
    "'''\n",
    "allocate_columns = 3000000\n",
    "x_train = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "y_train = np.zeros(allocate_columns)\n",
    "counter = 0\n",
    "\n",
    "with open('pos_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = 1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "with open('neg_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = -1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "# cut zero rows in x_train and y_train\n",
    "y_train = y_train[np.nonzero(y_train)]\n",
    "x_train = x_train[np.nonzero(y_train)]\n",
    "                               \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "np.save('x_train', x_train)\n",
    "np.save('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000  tweets processed\n",
      "10000  tweets processed\n",
      "(10000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Average words for testing data\n",
    "\n",
    "allocate_columns = 300000\n",
    "x_submission = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "embeddings_mean = np.expand_dims(np.mean(embeddings, axis=0), axis=0)\n",
    "counter = 0\n",
    "\n",
    "with open('test_data.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        # TODO: filter out the IDs\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_submission[counter, :] = mean\n",
    "        else:\n",
    "            # in case that we have no embedding for any word of the tweet\n",
    "            # just use the overall mean of the embeddings\n",
    "            x_submission[counter, :] = embeddings_mean\n",
    "        counter += 1\n",
    "        if counter % 5000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "# cut zero rows in x_submission\n",
    "x_submission = x_submission[np.nonzero(x_submission[:, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "np.save('x_submission', x_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "x_submission = np.load('x_submission.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t 1 \tloss:  66.26861130663835\n",
      "epoch\t 1 \tloss:  59.907747650788245\n",
      "epoch\t 1 \tloss:  58.16284050292517\n",
      "epoch\t 1 \tloss:  57.38746486283372\n",
      "epoch\t 2 \tloss:  56.77542477410269\n",
      "epoch\t 2 \tloss:  56.63717603724137\n",
      "epoch\t 2 \tloss:  56.526197040322664\n",
      "epoch\t 2 \tloss:  56.396517684591316\n",
      "epoch\t 3 \tloss:  56.266261003933984\n",
      "epoch\t 3 \tloss:  56.16599868645683\n",
      "epoch\t 3 \tloss:  56.27838804560098\n",
      "epoch\t 3 \tloss:  56.126030274204524\n",
      "epoch\t 4 \tloss:  55.997902618207235\n",
      "epoch\t 4 \tloss:  56.04623096622244\n",
      "epoch\t 4 \tloss:  56.08027005717659\n",
      "epoch\t 4 \tloss:  56.036732651334965\n",
      "epoch\t 5 \tloss:  55.985069198783755\n",
      "epoch\t 5 \tloss:  55.95040445518855\n",
      "epoch\t 5 \tloss:  56.07856103170952\n",
      "epoch\t 5 \tloss:  55.97498139095251\n"
     ]
    }
   ],
   "source": [
    "# set aside a small portion for validation\n",
    "\n",
    "x_test = x_train[0:100000, :]\n",
    "y_test = y_train[0:100000]\n",
    "x_train_log = x_train[100001:, :]\n",
    "y_train_log = y_train[100001:]\n",
    "\n",
    "# standardize\n",
    "#x_train_log, mean, std = standardize(x_train_log)\n",
    "\n",
    "# add offset\n",
    "x_train_log = add_offset(x_train_log)\n",
    "\n",
    "# train using logistic regression (SGD)\n",
    "initial_w = np.random.rand(x_train_log.shape[1])\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.0001\n",
    "print_every = int(500000 / batch_size)\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train_log, x_train_log, initial_w, epochs, batch_size, gamma, lambda_, print_every)\n",
    "\n",
    "# free up memory\n",
    "del x_train_log\n",
    "del y_train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on a local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69761\n"
     ]
    }
   ],
   "source": [
    "# standardize\n",
    "#x_test_log = standardize_test(x_test, mean, std)\n",
    "\n",
    "# add offset\n",
    "x_test_log = add_offset(x_test)\n",
    "\n",
    "y_pred = predict_logistic_labels(weights, x_test_log)\n",
    "accuracy = get_accuracy(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test dataset, prepare submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logreg_pretrained_25.csv'\n",
    "\n",
    "# standardize\n",
    "#x_submission_log = standardize_test(x_submission_log, mean, std)\n",
    "\n",
    "# add offset\n",
    "x_submission_log = add_offset(x_submission)\n",
    "\n",
    "y_submission = predict_logistic_labels(weights, x_submission_log)\n",
    "ids = np.arange(len(y_submission)) + 1\n",
    "\n",
    "create_csv_submission(ids, y_submission, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
