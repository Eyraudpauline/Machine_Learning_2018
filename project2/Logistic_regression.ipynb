{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (using GloVe)\n",
    "The idea behind this approach is to average the word vectors over every tweet, and use this average vectors to train logistic regression.\n",
    "\n",
    "Before running this notebook, make sure to have saved a embedding matrix named \"embeddings.npy\" beforehand. To do this, follow the instructions in Readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from logreg import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our GloVe word embeddings from file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load('embeddings.npy')\n",
    "# load vocabulary\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... ord load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word vectors over tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average vectors for training tweets\n",
    "\n",
    "x_train = np.zeros((0, embeddings.shape[1]))\n",
    "y_train = np.asarray([])\n",
    "\n",
    "with open('pos_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train = np.append(x_train, mean, axis=0)\n",
    "            y_train = np.append(y_train, 1)\n",
    "        \n",
    "with open('neg_train.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train = np.append(x_train, mean, axis=0)\n",
    "            y_train = np.append(y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "\n",
    "np.save('x_train', x_train)\n",
    "np.save('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average words \n",
    "\n",
    "x_submission = np.zeros((0, embeddings.shape[1]))\n",
    "embeddings_mean = np.expand_dims(np.mean(embeddings, axis=0), axis=0)\n",
    "\n",
    "with open('test_data.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        # TODO: filter out the IDs\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_submission = np.append(x_submission, mean, axis=0)\n",
    "        else:\n",
    "            # in case that we have no embedding for any word of the tweet\n",
    "            # just use the overall mean of the embeddings\n",
    "            x_submission = np.append(x_submission, embeddings_mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] save variables\n",
    "\n",
    "np.save('x_submission', x_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] load variables to save time\n",
    "\n",
    "x_submission = np.load('x_submission.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t 1 \tloss:  7.397889551882617\n",
      "epoch\t 1 \tloss:  7.310262854673712\n",
      "epoch\t 1 \tloss:  7.256834514203162\n",
      "epoch\t 2 \tloss:  7.194713854586844\n",
      "epoch\t 2 \tloss:  7.153948949222492\n",
      "epoch\t 2 \tloss:  7.136153655741204\n",
      "epoch\t 3 \tloss:  7.089086464402596\n",
      "epoch\t 3 \tloss:  7.070922796714321\n",
      "epoch\t 3 \tloss:  7.052130447356217\n",
      "epoch\t 4 \tloss:  7.026688884754656\n",
      "epoch\t 4 \tloss:  7.013396232726991\n",
      "epoch\t 4 \tloss:  7.003113173044302\n",
      "epoch\t 5 \tloss:  6.991018839991395\n",
      "epoch\t 5 \tloss:  6.981029060951287\n",
      "epoch\t 5 \tloss:  6.9754783894156445\n",
      "epoch\t 6 \tloss:  6.965948806117815\n",
      "epoch\t 6 \tloss:  6.960269312570624\n",
      "epoch\t 6 \tloss:  6.959923317978275\n",
      "epoch\t 7 \tloss:  6.951230434433854\n",
      "epoch\t 7 \tloss:  6.949458512216098\n",
      "epoch\t 7 \tloss:  6.948083237312386\n",
      "epoch\t 8 \tloss:  6.942848726614485\n",
      "epoch\t 8 \tloss:  6.942260721381591\n",
      "epoch\t 8 \tloss:  6.94178359712171\n",
      "epoch\t 9 \tloss:  6.939054554094291\n",
      "epoch\t 9 \tloss:  6.938797938075486\n",
      "epoch\t 9 \tloss:  6.936865105319706\n",
      "epoch\t 10 \tloss:  6.935862402197973\n",
      "epoch\t 10 \tloss:  6.93483916028237\n",
      "epoch\t 10 \tloss:  6.935685573612858\n",
      "epoch\t 11 \tloss:  6.934783210971516\n",
      "epoch\t 11 \tloss:  6.934463061080096\n",
      "epoch\t 11 \tloss:  6.933262068463929\n",
      "epoch\t 12 \tloss:  6.933329375712118\n",
      "epoch\t 12 \tloss:  6.932398483425578\n",
      "epoch\t 12 \tloss:  6.933146017825967\n",
      "epoch\t 13 \tloss:  6.932173037263733\n",
      "epoch\t 13 \tloss:  6.932818904572661\n",
      "epoch\t 13 \tloss:  6.932256262050518\n",
      "epoch\t 14 \tloss:  6.932372515398698\n",
      "epoch\t 14 \tloss:  6.931603653829746\n",
      "epoch\t 14 \tloss:  6.9320882184568795\n",
      "epoch\t 15 \tloss:  6.931928837365737\n",
      "epoch\t 15 \tloss:  6.9319629887507705\n",
      "epoch\t 15 \tloss:  6.9319356834310994\n",
      "epoch\t 16 \tloss:  6.931440681148003\n",
      "epoch\t 16 \tloss:  6.931954855145689\n",
      "epoch\t 16 \tloss:  6.931818105750683\n",
      "epoch\t 17 \tloss:  6.9315181691301975\n",
      "epoch\t 17 \tloss:  6.931686428810573\n",
      "epoch\t 17 \tloss:  6.93202194528669\n",
      "epoch\t 18 \tloss:  6.93148635825652\n",
      "epoch\t 18 \tloss:  6.931562769031492\n",
      "epoch\t 18 \tloss:  6.931620116627838\n",
      "epoch\t 19 \tloss:  6.931559061738137\n",
      "epoch\t 19 \tloss:  6.931685927666053\n",
      "epoch\t 19 \tloss:  6.93152604206555\n",
      "epoch\t 20 \tloss:  6.931624321987554\n",
      "epoch\t 20 \tloss:  6.931723397704594\n",
      "epoch\t 20 \tloss:  6.931191909812722\n",
      "epoch\t 21 \tloss:  6.931781156154945\n",
      "epoch\t 21 \tloss:  6.931703665564096\n",
      "epoch\t 21 \tloss:  6.931428902522198\n",
      "epoch\t 22 \tloss:  6.93142301396947\n",
      "epoch\t 22 \tloss:  6.9316810298974785\n",
      "epoch\t 22 \tloss:  6.931639057937472\n",
      "epoch\t 23 \tloss:  6.931415156544655\n",
      "epoch\t 23 \tloss:  6.931496916289687\n",
      "epoch\t 23 \tloss:  6.931494899269466\n",
      "epoch\t 24 \tloss:  6.931401100526507\n",
      "epoch\t 24 \tloss:  6.931496251729227\n",
      "epoch\t 24 \tloss:  6.931717860023798\n",
      "epoch\t 25 \tloss:  6.931462469799799\n",
      "epoch\t 25 \tloss:  6.931634303905984\n",
      "epoch\t 25 \tloss:  6.9316068569927545\n",
      "epoch\t 26 \tloss:  6.931446427208689\n",
      "epoch\t 26 \tloss:  6.931590698879142\n",
      "epoch\t 26 \tloss:  6.93158444543361\n",
      "epoch\t 27 \tloss:  6.931665576831851\n",
      "epoch\t 27 \tloss:  6.931132298186296\n",
      "epoch\t 27 \tloss:  6.931475304282055\n",
      "epoch\t 28 \tloss:  6.931613289899928\n",
      "epoch\t 28 \tloss:  6.931721444404559\n",
      "epoch\t 28 \tloss:  6.931468847561114\n",
      "epoch\t 29 \tloss:  6.931392958559203\n",
      "epoch\t 29 \tloss:  6.9314329270950035\n",
      "epoch\t 29 \tloss:  6.931548337146647\n",
      "epoch\t 30 \tloss:  6.9316017116887085\n",
      "epoch\t 30 \tloss:  6.9316453266775975\n",
      "epoch\t 30 \tloss:  6.931575004667052\n",
      "epoch\t 31 \tloss:  6.931638157992506\n",
      "epoch\t 31 \tloss:  6.931412536153598\n",
      "epoch\t 31 \tloss:  6.93164935615588\n",
      "epoch\t 32 \tloss:  6.931743230619314\n",
      "epoch\t 32 \tloss:  6.931609384319895\n",
      "epoch\t 32 \tloss:  6.931425701342005\n",
      "epoch\t 33 \tloss:  6.931640929811979\n",
      "epoch\t 33 \tloss:  6.931446461626493\n",
      "epoch\t 33 \tloss:  6.931374991151652\n",
      "epoch\t 34 \tloss:  6.931533450382025\n",
      "epoch\t 34 \tloss:  6.931491419740612\n",
      "epoch\t 34 \tloss:  6.931443040292873\n",
      "epoch\t 35 \tloss:  6.931459287373502\n",
      "epoch\t 35 \tloss:  6.9314135697601875\n",
      "epoch\t 35 \tloss:  6.931772864216378\n",
      "epoch\t 36 \tloss:  6.931620497658754\n",
      "epoch\t 36 \tloss:  6.931354231111713\n",
      "epoch\t 36 \tloss:  6.931624046332179\n",
      "epoch\t 37 \tloss:  6.931658999739164\n",
      "epoch\t 37 \tloss:  6.931274438638713\n",
      "epoch\t 37 \tloss:  6.931707515772277\n",
      "epoch\t 38 \tloss:  6.931500580801711\n",
      "epoch\t 38 \tloss:  6.931461991197629\n",
      "epoch\t 38 \tloss:  6.931710684464665\n",
      "epoch\t 39 \tloss:  6.931688988485504\n",
      "epoch\t 39 \tloss:  6.931510283294412\n",
      "epoch\t 39 \tloss:  6.931595949389388\n",
      "epoch\t 40 \tloss:  6.931494679101529\n",
      "epoch\t 40 \tloss:  6.931486161466715\n",
      "epoch\t 40 \tloss:  6.931256996083741\n",
      "epoch\t 41 \tloss:  6.931380432364419\n",
      "epoch\t 41 \tloss:  6.931564371297235\n",
      "epoch\t 41 \tloss:  6.931778997927964\n",
      "epoch\t 42 \tloss:  6.931535376764313\n",
      "epoch\t 42 \tloss:  6.931281917397385\n",
      "epoch\t 42 \tloss:  6.931709369163176\n",
      "epoch\t 43 \tloss:  6.931427414720407\n",
      "epoch\t 43 \tloss:  6.931487758013497\n",
      "epoch\t 43 \tloss:  6.931683090930839\n",
      "epoch\t 44 \tloss:  6.931535720309033\n",
      "epoch\t 44 \tloss:  6.931593093481387\n",
      "epoch\t 44 \tloss:  6.931523448960757\n",
      "epoch\t 45 \tloss:  6.931456147582513\n",
      "epoch\t 45 \tloss:  6.931508249282698\n",
      "epoch\t 45 \tloss:  6.931465651739052\n",
      "epoch\t 46 \tloss:  6.931552482977858\n",
      "epoch\t 46 \tloss:  6.931340575709798\n",
      "epoch\t 46 \tloss:  6.931685095820499\n",
      "epoch\t 47 \tloss:  6.9316804743333345\n",
      "epoch\t 47 \tloss:  6.9315738776663345\n",
      "epoch\t 47 \tloss:  6.931534302526104\n",
      "epoch\t 48 \tloss:  6.931511793080071\n",
      "epoch\t 48 \tloss:  6.931456199734239\n",
      "epoch\t 48 \tloss:  6.931467103392773\n",
      "epoch\t 49 \tloss:  6.930844465932363\n",
      "epoch\t 49 \tloss:  6.931636321129643\n",
      "epoch\t 49 \tloss:  6.931246760583931\n",
      "epoch\t 50 \tloss:  6.931597544284514\n",
      "epoch\t 50 \tloss:  6.931401074692311\n",
      "epoch\t 50 \tloss:  6.931618625431115\n"
     ]
    }
   ],
   "source": [
    "# set aside a small portion for validation\n",
    "np.random.seed(4133)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "x_test = x_train[0:10000, :]\n",
    "y_test = y_train[0:10000]\n",
    "x_train_log = x_train[10001:, :]\n",
    "y_train_log = y_train[10001:]\n",
    "\n",
    "# train using logistic regression (SGD)\n",
    "initial_w = np.random.rand(embeddings.shape[1])\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.05\n",
    "print_every = int(50000 / batch_size)\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train_log, x_train_log, initial_w, epochs, batch_size, gamma, lambda_, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on a local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5049\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_logistic_labels(weights, x_test)\n",
    "accuracy = get_accuracy(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test dataset, prepare submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logreg1.csv'\n",
    "\n",
    "y_submission = predict_logistic_labels(weights, x_submission)\n",
    "ids = np.arange(len(y_submission)) + 1\n",
    "\n",
    "create_csv_submission(ids, y_submission, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
