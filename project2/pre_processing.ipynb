{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = False\n",
    "cleaned_neg = False\n",
    "cleaned_neg_full = False\n",
    "cleaned_pos_full = False\n",
    "cleaned_test = False\n",
    "\n",
    "freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "freq_word = list(freq_word.text)\n",
    "# freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    into tresorpack difficulty of object disasembl...\n",
       "2         glad dot have take tomorrow thankful started\n",
       "3    vs celtic in the regular season were tucked if...\n",
       "4            could actual kill that girl i am so story\n",
       "5             find that very hard to believe in afraid\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    # interpret emoji\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/pre_processed/tweets_pos_removewords.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ad5d0e00e473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#correct words with textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT BLOB!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mtweets_neg_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_neg_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#     #lemmatize words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-ad5d0e00e473>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#correct words with textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT BLOB!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mtweets_neg_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_neg_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#     #lemmatize words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;31m# regex matches: word or punctuation or whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         '''\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         '''\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\en\\__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__iter__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__contains__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__getitem__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \"\"\" If the dictionary is empty, calls lazydict.load().\n\u001b[0;32m     82\u001b[0m             \u001b[0mReplaces\u001b[0m \u001b[0mlazydict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg_full :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg_full.index = tweets_neg_full.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' you ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos_full:\n",
    "    #read data .txt file\n",
    "    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' you ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 => test \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "    test_data['text'] = test_data['text'].str.replace(' ur ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' u ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' your ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' yourl ', ' ')\n",
    "\n",
    "            #remove punctuaction\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!!\")\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "\n",
    "        \n",
    "    #lemmatize words\n",
    "    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    \n",
    "    \n",
    "    test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_data_removewords.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/test_data_removewords.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text_lema']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_removewords.pickle\",\"rb\")\n",
    "test_data_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data .txt file\n",
    "# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos_full.drop_duplicates(inplace=True)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"Interpreting emojis!!\")\n",
    "#  # interpret emoji\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"Separating Numbers!!\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "# #     print(tweets_pos_full['text'].count())\n",
    "# print(\"Remove Hashtags!!\")\n",
    "# #remove hashtag #\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "# print(\"Replace ur by your!!\")\n",
    "# #replace ur by your\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions!!\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n",
    "# #remove punctuaction\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words!!\")\n",
    "# # remove words user, url, number\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"Replace more leeters!!\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data from .txt to preprocess\n",
    "# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "\n",
    "# #increase index to start at 1\n",
    "# tweets_neg_full.index = tweets_neg_full.index +1\n",
    "# #remove duplicates\n",
    "# tweets_neg_full.drop_duplicates(inplace=True)\n",
    "# test = tweets_neg_full.head(50)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"emoji\")\n",
    "# # interpret emoji\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"split numbers\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "# print(\"hastag\")\n",
    "# #remove hashtag #\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "# print(\"ur your\")\n",
    "# #replace ur by your\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n",
    "# print(\"punctuaction\")\n",
    "#     #remove punctuaction\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words\")\n",
    "# # remove words user, url, number\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"haaapy to haapy\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n",
    "\n",
    "# #remove stop words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n",
    "# combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove most frequent words \n",
    "# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "# freq_max_total = list(freq_max_total.index)\n",
    "\n",
    "# #remove least frequent words\n",
    "# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "# freq_min_total = list(freq_min_total.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_min_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words = freq_max_total+ freq_min_total\n",
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the file to pickle\n",
    "# print(\"Saving most frequent and lest frequent words\")\n",
    "# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n",
    "# pickle.dump(freq_words, pickle_out)\n",
    "# pickle_out.close()\n",
    "# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n",
    "# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n",
    "# f.close()\n",
    "# print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n",
    "# xxxxxx = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos.index = tweets_pos.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_full_pickle.str.contains('like').sum()\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_pos.replace(freq_words_20,' ', inplace=True)\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_pos.text.str.contains(' like ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n",
    "# tweets_neg_freqwords20 = pickle.load(pickle_in)\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
