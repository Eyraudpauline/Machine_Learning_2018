{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Andres\n",
      "[nltk_data]     Montero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = True\n",
    "cleaned_neg = True\n",
    "cleaned_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Trained!\n",
      "Opening pickle\n"
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    #separate number with letters  1234test123 => 1234 test 123\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    #filter digits\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: filter_digits(x))\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    # remove punctuaction ......=> \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[:10]\n",
    "    freq_max_neg = list(freq_max_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[-10:]\n",
    "    freq_min_neg = list(freq_min_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    pickle_in = open(\"data/pre_processed/tweets_neg.pickle\",\"rb\")\n",
    "    tweets_neg_pickle = pickle.load(pickle_in)\n",
    "    print(\"Opening pickle\")\n",
    "    tweets_neg_pickle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Trained!\n",
      "Opening pickle\n"
     ]
    }
   ],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    #separate number with letters  1234test123 => 1234 test 123\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    #filter digits\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: filter_digits(x))\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    # remove punctuaction ......=> \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[:10]\n",
    "    freq_max_pos = list(freq_max_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[-10:]\n",
    "    freq_min_pos = list(freq_min_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_pos['text_lema'] = tweets_pos['text'].apply(lemmatize_text)\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    pickle_in = open(\"data/pre_processed/tweets_pos.pickle\",\"rb\")\n",
    "    tweets_pos_pickle = pickle.load(pickle_in)\n",
    "    print(\"Opening pickle\")\n",
    "    tweets_pos_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Trained!\n",
      "Opening pickle\n"
     ]
    }
   ],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    #separate number with letters  1234test123 => 1234 test 123\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #filter digits\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: filter_digits(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    # remove punctuaction ......=> \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[:10]\n",
    "    freq_max_test = list(freq_max_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_test))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[-10:]\n",
    "    freq_min_test = list(freq_min_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_test))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_data.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    pickle_in = open(\"data/pre_processed/test_data.pickle\",\"rb\")\n",
    "    test_data_pickle = pickle.load(pickle_in)\n",
    "    print(\"Opening pickle\")\n",
    "    test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [into, tresorpack, difficulty, object, disasse...\n",
       "1        [glad, dot, take, tomorrow, thankful, started]\n",
       "2     [v, celtic, regular, season, tucked, play, pla...\n",
       "3                  [could, actually, kill, girl, sorry]\n",
       "4                     [find, hard, believe, in, afraid]\n",
       "5                         [wish, could, night, tonight]\n",
       "6                                     [got, kicked, wm]\n",
       "7                        [yes, tell, lip, closed, okay]\n",
       "8                                             [perfect]\n",
       "9     [hi, harry, have, good, time, a, didn, see, ma...\n",
       "10    [introduction, programming, +, +, nd, edition,...\n",
       "13                                           [white, a]\n",
       "14                   [dan, love, sad, wheresthegeneral]\n",
       "15               [many, wonderful, building, do, still]\n",
       "16                     [annoying, secretly, find, good]\n",
       "17    [post, boom, spanish, american, fiction, sun, ...\n",
       "18    [layer, heart, paperback, journey, inspired, r...\n",
       "20                       [guess, tested, want, u, back]\n",
       "21    [marrow, litter, offence, wilful, dispersed, p...\n",
       "22    [people, back's, ground, tonight, donna, see, ...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_neg_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [funny, austin, read, mention, austin, god, kn...\n",
       "1           [logic, dumb, even, crop, name, photo, ask]\n",
       "2         [put, caper, box, looked, battle, crakkbitch]\n",
       "3     [thanks, sir, trip, ll, mamma, keep, down, a, ...\n",
       "4     [visiting, brother, mr, beset, birthday, gift,...\n",
       "5     [may, lifecompleted, sweet, facebook, let, ple...\n",
       "6     [dnextalbumtitle, feel, rollercoaster, life, s...\n",
       "7     [working, hard, hardly, working, harder's, fut...\n",
       "8                                  [saw, replying, bit]\n",
       "9                                              [belong]\n",
       "10                               [and, cheer, national]\n",
       "11    [send, invitation, shop, line, find, everythin...\n",
       "12                             [woke, finn, go, church]\n",
       "13                             [agreed, day, left, the]\n",
       "14                                    [money, katemelo]\n",
       "15                [dam, alexis, got, lot, say, twitter]\n",
       "16    [grateful, today, dream, fulfilled, heart, ful...\n",
       "17                         [home, affair, shall, later]\n",
       "18      [back, bout, beat, real, madrid, saturday, doe]\n",
       "19    [lot, part, asia, especially, rat, live, count...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pos_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     [sea, do, pro, sea, sooner, sport, portable, s...\n",
       "2     [suck, well, work, week, come, cheer, oh, put,...\n",
       "3                    [can, stay, away, bug, that, baby]\n",
       "4     [madam, in, perfectly, fine, contagious, anymo...\n",
       "5     [whenever, fall, asleep, watching, to, always,...\n",
       "6     [need, get, rid, thing, scar, need, car, eithe...\n",
       "7                            [whatever, terrible, mood]\n",
       "8             [yes, thanks, jordan, donna, call, later]\n",
       "9                    [friend, text, check, last, night]\n",
       "10    [followback, please, unitytoyour, come, europe...\n",
       "11         [watch, dumb, mass, get, lock, today, happy]\n",
       "12    [obsessed, phase, killed, best, album, ever, n...\n",
       "13    [robert, de, no, gay, name, vewy, would, under...\n",
       "14              [canada, grade, since, grade, th, suck]\n",
       "15     [please, say, hi, denmark, would, amazing, live]\n",
       "16                                      [finally, home]\n",
       "17    [custom, picture, poster, wide, complete, gold...\n",
       "18            [new, follower, mention, followback, too]\n",
       "19    [yep, look, best, team, stay, proper, form, se...\n",
       "20    [nl's, bettah, suspension, criticism, gamesman...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextBlob(\"bout to be!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
