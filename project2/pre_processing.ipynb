{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = False\n",
    "cleaned_neg = False\n",
    "cleaned_neg_full = True\n",
    "cleaned_pos_full = True\n",
    "cleaned_test = False\n",
    "\n",
    "freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "freq_word = list(freq_word.text)\n",
    "# freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "numbers!!\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "TEXT BLOB!!\n"
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    # interpret emoji\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!\")\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "#     #lemmatize words\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/pre_processed/tweets_pos_removewords.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg_full :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg_full.index = tweets_neg_full.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg_full.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"split numbers\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastag\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "\n",
    "    print(\"expand contractions\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' your ', ' ')\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' ')\n",
    "    print(\"punctuaction\")\n",
    "        #remove punctuaction\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"remove words\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"haaapy to haapy\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #correct words with textblob\n",
    "    print(\"Correcting with textblob\")\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    print(\"lemmatizing\")\n",
    "    #lemmatize words\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "   \n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n",
    "#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg_full))\n",
    "    \n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_removewords.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"rb\")\n",
    "tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos_full:\n",
    "    #read data .txt file\n",
    "    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"Interpreting emojis!!\")\n",
    "     # interpret emoji\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"Separating Numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(tweets_pos_full['text'].count())\n",
    "    print(\"Remove Hashtags!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "    print(\"Replace ur by your!!\")\n",
    "    \n",
    "    print(\"expand contractions!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' we ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', ' ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', ' ')\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' your ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', ' ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', ' ')\n",
    "    #remove punctuaction\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"remove words!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"Replace more leeters!!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    \n",
    "#     #remove stop_words\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "     \n",
    "    \n",
    "    \n",
    "#     print(\"Remove most frequent words!!\")\n",
    "#     #remove most frequent words \n",
    "#     freq_max_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[:10]\n",
    "#     freq_max_pos_full = list(freq_max_pos_full.index)\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos_full))\n",
    "# #     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n",
    "    \n",
    "#     print(\"Remove least frequent words!!\")\n",
    "#     #remove least frequent words\n",
    "#     freq_min_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[-10:]\n",
    "#     freq_min_pos_full = list(freq_min_pos_full.index)\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    print(\"Lemmatizing!!\")\n",
    "    #lemmatize words\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "   \n",
    "    print(\"joining after lematize\")\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    print(\"removing commas \")\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n",
    "#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_removewords.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_removewords.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_removewords.pickle\",\"rb\")\n",
    "tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 => test \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "    test_data['text'] = test_data['text'].str.replace(' ur ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' u ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' your ', ' ')\n",
    "    test_data['text'] = test_data['text'].str.replace(' yourl ', ' ')\n",
    "\n",
    "            #remove punctuaction\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "    #correct words with textblob\n",
    "    print(\"TEXT BLOB!!!\")\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "\n",
    "        \n",
    "    #lemmatize words\n",
    "    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    \n",
    "    \n",
    "    test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_data_removewords.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    f = open(\"data/pre_processed/test_data_removewords.txt\", \"w\", encoding='utf-8')\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text_lema']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_removewords.pickle\",\"rb\")\n",
    "test_data_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data .txt file\n",
    "# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos_full.drop_duplicates(inplace=True)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"Interpreting emojis!!\")\n",
    "#  # interpret emoji\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"Separating Numbers!!\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "# #     print(tweets_pos_full['text'].count())\n",
    "# print(\"Remove Hashtags!!\")\n",
    "# #remove hashtag #\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "# print(\"Replace ur by your!!\")\n",
    "# #replace ur by your\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions!!\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n",
    "# #remove punctuaction\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words!!\")\n",
    "# # remove words user, url, number\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"Replace more leeters!!\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data from .txt to preprocess\n",
    "# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "\n",
    "# #increase index to start at 1\n",
    "# tweets_neg_full.index = tweets_neg_full.index +1\n",
    "# #remove duplicates\n",
    "# tweets_neg_full.drop_duplicates(inplace=True)\n",
    "# test = tweets_neg_full.head(50)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"emoji\")\n",
    "# # interpret emoji\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"split numbers\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "# print(\"hastag\")\n",
    "# #remove hashtag #\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "# print(\"ur your\")\n",
    "# #replace ur by your\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n",
    "# print(\"punctuaction\")\n",
    "#     #remove punctuaction\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words\")\n",
    "# # remove words user, url, number\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"haaapy to haapy\")\n",
    "# # replace more letters haaaaaaaaaappy => happy\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n",
    "\n",
    "# #remove stop words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n",
    "# combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove most frequent words \n",
    "# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "# freq_max_total = list(freq_max_total.index)\n",
    "\n",
    "# #remove least frequent words\n",
    "# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "# freq_min_total = list(freq_min_total.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_min_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words = freq_max_total+ freq_min_total\n",
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the file to pickle\n",
    "# print(\"Saving most frequent and lest frequent words\")\n",
    "# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n",
    "# pickle.dump(freq_words, pickle_out)\n",
    "# pickle_out.close()\n",
    "# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n",
    "# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n",
    "# f.close()\n",
    "# print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n",
    "# xxxxxx = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos.index = tweets_pos.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_full_pickle.str.contains('like').sum()\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_pos.replace(freq_words_20,' ', inplace=True)\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_pos.text.str.contains(' like ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n",
    "# tweets_neg_freqwords20 = pickle.load(pickle_in)\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
