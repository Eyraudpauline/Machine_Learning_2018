{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Andres\n",
      "[nltk_data]     Montero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = False\n",
    "cleaned_neg = False\n",
    "cleaned_test = False\n",
    "cleaned_neg_full = False\n",
    "cleaned_pos_full = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack difficulty object disassemble...\n",
       "2         glad dot have taks tomorrow thankful startho\n",
       "3    v celtic regular season were fucked if we play...\n",
       "4          could actually kill that girl i am so sorry\n",
       "5                find that very hard believe im afraid\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "        #remove punctuaction\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[:10]\n",
    "    freq_max_neg = list(freq_max_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg))\n",
    "    \n",
    "#     remove least frequent words\n",
    "    freq_min_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[-10:]\n",
    "    freq_min_neg = list(freq_min_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_neg_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_wstp.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90233\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    dunno justin read mention or only justin god k...\n",
       "2    because yoyour logic so dumb will even crop ou...\n",
       "3      just put casper in box looved battle crakkbitch\n",
       "4    thanks sir do trip lil mama just keep doin ya ...\n",
       "5    visiting brother tmr bestest birthday gift eveerr\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    print(tweets_pos['text'].count())\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "        #remove punctuaction\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[:10]\n",
    "    freq_max_pos = list(freq_max_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[-10:]\n",
    "    freq_min_pos = list(freq_min_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_pos['text_lema'] = tweets_pos['text'].apply(lemmatize_text)\n",
    "    tweets_pos['text_lema'] = tweets_pos['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_pos['text_lema'] = tweets_pos['text_lema'].str.replace(',','')\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_pos_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_wstp.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     vinco tresorpack difficulty object disassemble...\n",
       "2          glad dot have taks tomorrow thankful startho\n",
       "3     v celtic regular season were fucked if we play...\n",
       "4           could actually kill that girl i am so sorry\n",
       "5                 find that very hard believe im afraid\n",
       "6                   wish could be out all night tonight\n",
       "7                                    got kicked out wgm\n",
       "8                rt yes she tell it lip are closed okay\n",
       "9                                    why she so perfect\n",
       "10    hi harry did havea good time au didnt get see ...\n",
       "11    introduction programming with + + nd edition t...\n",
       "14                                        i am white aw\n",
       "15             dan love miss do be sad wheresthegeneral\n",
       "16         so many wonderful building dc but still miss\n",
       "17         it annoying because secretly find it so good\n",
       "18    post boom spanish american fiction suny series...\n",
       "19    layer heart paperback this joyourney wa inspir...\n",
       "21                guess who texted me again want u back\n",
       "22    farrow litter offence wilful dispersal piglet ...\n",
       "23    people at barca's ground tonight are gonna see...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_neg_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     dunno justin read mention or only justin god k...\n",
       "2     because yoyour logic so dumb will even crop ou...\n",
       "3       just put casper in box looved battle crakkbitch\n",
       "4     thanks sir do trip lil mama just keep doin ya ...\n",
       "5     visiting brother tmr bestest birthday gift eveerr\n",
       "6      yay lifecompleted tweet facebook let know please\n",
       "7     dnextalbumtitle feel rollercoaster of life son...\n",
       "8     workin hard or hardly workin rt at hardee's wi...\n",
       "9                         saw i will be replying in bit\n",
       "10                                     this were belong\n",
       "11                                  andd cheer national\n",
       "12    we send an invitation shop on line here will f...\n",
       "13                       just woke up finna go chyourch\n",
       "14                             agreed more day left tho\n",
       "15                                  monet with katemelo\n",
       "16    like damm lexis got lot say when your on twitt...\n",
       "17    grateful today dream fulfilled heart so full f...\n",
       "18                        at home affair shall do later\n",
       "19        barca bout beat real madrid on satyourday doe\n",
       "20    lot of part of asia especially rat that live i...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pos_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n"
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg_full :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg_full.index = tweets_neg_full.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg_full.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"emoji\")\n",
    "    # interpret emoji\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"split numbers\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "    print(\"hastag\")\n",
    "    #remove hashtag #\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "    print(\"ur your\")\n",
    "    #replace ur by your\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "    print(\"expand contractions\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    print(\"punctuaction\")\n",
    "        #remove punctuaction\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"remove words\")\n",
    "    # remove words user, url, number\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"haaapy to haapy\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    print(\"frequent words\")\n",
    "    #remove most frequent words \n",
    "    freq_max_neg_full = pd.Series(' '.join(tweets_neg_full['text']).split()).value_counts()[:10]\n",
    "    freq_max_neg_full = list(freq_max_neg_full.index)\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg_full))\n",
    "    \n",
    "    print(\"least frequent words\")\n",
    "    #remove least frequent words\n",
    "    freq_min_neg_full = pd.Series(' '.join(tweets_neg_full['text']).split()).value_counts()[-10:]\n",
    "    freq_min_neg_full = list(freq_min_neg_full.index)\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg_full))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    print(\"lemmatizing\")\n",
    "    #lemmatize words\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_wstp.pickle\",\"rb\")\n",
    "tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos_full:\n",
    "    #read data .txt file\n",
    "    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(\"Interpreting emojis!!\")\n",
    "     # interpret emoji\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    print(\"Separating Numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(tweets_pos_full['text'].count())\n",
    "    print(\"Remove Hashtags!!\")\n",
    "    #remove hashtag #\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "    print(\"Replace ur by your!!\")\n",
    "    #replace ur by your\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "    print(\"expand contractions!!\")\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    #remove punctuaction\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "    print(\"remove words!!\")\n",
    "    # remove words user, url, number\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "    print(\"Replace more leeters!!\")\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    print(\"Remove most frequent words!!\")\n",
    "    #remove most frequent words \n",
    "    freq_max_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[:10]\n",
    "    freq_max_pos_full = list(freq_max_pos_full.index)\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos_full))\n",
    "    \n",
    "    print(\"Remove least frequent words!!\")\n",
    "    #remove least frequent words\n",
    "    freq_min_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[-10:]\n",
    "    freq_min_pos_full = list(freq_min_pos_full.index)\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    print(\"Lemmatizing!!\")\n",
    "    #lemmatize words\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    print(\"joining after lematize\")\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    print(\"removing commas \")\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_wstp.pickle\",\"rb\")\n",
    "tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 => test \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "        #remove punctuaction\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words from poss_full\n",
    "    freq_max_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[:10]\n",
    "    freq_max_test = list(freq_max_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos_full))\n",
    "    \n",
    "    #remove most frequent words from neg_full\n",
    "    freq_max_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[:10]\n",
    "    freq_max_test = list(freq_max_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg_full))\n",
    "    \n",
    "    \n",
    "    #remove least frequent words from poss_full\n",
    "    freq_min_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[-10:]\n",
    "    freq_min_test = list(freq_min_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n",
    "    \n",
    "     #remove least frequent words from poss_full\n",
    "    freq_min_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[-10:]\n",
    "    freq_min_test = list(freq_min_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg_full))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "    index = test_data.index.to_series()\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_data_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/test_data_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text_lema']))))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_wstp.pickle\",\"rb\")\n",
    "test_data_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
