{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Set....\n",
      "Data Set loaded !\n"
     ]
    }
   ],
   "source": [
    "#Load Data Sets\n",
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "print(\"Loading Data Set....\")\n",
    "tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_test = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "tweets_test.drop(columns=[1,2], inplace=True)\n",
    "tweets_test.rename(columns={0:'text'}, inplace= True)\n",
    "\n",
    "    \n",
    "# if not cleaned_test:  \n",
    "#     #read data from .txt\n",
    "#     test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', )\n",
    "#     #index start at 1\n",
    "#     test_data.index = test_data.index +1\n",
    "#     #remove duplicates\n",
    "#     test_data.drop_duplicates(inplace=True)\n",
    "#     test_data.drop(columns=[1,2], inplace=True)\n",
    "#     test_data.rename(columns={0:'text'}, inplace= True)\n",
    "print(\"Data Set loaded !\")\n",
    "\n",
    "\n",
    "#bool variables to clean data\n",
    "#If set to False the cleaning process will take place\n",
    "already_cleaned_neg = False\n",
    "already_cleaned_pos = False\n",
    "already_cleaned_neg_full = True\n",
    "already_cleaned_pos_full = True\n",
    "already_cleaned_test = False\n",
    "\n",
    "#variable to define if saved the lemmatized text or not, if set to false will save column with no lemmatize\n",
    "lemmatize = False\n",
    "\n",
    "\n",
    "# freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "# freq_word = list(freq_word.text)\n",
    "# # freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from .txt\n",
    "test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "#index start at 1\n",
    "test_data.index = test_data.index +1\n",
    "#remove duplicates\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "test_data.drop(columns=[1,2], inplace=True)\n",
    "test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"Function to clean the data.\n",
    "\n",
    "    Args:\n",
    "        data: data to clean\n",
    "        \n",
    "    Returns:\n",
    "        data cleaned\n",
    "    \"\"\"\n",
    "    #increase index to start at 1\n",
    "    data.index = data.index +1\n",
    "    \n",
    "    #remove duplicates\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    data['text'] = data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    \n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    data['text'] = data['text'].str.replace('#', '')\n",
    "    \n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    data['text'] = data['text'].apply(lambda x: expand_contractions(x))\n",
    "    \n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    data['text'] = data['text'].str.replace(' ur ', ' your ')\n",
    "    data['text'] = data['text'].str.replace(' u ', ' you ')\n",
    "    data['text'] = data['text'].str.replace(' cant ', ' can not ')\n",
    "    data['text'] = data['text'].str.replace(' yourl ', ' your ')\n",
    "    data['text'] = data['text'].str.replace(' lol ', ' laugh ')\n",
    "    \n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    data['text'] = data['text'].apply(lambda x: split_number_text(x))\n",
    "   \n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    data['text'] = data['text'].apply(lambda x: remove_punctuation(x))\n",
    "    \n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    data['text'] = data['text'].apply(lambda x: remove_words(x))\n",
    "    \n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => haappy\n",
    "    data['text'] = data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    print(\"lemmatizing!\")\n",
    "        #lemmatize words\n",
    "    data['text_lema'] = data['text'].apply(lemmatize_text)\n",
    "    data['text_lema'] = data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    data['text_lema'] = data['text_lema'].str.replace(',','')\n",
    "    return  data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Neg\n",
      "emoji\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "numbers!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "lemmatizing!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack difficulty of object disassemble and reassemble the wooden pieces this beautiful wo\n",
       "2    glad dot have taks tomorrow thankful startho                                                        \n",
       "3    vs celtics in the regular season were fucked if we play them in the playoffs                        \n",
       "4    could actually kill that girl am so sorry                                                           \n",
       "5    find that very hard to believe im afraid                                                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_neg :\n",
    "    print(\"Cleaning Tweets Neg\")\n",
    "    clean_data(tweets_neg).head()\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_neg_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Pos Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Pos\n",
      "emoji\n"
     ]
    }
   ],
   "source": [
    "if not already_cleaned_pos :\n",
    "    print(\"Cleaning Tweets Pos\")\n",
    "    clean_data(tweets_pos).head()\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_pos['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_pos_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Neg Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already_cleaned_neg_full :\n",
    "#     print(\"Cleaning Tweets Neg Full\")\n",
    "#     clean_data(tweets_neg_full).head()\n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_full_no_textblob.pickle\",\"wb\")\n",
    "#     if not lemmatize:\n",
    "#         pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "#     else:\n",
    "#         pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     #save file to text\n",
    "#     f = open(\"data/pre_processed/tweets_neg_full_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     if not lemmatize:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "#     else:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Tweets Neg Full Already Cleaned!\")\n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_full_no_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already_cleaned_pos_full :\n",
    "#     print(\"Cleaning Tweets Pos Full\")\n",
    "#     clean_data(tweets_pos_full).head()\n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_full_no_textblob.pickle\",\"wb\")\n",
    "#     if not lemmatize:\n",
    "#         pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "#     else:\n",
    "#         pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     #save file to text\n",
    "#     f = open(\"data/pre_processed/tweets_pos_full_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     if not lemmatize:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "#     else:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Tweets Pos Full Already Cleaned!\")\n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_full_no_textblob.pickle\",\"rb\")\n",
    "# tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_pos_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_cleaned_test :\n",
    "    tweets_test.index = tweets_test.index +1\n",
    "    print(\"Cleaning Test\")\n",
    "    clean_data(tweets_test).head()\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    #save to pickle\n",
    "    pickle_out = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_test['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_test['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save to txt\n",
    "    f = open(\"data/pre_processed/test_data_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text']))))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text_lema']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Test Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"rb\")\n",
    "tweets_test_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_test_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #verify  flag for data already cleaned\n",
    "# if not cleaned_neg :\n",
    "#     #read data from .txt to preprocess\n",
    "#     tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "    \n",
    "#     #increase index to start at 1\n",
    "#     tweets_neg.index = tweets_neg.index +1\n",
    "#     #remove duplicates\n",
    "#     tweets_neg.drop_duplicates(inplace=True)\n",
    "#     test = tweets_neg.head(50)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "# #     print(\"TEXT BLOB!!\")\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_neg\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# # #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_neg['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_neg_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_pos:\n",
    "#     #read data .txt file\n",
    "#     tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "#     # index start at 1\n",
    "#     tweets_pos.index = tweets_pos.index + 1\n",
    "#     #remove duplicates\n",
    "#     tweets_pos.drop_duplicates(inplace=True)\n",
    "    \n",
    "#     #apply pre_process funtioncs\n",
    "#     # interpret emoji\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "# #     print(\"TEXT BLOB!!\")\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# # #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_pos['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_pos_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"rb\")\n",
    "# tweets_pos_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #verify  flag for data already cleaned\n",
    "# if not cleaned_neg_full :\n",
    "#     #read data from .txt to preprocess\n",
    "#     tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "    \n",
    "#     #increase index to start at 1\n",
    "#     tweets_neg_full.index = tweets_neg_full.index +1\n",
    "#     #remove duplicates\n",
    "#     tweets_neg_full.drop_duplicates(inplace=True)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!\")\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_neg_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_pos_full:\n",
    "#     #read data .txt file\n",
    "#     tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "#     # index start at 1\n",
    "#     tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "#     #remove duplicates\n",
    "#     tweets_pos_full.drop_duplicates(inplace=True)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!\")\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_neg\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_pos_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_test:  \n",
    "#     #read data from .txt\n",
    "#     test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "#     #index start at 1\n",
    "#     test_data.index = test_data.index +1\n",
    "#     #remove duplicates\n",
    "#     test_data.drop_duplicates(inplace=True)\n",
    "#     test_data.drop(columns=[1,2], inplace=True)\n",
    "#     test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "#     #apply pre_process funtioncs\n",
    "#     #remove stop words\n",
    "# #     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#     # interpret emoji\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     #separate number with letters  1234test123 => test \n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "#     #remove hashtag #\n",
    "#     test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "#     #replace ur by your\n",
    "#     test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "#     #expand contractions don't => do not\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "#     test_data['text'] = test_data['text'].str.replace(' ur ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' u ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' your ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' yourl ', ' ')\n",
    "\n",
    "#             #remove punctuaction\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "#     # remove words user, url, number\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "#     # replace more letters haaaaaaaaaapy => haapy\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!!\")\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_pos\n",
    "    \n",
    "\n",
    "        \n",
    "#     #lemmatize words\n",
    "# #     test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    \n",
    "    \n",
    "# #     test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(test_data['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/test_data_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text']))))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "# pickle_in = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"rb\")\n",
    "# test_data_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data .txt file\n",
    "# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos_full.drop_duplicates(inplace=True)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"Interpreting emojis!!\")\n",
    "#  # interpret emoji\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"Separating Numbers!!\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "# #     print(tweets_pos_full['text'].count())\n",
    "# print(\"Remove Hashtags!!\")\n",
    "# #remove hashtag #\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "# print(\"Replace ur by your!!\")\n",
    "# #replace ur by your\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions!!\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n",
    "# #remove punctuaction\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words!!\")\n",
    "# # remove words user, url, number\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"Replace more leeters!!\")\n",
    "# # replace more letters haaaaaaaaaappy => haappy\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data from .txt to preprocess\n",
    "# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "\n",
    "# #increase index to start at 1\n",
    "# tweets_neg_full.index = tweets_neg_full.index +1\n",
    "# #remove duplicates\n",
    "# tweets_neg_full.drop_duplicates(inplace=True)\n",
    "# test = tweets_neg_full.head(50)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"emoji\")\n",
    "# # interpret emoji\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"split numbers\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "# print(\"hastag\")\n",
    "# #remove hashtag #\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "# print(\"ur your\")\n",
    "# #replace ur by your\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n",
    "# print(\"punctuaction\")\n",
    "#     #remove punctuaction\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words\")\n",
    "# # remove words user, url, number\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"haaapy to haapy\")\n",
    "# # replace more letters haaaaaaaaaappy => haappy\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n",
    "\n",
    "# #remove stop words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n",
    "# combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove most frequent words \n",
    "# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "# freq_max_total = list(freq_max_total.index)\n",
    "\n",
    "# #remove least frequent words\n",
    "# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "# freq_min_total = list(freq_min_total.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_min_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words = freq_max_total+ freq_min_total\n",
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the file to pickle\n",
    "# print(\"Saving most frequent and lest frequent words\")\n",
    "# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n",
    "# pickle.dump(freq_words, pickle_out)\n",
    "# pickle_out.close()\n",
    "# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n",
    "# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n",
    "# f.close()\n",
    "# print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n",
    "# xxxxxx = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos.index = tweets_pos.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_full_pickle.str.contains('like').sum()\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_pos.replace(freq_words_20,' ', inplace=True)\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_pos.text.str.contains(' like ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n",
    "# tweets_neg_freqwords20 = pickle.load(pickle_in)\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
