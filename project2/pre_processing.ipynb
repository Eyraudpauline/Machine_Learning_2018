{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Andres\n",
      "[nltk_data]     Montero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "cleaned_pos = False\n",
    "cleaned_neg = False\n",
    "cleaned_test = False\n",
    "cleaned_pos_full = True\n",
    "cleaned_neg_full = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack difficulty of object disassem...\n",
       "2         glad dot have taks tomorrow thankful startho\n",
       "3    v celtic in regular season were fucked if we p...\n",
       "4          could actually kill that girl i am so sorry\n",
       "5                find that very hard believe im afraid\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg.index = tweets_neg.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_neg['text'] = tweets_neg['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[:10]\n",
    "    freq_max_neg = list(freq_max_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_neg = pd.Series(' '.join(tweets_neg['text']).split()).value_counts()[-10:]\n",
    "    freq_min_neg = list(freq_min_neg.index)\n",
    "    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_neg_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_wstp.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90233\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    dunno justin read mention or not only justin g...\n",
       "2    because yoyour logic so dumb will not even cro...\n",
       "3      just put casper in box looved battle crakkbitch\n",
       "4    thanks sir do not trip lil mama .. just keep d...\n",
       "5    visiting brother tmr bestest birthday gift eveerr\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not cleaned_pos:\n",
    "    #read data .txt file\n",
    "    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos.index = tweets_pos.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "    print(tweets_pos['text'].count())\n",
    "    #remove hashtag #\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_pos['text'] = tweets_pos['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[:10]\n",
    "    freq_max_pos = list(freq_max_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_pos = pd.Series(' '.join(tweets_pos['text']).split()).value_counts()[-10:]\n",
    "    freq_min_pos = list(freq_min_pos.index)\n",
    "    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_pos['text_lema'] = tweets_pos['text'].apply(lemmatize_text)\n",
    "    tweets_pos['text_lema'] = tweets_pos['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_pos['text_lema'] = tweets_pos['text_lema'].str.replace(',','')\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_pos_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_wstp.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    sea doo pro sea scooter sport with portable se...\n",
       "2    shuck well work all week so now can not come c...\n",
       "3              i cant stay away from bug thats my baby\n",
       "4    no madam lol im perfectly fine not contagious ...\n",
       "5    whenever fall asleep watching tv always wake u...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not cleaned_test:  \n",
    "    #read data from .txt\n",
    "    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "    #index start at 1\n",
    "    test_data.index = test_data.index +1\n",
    "    #remove duplicates\n",
    "    test_data.drop_duplicates(inplace=True)\n",
    "    test_data.drop(columns=[1,2], inplace=True)\n",
    "    test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 => test \n",
    "    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "    # remove words user, url, number\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaapy => hapy\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[:10]\n",
    "    freq_max_test = list(freq_max_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_test))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_test = pd.Series(' '.join(test_data['text']).split()).value_counts()[-10:]\n",
    "    freq_min_test = list(freq_min_test.index)\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_test))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "    index = test_data.index.to_series()\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/test_data_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(test_data['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/test_data_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text_lema']))))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_wstp.pickle\",\"rb\")\n",
    "test_data_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     vinco tresorpack difficulty of object disassem...\n",
       "2          glad dot have taks tomorrow thankful startho\n",
       "3     v celtic in regular season were fucked if we p...\n",
       "4           could actually kill that girl i am so sorry\n",
       "5                 find that very hard believe im afraid\n",
       "6                   wish could be out all night tonight\n",
       "7                                    got kicked out wgm\n",
       "8             rt yes she tell it my lip are closed okay\n",
       "9                                    why she so perfect\n",
       "10    hi harry did havea good time in au didnt get s...\n",
       "11    introduction programming with + + nd edition t...\n",
       "14                                        i am white aw\n",
       "15             dan love miss do be sad wheresthegeneral\n",
       "16      so many wonderful building in dc but still miss\n",
       "17         it annoying because secretly find it so good\n",
       "18    post-boom in spanish american fiction suny ser...\n",
       "19    layer of heart paperback this joyourney wa ins...\n",
       "21                guess who texted me again want u back\n",
       "22    farrow litter offence wilful dispersal of pigl...\n",
       "23    people at barca's ground tonight are gonna see...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_neg_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     dunno justin read mention or not only justin g...\n",
       "2     because yoyour logic so dumb will not even cro...\n",
       "3       just put casper in box looved battle crakkbitch\n",
       "4     thanks sir do not trip lil mama .. just keep d...\n",
       "5     visiting brother tmr bestest birthday gift eveerr\n",
       "6      yay lifecompleted tweet facebook let know please\n",
       "7     dnextalbumtitle feel for rollercoaster of life...\n",
       "8     workin hard or hardly workin rt at hardee's wi...\n",
       "9                         saw i will be replying in bit\n",
       "10                                     this were belong\n",
       "11                                  andd cheer national\n",
       "12    we send an invitation shop on-line here will f...\n",
       "13                       just woke up finna go chyourch\n",
       "14                             agreed more day left tho\n",
       "15                                  monet with katemelo\n",
       "16    like damm lexis got lot say when your on twitt...\n",
       "17    grateful today for dream fulfilled heart so fu...\n",
       "18                        at home affair shall do later\n",
       "19        barca bout beat real madrid on satyourday doe\n",
       "20    lot of part of asia especially rat that live i...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pos_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     sea doo pro sea scooter sport with portable se...\n",
       "2     shuck well work all week so now can not come c...\n",
       "3               i cant stay away from bug thats my baby\n",
       "4     no madam lol im perfectly fine not contagious ...\n",
       "5     whenever fall asleep watching tv always wake u...\n",
       "6     he need get rid of that thing it scare me lol ...\n",
       "7                         its whatever in terrible mood\n",
       "8      yess rt thanks jordan love i am gonna call later\n",
       "9           my friend text me check up on me last night\n",
       "10    followback please when will your unitytoyour c...\n",
       "11    watch some of all dumb ass get lock up today h...\n",
       "12    obsessed with phasell killed it best album eve...\n",
       "13    robert de niro not gay but with name like lewy...\n",
       "14    canada have do it in grade but since we do not...\n",
       "15    please say hi denmark that would be amazing li...\n",
       "16                                  finally am home now\n",
       "17    custom pictyoure frame poster frame wide compl...\n",
       "18      s my new follower mention me for followback boo\n",
       "19    yep look like best team will stay up proper on...\n",
       "20    nhl's bettman suspension criticism gamesmanshi...\n",
       "Name: text_lema, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_pickle.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        sea doo pro sea scooter sport with portable se...\n",
       "2        shuck well work all week so now can not come c...\n",
       "3                  i cant stay away from bug thats my baby\n",
       "4        no madam lol im perfectly fine not contagious ...\n",
       "5        whenever fall asleep watching tv always wake u...\n",
       "6        he need get rid of that thing it scare me lol ...\n",
       "7                            its whatever in terrible mood\n",
       "8         yess rt thanks jordan love i am gonna call later\n",
       "9              my friend text me check up on me last night\n",
       "10       followback please when will your unitytoyour c...\n",
       "11       watch some of all dumb ass get lock up today h...\n",
       "12       obsessed with phasell killed it best album eve...\n",
       "13       robert de niro not gay but with name like lewy...\n",
       "14       canada have do it in grade but since we do not...\n",
       "15       please say hi denmark that would be amazing li...\n",
       "16                                     finally am home now\n",
       "17       custom pictyoure frame poster frame wide compl...\n",
       "18         s my new follower mention me for followback boo\n",
       "19       yep look like best team will stay up proper on...\n",
       "20       nhl's bettman suspension criticism gamesmanshi...\n",
       "21       no way i am pro andd lool she rubbish also see...\n",
       "22       barrel pickle pound these german pickle are cr...\n",
       "23       i completed whole cup of tea today at jessica'...\n",
       "24       expo precision point whiteboard eraser <smile>...\n",
       "25       yougetmajorpointsif play soccer idc about any ...\n",
       "26       task essential harness leather scribe keeper s...\n",
       "27       needtoreesthisshit shouldhavereaditwithaglasso...\n",
       "28       hahahaha ok i will come over ton of blanket ca...\n",
       "29       layourie's garden design protective decal skin...\n",
       "30                                          no way he mine\n",
       "                               ...                        \n",
       "9971     it clear that these new goth are learning danc...\n",
       "9972     it clear that these new goth are learning danc...\n",
       "9973     another day at home sick can this be over already\n",
       "9974     of yoyour own wee golden nugget there simmey q...\n",
       "9975     all disney movie are classic film they will al...\n",
       "9976     custom pictyoure frame poster frame wide compl...\n",
       "9977     custom pictyoure frame poster frame wide compl...\n",
       "9978                                glad it arrived safely\n",
       "9979     im not gonna care if your not going either i a...\n",
       "9980     monk season foyour dvd retyourn scene of crime...\n",
       "9981     i want wish my beautiful cousin happy th birth...\n",
       "9982                    rt first rule of sex there no rule\n",
       "9983     stay a are darla rt know hahaha rt rani rt yes...\n",
       "9984     most embarrassing thing thats ever happened me...\n",
       "9985     i wonder if will ever followback probably not ...\n",
       "9986     i wonder if will ever followback probably not ...\n",
       "9987     anaheim duck cake cupcake topper pack quality ...\n",
       "9988                            shoutout go follow her guy\n",
       "9989     good morning brother noon in england hav great...\n",
       "9990     you are welcome mr.bruce tell him kun rt he sa...\n",
       "9991                                 follow me back please\n",
       "9992     omg screamed at end that amazing are beautiful...\n",
       "9993     aww that dissapointing that can not make my ow...\n",
       "9994                                          thankyouu po\n",
       "9995               i am afraid are coming back lot of rain\n",
       "9996                      had nice time my friend lastnite\n",
       "9997                                 no it not please stop\n",
       "9998     not without my daughter dvd two-time oscar win...\n",
       "9999                         have fun in class sweetcheeks\n",
       "10000    making difference get recreational leadership ...\n",
       "Name: text_lema, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Trained!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/pre_processed/tweets_neg_full_wstp.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-408698d11abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Already Trained!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/pre_processed/tweets_neg_full_wstp.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[0mtweets_neg_full_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Opening pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/pre_processed/tweets_neg_full_wstp.pickle'"
     ]
    }
   ],
   "source": [
    "#verify  flag for data already cleaned\n",
    "if not cleaned_neg_full :\n",
    "    #read data from .txt to preprocess\n",
    "    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    \n",
    "    #increase index to start at 1\n",
    "    tweets_neg_full.index = tweets_neg_full.index +1\n",
    "    #remove duplicates\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    test = tweets_neg_full.head(50)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    # interpret emoji\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "    #remove hashtag #\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_neg = pd.Series(' '.join(tweets_neg_full['text']).split()).value_counts()[:10]\n",
    "    freq_max_neg = list(freq_max_neg.index)\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_neg))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_neg = pd.Series(' '.join(tweets_neg_full['text']).split()).value_counts()[-10:]\n",
    "    freq_min_neg = list(tweets_neg_full.index)\n",
    "    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_neg\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "    \n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "    \n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_wstp.pickle\",\"rb\")\n",
    "tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cleaned_pos_full:\n",
    "    #read data .txt file\n",
    "    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "    # index start at 1\n",
    "    tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "    #remove duplicates\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    #apply pre_process funtioncs\n",
    "    #remove stop words\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "     # interpret emoji\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(tweets_pos_full['text'].count())\n",
    "    #remove hashtag #\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "    #replace ur by your\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "    #expand contractions don't => do not\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "    # remove words user, url, number\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "    # replace more letters haaaaaaaaaappy => happy\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    #remove most frequent words \n",
    "    freq_max_pos = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[:10]\n",
    "    freq_max_pos = list(freq_max_pos.index)\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos))\n",
    "    \n",
    "    #remove least frequent words\n",
    "    freq_min_pos = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[-10:]\n",
    "    freq_min_pos = list(freq_min_pos.index)\n",
    "    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos))\n",
    "\n",
    "    #correct words with textblob\n",
    "    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    # tweets_pos\n",
    "    \n",
    "    #lemmatize words\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_wstp.pickle\",\"wb\")\n",
    "    pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(\"Saved!\")\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_wstp.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Already Trained!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_wstp.pickle\",\"rb\")\n",
    "tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_full_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
