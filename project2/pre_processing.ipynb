{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Set....\n",
      "Data Set loaded !\n"
     ]
    }
   ],
   "source": [
    "#Load Data Sets\n",
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "print(\"Loading Data Set....\")\n",
    "tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_test = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "tweets_test.drop(columns=[1,2], inplace=True)\n",
    "tweets_test.rename(columns={0:'text'}, inplace= True)\n",
    "\n",
    "    \n",
    "# if not cleaned_test:  \n",
    "#     #read data from .txt\n",
    "#     test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', )\n",
    "#     #index start at 1\n",
    "#     test_data.index = test_data.index +1\n",
    "#     #remove duplicates\n",
    "#     test_data.drop_duplicates(inplace=True)\n",
    "#     test_data.drop(columns=[1,2], inplace=True)\n",
    "#     test_data.rename(columns={0:'text'}, inplace= True)\n",
    "print(\"Data Set loaded !\")\n",
    "\n",
    "\n",
    "#bool variables to clean data\n",
    "#If set to False the cleaning process will take place\n",
    "already_cleaned_neg = False\n",
    "already_cleaned_pos = False\n",
    "already_cleaned_neg_full = True\n",
    "already_cleaned_pos_full = True\n",
    "already_cleaned_test = False\n",
    "\n",
    "#variable to define if saved the lemmatized text or not, if set to false will save column with no lemmatize\n",
    "lemmatize = False\n",
    "\n",
    "\n",
    "# freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "# freq_word = list(freq_word.text)\n",
    "# # freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from .txt\n",
    "test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "#index start at 1\n",
    "test_data.index = test_data.index +1\n",
    "#remove duplicates\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "test_data.drop(columns=[1,2], inplace=True)\n",
    "test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"Function to clean the data.\n",
    "\n",
    "    Args:\n",
    "        data: data to clean\n",
    "        \n",
    "    Returns:\n",
    "        data cleaned\n",
    "    \"\"\"\n",
    "    #increase index to start at 1\n",
    "    data.index = data.index +1\n",
    "    \n",
    "    #remove duplicates\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #apply pre_process funtioncs\n",
    "    print(\"emoji\")\n",
    "    data['text'] = data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    \n",
    "    print(\"hastg!!\")\n",
    "    #remove hashtag #\n",
    "    data['text'] = data['text'].str.replace('#', '')\n",
    "    \n",
    "    print(\"contraction!!\")\n",
    "    #expand contractions don't => do not\n",
    "    data['text'] = data['text'].apply(lambda x: expand_contractions(x))\n",
    "    \n",
    "    print(\"special cases!!\")\n",
    "        #replace ur by your and other special cases\n",
    "    data['text'] = data['text'].str.replace(' ur ', ' your ')\n",
    "    data['text'] = data['text'].str.replace(' u ', ' you ')\n",
    "    data['text'] = data['text'].str.replace(' cant ', ' can not ')\n",
    "    data['text'] = data['text'].str.replace(' yourl ', ' your ')\n",
    "    data['text'] = data['text'].str.replace(' lol ', ' laugh ')\n",
    "    \n",
    "    print(\"numbers!!\")\n",
    "    #separate number with letters  1234test123 =>  test \n",
    "    data['text'] = data['text'].apply(lambda x: split_number_text(x))\n",
    "   \n",
    "    print(\"punctuaction!!\")\n",
    "    #remove punctuaction .........\n",
    "    data['text'] = data['text'].apply(lambda x: remove_punctuation(x))\n",
    "    \n",
    "    print(\"user,url,number!!\")\n",
    "    # remove words user, url, number\n",
    "    data['text'] = data['text'].apply(lambda x: remove_words(x))\n",
    "    \n",
    "    print(\"more letters!\")\n",
    "    # replace more letters haaaaaaaaaappy => haappy\n",
    "    data['text'] = data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    print(\"lemmatizing!\")\n",
    "        #lemmatize words\n",
    "    data['text_lema'] = data['text'].apply(lemmatize_text)\n",
    "    data['text_lema'] = data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "    data['text_lema'] = data['text_lema'].str.replace(',','')\n",
    "    return  data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Neg\n",
      "emoji\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "numbers!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "lemmatizing!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack difficulty of object disassemble and reassemble the wooden pieces this beautiful wo\n",
       "2    glad dot have taks tomorrow thankful startho                                                        \n",
       "3    vs celtics in the regular season were fucked if we play them in the playoffs                        \n",
       "4    could actually kill that girl am so sorry                                                           \n",
       "5    find that very hard to believe im afraid                                                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_neg :\n",
    "    print(\"Cleaning Tweets Neg\")\n",
    "    clean_data(tweets_neg).head()\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_neg_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Pos Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Pos\n",
      "emoji\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "numbers!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "lemmatizing!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    dunno justin read my mention or not only justin and god knows about that but hope you will follow me believe\n",
       "2    because your logic is so dumb will not even crop out your name or your photo tsk                            \n",
       "3    just put casper in box looved the battle crakkbitch                                                         \n",
       "4    thanks sir do not trip lil mama just keep doin ya thang                                                     \n",
       "5    visiting my brother tmr is the bestest birthday gift eveerr                                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_pos :\n",
    "    print(\"Cleaning Tweets Pos\")\n",
    "    clean_data(tweets_pos).head()\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_pos['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_pos_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Neg Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already_cleaned_neg_full :\n",
    "#     print(\"Cleaning Tweets Neg Full\")\n",
    "#     clean_data(tweets_neg_full).head()\n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_full_no_textblob.pickle\",\"wb\")\n",
    "#     if not lemmatize:\n",
    "#         pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "#     else:\n",
    "#         pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     #save file to text\n",
    "#     f = open(\"data/pre_processed/tweets_neg_full_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     if not lemmatize:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "#     else:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Tweets Neg Full Already Cleaned!\")\n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_full_no_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already_cleaned_pos_full :\n",
    "#     print(\"Cleaning Tweets Pos Full\")\n",
    "#     clean_data(tweets_pos_full).head()\n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_full_no_textblob.pickle\",\"wb\")\n",
    "#     if not lemmatize:\n",
    "#         pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "#     else:\n",
    "#         pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     #save file to text\n",
    "#     f = open(\"data/pre_processed/tweets_pos_full_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     if not lemmatize:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "#     else:\n",
    "#         f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Tweets Pos Full Already Cleaned!\")\n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_full_no_textblob.pickle\",\"rb\")\n",
    "# tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_pos_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Test\n",
      "emoji\n",
      "hastg!!\n",
      "contraction!!\n",
      "special cases!!\n",
      "numbers!!\n",
      "punctuaction!!\n",
      "user,url,number!!\n",
      "more letters!\n",
      "lemmatizing!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    sea doo pro sea scooter sports with the portable sea doo seascootersave air stay longer in the water and\n",
       "5    shucks well work all week so now can not come cheer you on oh and put those batteries in your calculator\n",
       "6    can not stay away from bug thats my baby                                                                \n",
       "7    no madam laugh im perfectly fine and not contagious anymore lmao                                        \n",
       "8    whenever fall asleep watching the tv always wake up with headache                                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_test :\n",
    "    tweets_test.index = tweets_test.index +1\n",
    "    #remove duplicates\n",
    "    tweets_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "    print(\"Cleaning Test\")\n",
    "    clean_data(tweets_test).head()\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    #save to pickle\n",
    "    pickle_out = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_test['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_test['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save to txt\n",
    "    f = open(\"data/pre_processed/test_data_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text']))))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text_lema']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Test Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"rb\")\n",
    "tweets_test_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_test_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #verify  flag for data already cleaned\n",
    "# if not cleaned_neg :\n",
    "#     #read data from .txt to preprocess\n",
    "#     tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "    \n",
    "#     #increase index to start at 1\n",
    "#     tweets_neg.index = tweets_neg.index +1\n",
    "#     #remove duplicates\n",
    "#     tweets_neg.drop_duplicates(inplace=True)\n",
    "#     test = tweets_neg.head(50)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "# #     print(\"TEXT BLOB!!\")\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_neg\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# # #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_neg['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_neg_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_no_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_pos:\n",
    "#     #read data .txt file\n",
    "#     tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "#     # index start at 1\n",
    "#     tweets_pos.index = tweets_pos.index + 1\n",
    "#     #remove duplicates\n",
    "#     tweets_pos.drop_duplicates(inplace=True)\n",
    "    \n",
    "#     #apply pre_process funtioncs\n",
    "#     # interpret emoji\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "# #     print(\"TEXT BLOB!!\")\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n",
    "# # #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_pos['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_pos_no_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_no_textblob.pickle\",\"rb\")\n",
    "# tweets_pos_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #verify  flag for data already cleaned\n",
    "# if not cleaned_neg_full :\n",
    "#     #read data from .txt to preprocess\n",
    "#     tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "    \n",
    "#     #increase index to start at 1\n",
    "#     tweets_neg_full.index = tweets_neg_full.index +1\n",
    "#     #remove duplicates\n",
    "#     tweets_neg_full.drop_duplicates(inplace=True)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!\")\n",
    "#     tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_neg_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_full_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_pos_full:\n",
    "#     #read data .txt file\n",
    "#     tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                                   dtype=tweets_dtypes)\n",
    "#     # index start at 1\n",
    "#     tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "#     #remove duplicates\n",
    "#     tweets_pos_full.drop_duplicates(inplace=True)\n",
    "#     #apply pre_process funtioncs\n",
    "#     print(\"emoji\")\n",
    "#     # interpret emoji\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     print(\"numbers!!\")\n",
    "#     #separate number with letters  1234test123 =>  test \n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "#     print(\"hastg!!\")\n",
    "#     #remove hashtag #\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "#     print(\"contraction!!\")\n",
    "#     #expand contractions don't => do not\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "#     print(\"special cases!!\")\n",
    "#         #replace ur by your and other special cases\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' your ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' you ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' cant ', ' can not ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' your ')\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' lol ', ' laugh ')\n",
    "#     print(\"punctuaction!!\")\n",
    "#     #remove punctuaction .........\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "#     print(\"user,url,number!!\")\n",
    "#     # remove words user, url, number\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "#     print(\"more letters!\")\n",
    "#     # replace more letters haaaaaaaaaappy => haappy\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!\")\n",
    "#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_neg\n",
    "    \n",
    "# #     #lemmatize words\n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n",
    "    \n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n",
    "   \n",
    "#     #save the file to pickle\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/tweets_pos_full_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_pos_full_textblob.pickle\",\"rb\")\n",
    "# tweets_neg_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# tweets_neg_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cleaned_test:  \n",
    "#     #read data from .txt\n",
    "#     test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "#     #index start at 1\n",
    "#     test_data.index = test_data.index +1\n",
    "#     #remove duplicates\n",
    "#     test_data.drop_duplicates(inplace=True)\n",
    "#     test_data.drop(columns=[1,2], inplace=True)\n",
    "#     test_data.rename(columns={0:'text'}, inplace= True)\n",
    "    \n",
    "#     #apply pre_process funtioncs\n",
    "#     #remove stop words\n",
    "# #     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#     # interpret emoji\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n",
    "#     #separate number with letters  1234test123 => test \n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n",
    "#     #remove hashtag #\n",
    "#     test_data['text'] = test_data['text'].str.replace('#', '')\n",
    "#     #replace ur by your\n",
    "#     test_data['text'] = test_data['text'].str.replace('ur', 'your')\n",
    "#     #expand contractions don't => do not\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n",
    "#     test_data['text'] = test_data['text'].str.replace(' ur ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' u ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' your ', ' ')\n",
    "#     test_data['text'] = test_data['text'].str.replace(' yourl ', ' ')\n",
    "\n",
    "#             #remove punctuaction\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n",
    " \n",
    "#     # remove words user, url, number\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n",
    "#     # replace more letters haaaaaaaaaapy => haapy\n",
    "#     test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n",
    "\n",
    "#     #correct words with textblob\n",
    "#     print(\"TEXT BLOB!!!\")\n",
    "#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "#     # tweets_pos\n",
    "    \n",
    "\n",
    "        \n",
    "#     #lemmatize words\n",
    "# #     test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n",
    "    \n",
    "    \n",
    "# #     test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "# #     test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n",
    "\n",
    "#     print(\"Saving file with preprocessed Tweets\")\n",
    "#     pickle_out = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"wb\")\n",
    "#     pickle.dump(test_data['text'], pickle_out)\n",
    "#     pickle_out.close()\n",
    "#     f = open(\"data/pre_processed/test_data_textblob.txt\", \"w\", encoding='utf-8')\n",
    "#     f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text']))))\n",
    "#     f.close()\n",
    "#     print(\"Saved!\")\n",
    "# else:\n",
    "#     print(\"Already Trained!\")\n",
    "# pickle_in = open(\"data/pre_processed/test_data_no_textblob.pickle\",\"rb\")\n",
    "# test_data_pickle = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# test_data_pickle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data .txt file\n",
    "# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos_full.index = tweets_pos_full.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos_full.drop_duplicates(inplace=True)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"Interpreting emojis!!\")\n",
    "#  # interpret emoji\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"Separating Numbers!!\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n",
    "# #     print(tweets_pos_full['text'].count())\n",
    "# print(\"Remove Hashtags!!\")\n",
    "# #remove hashtag #\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n",
    "# print(\"Replace ur by your!!\")\n",
    "# #replace ur by your\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions!!\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n",
    "# #remove punctuaction\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words!!\")\n",
    "# # remove words user, url, number\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"Replace more leeters!!\")\n",
    "# # replace more letters haaaaaaaaaappy => haappy\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #read data from .txt to preprocess\n",
    "# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "\n",
    "# #increase index to start at 1\n",
    "# tweets_neg_full.index = tweets_neg_full.index +1\n",
    "# #remove duplicates\n",
    "# tweets_neg_full.drop_duplicates(inplace=True)\n",
    "# test = tweets_neg_full.head(50)\n",
    "# #apply pre_process funtioncs\n",
    "# #remove stop words\n",
    "# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# print(\"emoji\")\n",
    "# # interpret emoji\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n",
    "# print(\"split numbers\")\n",
    "# #separate number with letters  1234test123 =>  test \n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n",
    "# print(\"hastag\")\n",
    "# #remove hashtag #\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n",
    "# print(\"ur your\")\n",
    "# #replace ur by your\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n",
    "# print(\"expand contractions\")\n",
    "# #expand contractions don't => do not\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n",
    "# print(\"punctuaction\")\n",
    "#     #remove punctuaction\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n",
    "# print(\"remove words\")\n",
    "# # remove words user, url, number\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n",
    "# print(\"haaapy to haapy\")\n",
    "# # replace more letters haaaaaaaaaappy => haappy\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n",
    "# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n",
    "\n",
    "# #remove stop words\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n",
    "# combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove most frequent words \n",
    "# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n",
    "# freq_max_total = list(freq_max_total.index)\n",
    "\n",
    "# #remove least frequent words\n",
    "# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n",
    "# freq_min_total = list(freq_min_total.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_min_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words = freq_max_total+ freq_min_total\n",
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the file to pickle\n",
    "# print(\"Saving most frequent and lest frequent words\")\n",
    "# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n",
    "# pickle.dump(freq_words, pickle_out)\n",
    "# pickle_out.close()\n",
    "# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n",
    "# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n",
    "# f.close()\n",
    "# print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n",
    "# xxxxxx = pickle.load(pickle_in)\n",
    "# print(\"Opening pickle\")\n",
    "# xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "#                               dtype=tweets_dtypes)\n",
    "# # index start at 1\n",
    "# tweets_pos.index = tweets_pos.index + 1\n",
    "# #remove duplicates\n",
    "# tweets_pos.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_full_pickle.str.contains('like').sum()\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets_pos.replace(freq_words_20,' ', inplace=True)\n",
    "# tweets_pos.text.str.contains('like').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_pos.text.str.contains(' like ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n",
    "# tweets_neg_freqwords20 = pickle.load(pickle_in)\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n",
    "# tweets_neg_freqwords20.str.contains('yourl').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_lema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dunno justin read my mention or not only justin and god knows about that but hope you will follow me believe</td>\n",
       "      <td>dunno justin read my mention or not only justin and god know about that but hope you will follow me believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>because your logic is so dumb will not even crop out your name or your photo tsk</td>\n",
       "      <td>because your logic is so dumb will not even crop out your name or your photo tsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just put casper in box looved the battle crakkbitch</td>\n",
       "      <td>just put casper in box looved the battle crakkbitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks sir do not trip lil mama just keep doin ya thang</td>\n",
       "      <td>thanks sir do not trip lil mama just keep doin ya thang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>visiting my brother tmr is the bestest birthday gift eveerr</td>\n",
       "      <td>visiting my brother tmr is the bestest birthday gift eveerr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yay lifecompleted tweet facebook me to let me know please</td>\n",
       "      <td>yay lifecompleted tweet facebook me to let me know please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dnextalbumtitle feel for you rollercoaster of life song cocept life yolo becoming famous heart followmeplz heart</td>\n",
       "      <td>dnextalbumtitle feel for you rollercoaster of life song cocept life yolo becoming famous heart followmeplz heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>workin hard or hardly workin rt at hardee's with my future coworker</td>\n",
       "      <td>workin hard or hardly workin rt at hardee's with my future coworker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saw will be replying in bit</td>\n",
       "      <td>saw will be replying in bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>this is were belong</td>\n",
       "      <td>this is were belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>andd to cheer nationals</td>\n",
       "      <td>andd to cheer national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>we send an invitation to shop on line here you will find everything you need without leaving home</td>\n",
       "      <td>we send an invitation to shop on line here you will find everything you need without leaving home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>just woke up finna go to church</td>\n",
       "      <td>just woke up finna go to church</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>agreed more days left tho</td>\n",
       "      <td>agreed more day left tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>monet with katemelo</td>\n",
       "      <td>monet with katemelo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>like damm lexis you got lot to say when your on twitter lol</td>\n",
       "      <td>like damm lexis you got lot to say when your on twitter lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grateful today for dream fulfilled my heart is so full first completed tracks have arrived back from new york yeslord</td>\n",
       "      <td>grateful today for dream fulfilled my heart is so full first completed track have arrived back from new york yeslord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>at home affairs shall do it later</td>\n",
       "      <td>at home affair shall do it later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>barca bout to beat real madrid on saturday doe</td>\n",
       "      <td>barca bout to beat real madrid on saturday doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lot of parts of asia especially rats that live in the country and live on grains supposed to be quite tasty</td>\n",
       "      <td>lot of part of asia especially rat that live in the country and live on grain supposed to be quite tasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>was not even sleeping so shut cho ole go back to sleep lookin ass</td>\n",
       "      <td>wa not even sleeping so shut cho ole go back to sleep lookin as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>have the worlds best dad heart</td>\n",
       "      <td>have the world best dad heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ab jaeyay werna meeting khatam hi hojaeygi baaqi ki buttering baad may karaygay hum sab</td>\n",
       "      <td>ab jaeyay werna meeting khatam hi hojaeygi baaqi ki buttering baad may karaygay hum sab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>no one doubts that ability</td>\n",
       "      <td>no one doubt that ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>check my tweet pic out that was the outfit before this is it after</td>\n",
       "      <td>check my tweet pic out that wa the outfit before this is it after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>just got my mid term and am impressed happy</td>\n",
       "      <td>just got my mid term and am impressed happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>my summer days work from ish home shower eat go out till whenever and do it all over again</td>\n",
       "      <td>my summer day work from ish home shower eat go out till whenever and do it all over again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>laugh noo food is your friend</td>\n",
       "      <td>laugh noo food is your friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>millionbritneyfan rt and tweet</td>\n",
       "      <td>millionbritneyfan rt and tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>but seriously though it is called vanity fairest</td>\n",
       "      <td>but seriously though it is called vanity fairest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>like this song youtube luke james want you</td>\n",
       "      <td>like this song youtube luke james want you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>hey what is good take listen fb brokenhearts</td>\n",
       "      <td>hey what is good take listen fb brokenhearts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>do not apologize you are doing your best nobody can do better than that</td>\n",
       "      <td>do not apologize you are doing your best nobody can do better than that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>texting she got me laughing</td>\n",
       "      <td>texting she got me laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>now am following you white girl</td>\n",
       "      <td>now am following you white girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>woot friday is here it is heating up between two of our entries well done ladies keep voting happy</td>\n",
       "      <td>woot friday is here it is heating up between two of our entry well done lady keep voting happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>what did he do when you grab his arse or did he not care</td>\n",
       "      <td>what did he do when you grab his arse or did he not care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>oh yeah know what ones those are good you made those wow kudos heart</td>\n",
       "      <td>oh yeah know what one those are good you made those wow kudos heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>as long as you dont make this awkward we should be good real good</td>\n",
       "      <td>a long a you dont make this awkward we should be good real good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>just ousted anna as the mayor of anna's house on</td>\n",
       "      <td>just ousted anna a the mayor of anna's house on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>kerp being all negative see what happens</td>\n",
       "      <td>kerp being all negative see what happens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>pro tip for fellas who like busty ladies rub her shoulders she will love ya for it</td>\n",
       "      <td>pro tip for fella who like busty lady rub her shoulder she will love ya for it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>jgn la takut you mest dpt mumtaz punya when is your exam</td>\n",
       "      <td>jgn la takut you mest dpt mumtaz punya when is your exam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>gonna be headed home from this job in half hour teamsleep</td>\n",
       "      <td>gonna be headed home from this job in half hour teamsleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>shouts out to for telling me how it is even if did not like it atleast you told me</td>\n",
       "      <td>shout out to for telling me how it is even if did not like it atleast you told me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>does it make you think of me dancing around like loon painting your house aha</td>\n",
       "      <td>doe it make you think of me dancing around like loon painting your house aha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>love knowing that get to hang out with my friends instead of having practice</td>\n",
       "      <td>love knowing that get to hang out with my friend instead of having practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>will enough fun for you too</td>\n",
       "      <td>will enough fun for you too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>just heard you are out with this tour sad am not overseas to see you and cause some trouble for ya</td>\n",
       "      <td>just heard you are out with this tour sad am not overseas to see you and cause some trouble for ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>aww thanks dj tim xx</td>\n",
       "      <td>aww thanks dj tim xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>may say hate you like billion times but do care you know lot more than what you think im not as cold hearted as you</td>\n",
       "      <td>may say hate you like billion time but do care you know lot more than what you think im not a cold hearted a you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>can be apart of the thumbs club please follow me pretty please with follow on top</td>\n",
       "      <td>can be apart of the thumb club please follow me pretty please with follow on top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>linfield tickets go on sale morrow good deals aswell</td>\n",
       "      <td>linfield ticket go on sale morrow good deal aswell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>coming to visit you</td>\n",
       "      <td>coming to visit you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>it really is date</td>\n",
       "      <td>it really is date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>hey gina what is up</td>\n",
       "      <td>hey gina what is up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>sas and east s plus statxact sas procs pass equivtest nquery is this what you code wit</td>\n",
       "      <td>sa and east s plus statxact sa procs pas equivtest nquery is this what you code wit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>um gord just read your profile am not sure can have lunch with riders fan</td>\n",
       "      <td>um gord just read your profile am not sure can have lunch with rider fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>am so excited for tomorrow look out for two leprechauns xx</td>\n",
       "      <td>am so excited for tomorrow look out for two leprechaun xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>always wondered what the job application is like at hooters do they just give you bra and say here fill this out</td>\n",
       "      <td>always wondered what the job application is like at hooter do they just give you bra and say here fill this out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         text  \\\n",
       "1       dunno justin read my mention or not only justin and god knows about that but hope you will follow me believe            \n",
       "2       because your logic is so dumb will not even crop out your name or your photo tsk                                        \n",
       "3       just put casper in box looved the battle crakkbitch                                                                     \n",
       "4       thanks sir do not trip lil mama just keep doin ya thang                                                                 \n",
       "5       visiting my brother tmr is the bestest birthday gift eveerr                                                             \n",
       "6       yay lifecompleted tweet facebook me to let me know please                                                               \n",
       "7       dnextalbumtitle feel for you rollercoaster of life song cocept life yolo becoming famous heart followmeplz heart        \n",
       "8       workin hard or hardly workin rt at hardee's with my future coworker                                                     \n",
       "9       saw will be replying in bit                                                                                             \n",
       "10      this is were belong                                                                                                     \n",
       "11      andd to cheer nationals                                                                                                 \n",
       "12      we send an invitation to shop on line here you will find everything you need without leaving home                       \n",
       "13      just woke up finna go to church                                                                                         \n",
       "14      agreed more days left tho                                                                                               \n",
       "15      monet with katemelo                                                                                                     \n",
       "16      like damm lexis you got lot to say when your on twitter lol                                                             \n",
       "17      grateful today for dream fulfilled my heart is so full first completed tracks have arrived back from new york yeslord   \n",
       "18      at home affairs shall do it later                                                                                       \n",
       "19      barca bout to beat real madrid on saturday doe                                                                          \n",
       "20      lot of parts of asia especially rats that live in the country and live on grains supposed to be quite tasty             \n",
       "21      was not even sleeping so shut cho ole go back to sleep lookin ass                                                       \n",
       "22      have the worlds best dad heart                                                                                          \n",
       "23      ab jaeyay werna meeting khatam hi hojaeygi baaqi ki buttering baad may karaygay hum sab                                 \n",
       "24      no one doubts that ability                                                                                              \n",
       "25      check my tweet pic out that was the outfit before this is it after                                                      \n",
       "26      just got my mid term and am impressed happy                                                                             \n",
       "27      my summer days work from ish home shower eat go out till whenever and do it all over again                              \n",
       "28      laugh noo food is your friend                                                                                           \n",
       "29      millionbritneyfan rt and tweet                                                                                          \n",
       "30      but seriously though it is called vanity fairest                                                                        \n",
       "...                                                  ...                                                                        \n",
       "99969   like this song youtube luke james want you                                                                              \n",
       "99970   hey what is good take listen fb brokenhearts                                                                            \n",
       "99973   do not apologize you are doing your best nobody can do better than that                                                 \n",
       "99974   texting she got me laughing                                                                                             \n",
       "99975   now am following you white girl                                                                                         \n",
       "99976   woot friday is here it is heating up between two of our entries well done ladies keep voting happy                      \n",
       "99977   what did he do when you grab his arse or did he not care                                                                \n",
       "99978   oh yeah know what ones those are good you made those wow kudos heart                                                    \n",
       "99979   as long as you dont make this awkward we should be good real good                                                       \n",
       "99980   just ousted anna as the mayor of anna's house on                                                                        \n",
       "99981   kerp being all negative see what happens                                                                                \n",
       "99982   pro tip for fellas who like busty ladies rub her shoulders she will love ya for it                                      \n",
       "99983   jgn la takut you mest dpt mumtaz punya when is your exam                                                                \n",
       "99984   gonna be headed home from this job in half hour teamsleep                                                               \n",
       "99985   shouts out to for telling me how it is even if did not like it atleast you told me                                      \n",
       "99986   does it make you think of me dancing around like loon painting your house aha                                           \n",
       "99987   love knowing that get to hang out with my friends instead of having practice                                            \n",
       "99988   will enough fun for you too                                                                                             \n",
       "99989   just heard you are out with this tour sad am not overseas to see you and cause some trouble for ya                      \n",
       "99990   aww thanks dj tim xx                                                                                                    \n",
       "99991   may say hate you like billion times but do care you know lot more than what you think im not as cold hearted as you     \n",
       "99992   can be apart of the thumbs club please follow me pretty please with follow on top                                       \n",
       "99993   linfield tickets go on sale morrow good deals aswell                                                                    \n",
       "99994   coming to visit you                                                                                                     \n",
       "99995   it really is date                                                                                                       \n",
       "99996   hey gina what is up                                                                                                     \n",
       "99997   sas and east s plus statxact sas procs pass equivtest nquery is this what you code wit                                  \n",
       "99998   um gord just read your profile am not sure can have lunch with riders fan                                               \n",
       "99999   am so excited for tomorrow look out for two leprechauns xx                                                              \n",
       "100000  always wondered what the job application is like at hooters do they just give you bra and say here fill this out        \n",
       "\n",
       "                                                                                                                   text_lema  \n",
       "1       dunno justin read my mention or not only justin and god know about that but hope you will follow me believe           \n",
       "2       because your logic is so dumb will not even crop out your name or your photo tsk                                      \n",
       "3       just put casper in box looved the battle crakkbitch                                                                   \n",
       "4       thanks sir do not trip lil mama just keep doin ya thang                                                               \n",
       "5       visiting my brother tmr is the bestest birthday gift eveerr                                                           \n",
       "6       yay lifecompleted tweet facebook me to let me know please                                                             \n",
       "7       dnextalbumtitle feel for you rollercoaster of life song cocept life yolo becoming famous heart followmeplz heart      \n",
       "8       workin hard or hardly workin rt at hardee's with my future coworker                                                   \n",
       "9       saw will be replying in bit                                                                                           \n",
       "10      this is were belong                                                                                                   \n",
       "11      andd to cheer national                                                                                                \n",
       "12      we send an invitation to shop on line here you will find everything you need without leaving home                     \n",
       "13      just woke up finna go to church                                                                                       \n",
       "14      agreed more day left tho                                                                                              \n",
       "15      monet with katemelo                                                                                                   \n",
       "16      like damm lexis you got lot to say when your on twitter lol                                                           \n",
       "17      grateful today for dream fulfilled my heart is so full first completed track have arrived back from new york yeslord  \n",
       "18      at home affair shall do it later                                                                                      \n",
       "19      barca bout to beat real madrid on saturday doe                                                                        \n",
       "20      lot of part of asia especially rat that live in the country and live on grain supposed to be quite tasty              \n",
       "21      wa not even sleeping so shut cho ole go back to sleep lookin as                                                       \n",
       "22      have the world best dad heart                                                                                         \n",
       "23      ab jaeyay werna meeting khatam hi hojaeygi baaqi ki buttering baad may karaygay hum sab                               \n",
       "24      no one doubt that ability                                                                                             \n",
       "25      check my tweet pic out that wa the outfit before this is it after                                                     \n",
       "26      just got my mid term and am impressed happy                                                                           \n",
       "27      my summer day work from ish home shower eat go out till whenever and do it all over again                             \n",
       "28      laugh noo food is your friend                                                                                         \n",
       "29      millionbritneyfan rt and tweet                                                                                        \n",
       "30      but seriously though it is called vanity fairest                                                                      \n",
       "...                                                  ...                                                                      \n",
       "99969   like this song youtube luke james want you                                                                            \n",
       "99970   hey what is good take listen fb brokenhearts                                                                          \n",
       "99973   do not apologize you are doing your best nobody can do better than that                                               \n",
       "99974   texting she got me laughing                                                                                           \n",
       "99975   now am following you white girl                                                                                       \n",
       "99976   woot friday is here it is heating up between two of our entry well done lady keep voting happy                        \n",
       "99977   what did he do when you grab his arse or did he not care                                                              \n",
       "99978   oh yeah know what one those are good you made those wow kudos heart                                                   \n",
       "99979   a long a you dont make this awkward we should be good real good                                                       \n",
       "99980   just ousted anna a the mayor of anna's house on                                                                       \n",
       "99981   kerp being all negative see what happens                                                                              \n",
       "99982   pro tip for fella who like busty lady rub her shoulder she will love ya for it                                        \n",
       "99983   jgn la takut you mest dpt mumtaz punya when is your exam                                                              \n",
       "99984   gonna be headed home from this job in half hour teamsleep                                                             \n",
       "99985   shout out to for telling me how it is even if did not like it atleast you told me                                     \n",
       "99986   doe it make you think of me dancing around like loon painting your house aha                                          \n",
       "99987   love knowing that get to hang out with my friend instead of having practice                                           \n",
       "99988   will enough fun for you too                                                                                           \n",
       "99989   just heard you are out with this tour sad am not overseas to see you and cause some trouble for ya                    \n",
       "99990   aww thanks dj tim xx                                                                                                  \n",
       "99991   may say hate you like billion time but do care you know lot more than what you think im not a cold hearted a you      \n",
       "99992   can be apart of the thumb club please follow me pretty please with follow on top                                      \n",
       "99993   linfield ticket go on sale morrow good deal aswell                                                                    \n",
       "99994   coming to visit you                                                                                                   \n",
       "99995   it really is date                                                                                                     \n",
       "99996   hey gina what is up                                                                                                   \n",
       "99997   sa and east s plus statxact sa procs pas equivtest nquery is this what you code wit                                   \n",
       "99998   um gord just read your profile am not sure can have lunch with rider fan                                              \n",
       "99999   am so excited for tomorrow look out for two leprechaun xx                                                             \n",
       "100000  always wondered what the job application is like at hooter do they just give you bra and say here fill this out       \n",
       "\n",
       "[90233 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
