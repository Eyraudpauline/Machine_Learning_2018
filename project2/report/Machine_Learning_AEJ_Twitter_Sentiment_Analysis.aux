\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{crowdai_01}
\citation{embeddings_01}
\citation{embeddings_02}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Data representation}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Word Embedding}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}N-gram}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Text pre-processing}{1}{section.3}}
\citation{nltk01}
\citation{Textblob01}
\citation{britz_2016}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Baseline - Logistic regression}{2}{section.4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Logarithmic regression - Accuracy in Percent for training on 190'000 tweets , while using 10'000 tweets for validation}}{2}{table.1}}
\newlabel{tab:models}{{I}{2}{Logarithmic regression - Accuracy in Percent for training on 190'000 tweets , while using 10'000 tweets for validation}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Convolutional Neural Nets}{2}{section.5}}
\citation{bentrevett}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Network architecture}{3}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of the simple convolutional net approach.}}{3}{figure.1}}
\newlabel{fig:network_architecture}{{1}{3}{Architecture of the simple convolutional net approach}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Comparison between the performance of the net architectures. Nets are trained on 190'000 tweets for 5 epochs, using batches of 1024 tweets. Validation accuracy is estimated using 10'000 tweets of the data-set.}}{3}{table.2}}
\citation{budhiraja_2016}
\citation{nielsen_a._1970}
\bibstyle{IEEEtran}
\bibdata{literature}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Measures against over-fitting}{4}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of the evolution of Loss and Validation Accuracy. Note: The period represented is not the same for both nets, as training with dropout demands more cycles to reach convergence.}}{4}{figure.2}}
\newlabel{fig:Validation accuracy vs Loss}{{2}{4}{Plot of the evolution of Loss and Validation Accuracy. Note: The period represented is not the same for both nets, as training with dropout demands more cycles to reach convergence}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Results and Summary}{4}{section.6}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Text preprocessing - Accuracy on a validation set of 10'000 tweets, using the N-grams network architecture}}{4}{table.3}}
\newlabel{tab:models_pre_processing}{{III}{4}{Text preprocessing - Accuracy on a validation set of 10'000 tweets, using the N-grams network architecture}{table.3}{}}
