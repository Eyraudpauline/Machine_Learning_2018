{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from custom_helpers import *\n",
    "from plot import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x, y, flag_method, degree, lambda_=0, gamma=1.e-6, max_iters=1000, k_fold=int(5), seed=143225):\n",
    "    \"Train the model and evaluate loss based on cross validation\"\n",
    "    mses_tr = []\n",
    "    mses_te = []\n",
    "    accuracy_tr = []\n",
    "    accuracy_te = []\n",
    "    \n",
    "    flag_add_offset = True\n",
    "    flag_standardize = True\n",
    "    flag_remove_outliers = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed);\n",
    "    for i in range(k_fold):\n",
    "        newk_index = np.delete(k_indices, i, 0)\n",
    "        indices_train = newk_index.ravel()\n",
    "        indices_test = k_indices[i]\n",
    "\n",
    "        # Train data at each iteration \"i\" of the loop\n",
    "        x_train = x[indices_train]\n",
    "        y_train = y[indices_train]\n",
    "\n",
    "        # Validate the data at each iteration \"i\" of the loop\n",
    "        x_test = x[indices_test]\n",
    "        y_test = y[indices_test]\n",
    "\n",
    "        # Prepare data (Standardisation and offset)\n",
    "        training_tx, testing_tx = prepare_data(x_train, x_test, flag_add_offset, flag_standardize, flag_remove_outliers, degree)\n",
    "        \n",
    "        # create initial w for methods using it\n",
    "        initial_w = np.zeros(training_tx.shape[1])\n",
    "\n",
    "        if flag_method == 0:\n",
    "            # Use linear regression (full gradient descent)\n",
    "            weight, _ = least_squares_GD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 1:\n",
    "            # Use linear regression (stochastic gradient descent)\n",
    "            weight, _ = least_squares_SGD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 2:\n",
    "            # Use least squares method\n",
    "            weight, _ = least_squares(y_train, training_tx)\n",
    "            \n",
    "        if flag_method == 3:\n",
    "            # Use ridge regression\n",
    "            weight, _ = ridge_regression(y_train, training_tx, lambda_)\n",
    "            \n",
    "        if flag_method == 4:\n",
    "            # Use logistic regression\n",
    "            weight, _ = logistic_regression(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 5:\n",
    "            # Use regularized logistic regression\n",
    "            weight, _ = reg_logistic_regression(y_train, training_tx, initial_w, max_iters, gamma, lambda_)\n",
    "            \n",
    "        loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "        loss_tr = np.sqrt(2 * compute_mse(y_train, training_tx, weight))\n",
    "        \n",
    "        # Append loss of this round to list\n",
    "        mses_tr.append(loss_tr)\n",
    "        mses_te.append(loss_te)\n",
    "        \n",
    "        # calculate accuracy and add it to list\n",
    "        y_pred_tr = predict_labels(weight, training_tx)\n",
    "        y_pred_te = predict_labels(weight, testing_tx)\n",
    "        accuracy_tr.append(np.sum(y_pred_tr == y_train)/len(y_train))\n",
    "        accuracy_te.append(np.sum(y_pred_te == y_test)/len(y_test))\n",
    "\n",
    "\n",
    "    mean_accuracy_tr = np.mean(accuracy_tr)\n",
    "    mean_accuracy_te = np.mean(accuracy_te)\n",
    "    loss_tr = np.mean(mses_tr)\n",
    "    loss_te = np.mean(mses_te)\n",
    "    return loss_tr, loss_te, mean_accuracy_tr, mean_accuracy_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Main\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "print(\"Loading Data, please wait\")\n",
    "train_y, train_x, ids_train = load_csv_data('data/train.csv')\n",
    "print(\"Data loaded, continue!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods mapping\n",
    "0    Linear regression (full gradient descent)\n",
    "1    Linear regression (stochastic gradient descent)\n",
    "2    Least squares method\n",
    "3    Ridge regression\n",
    "4    Logistic regression (stochastic gradient descent)\n",
    "5    Regularized logistic regression (stochastic gradient descent)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for one set of parameters only\n",
    "Get the RMSE for one method with defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 2;\n",
    "degree = 2\n",
    "lambda_ = 0.001\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.01\n",
    "max_iters = 20\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "_, loss_te, _, accuracy_te = cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "print(\"For the Degree: %d cross-validation loss is %f, and accuracy is %f\" %(degree, loss_te, accuracy_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for methods without regularisation\n",
    "Test polynominal expansion of different degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 4;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 1000\n",
    "\n",
    "# Define range for the polynomial expansion\n",
    "degree_range = np.arange(1, 3)\n",
    "\n",
    "train_losses = np.zeros(len(degree_range))\n",
    "test_losses = np.zeros(len(degree_range))\n",
    "train_accuracies = np.zeros(len(degree_range))\n",
    "test_accuracies = np.zeros(len(degree_range))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, 0, gamma, max_iters)\n",
    "    print(\"For the Degree: %d , The LOSS is : %f\" %(degree, loss_te))\n",
    "    train_losses[ind_degree] = loss_tr\n",
    "    test_losses[ind_degree] = loss_te\n",
    "    test_accuracies[ind_degree] = accuracy_tr\n",
    "    test_accuracies[ind_degree] = accuracy_te\n",
    "    \n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses), test_losses.shape)\n",
    "print(\"The best degrees are: \", degree_range[best_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies)\n",
    "cross_validation_visualization_degree(degree_range, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for Methods using regularisation\n",
    "Grid search over different degrees of polynominal expansion and for different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 5;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 500\n",
    "\n",
    "# Define range for the polynomial expansion and for lambda\n",
    "degree_range = np.arange(1, 2)\n",
    "lambda_range = np.arange(0.0, 10, 3)\n",
    "\n",
    "train_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "train_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain = remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    for ind_lambda_, lambda_ in enumerate(lambda_range):\n",
    "        loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "        print(\"For the Degree: %d and lambda %.2E, The LOSS is : %f\" %(degree, lambda_, loss_te))\n",
    "        train_losses_matrix[ind_degree, ind_lambda_] = loss_tr\n",
    "        test_losses_matrix[ind_degree, ind_lambda_] = loss_te\n",
    "        train_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_tr\n",
    "        test_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_te\n",
    "\n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses_matrix), test_losses_matrix.shape)\n",
    "print(best_value)\n",
    "print(\"Best degree: %d, with lambda %f \" %(degree_range[best_value[0]], lambda_range[best_value[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies_matrix)\n",
    "cross_validation_visualization_lambda(lambda_range, train_losses_matrix[0, :], test_losses_matrix[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
