{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from custom_helpers import *\n",
    "from plot import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x, y, flag_method, degree, lambda_=0, gamma=1.e-6, max_iters=1000, k_fold=int(5), seed=143225):\n",
    "    \"Train the model and evaluate loss based on cross validation\"\n",
    "    mses_tr = []\n",
    "    mses_te = []\n",
    "    accuracy_tr = []\n",
    "    accuracy_te = []\n",
    "    \n",
    "    flag_add_offset = True\n",
    "    flag_standardize = True\n",
    "    flag_remove_outliers = False\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed);\n",
    "    for i in range(k_fold):\n",
    "        newk_index = np.delete(k_indices, i, 0)\n",
    "        indices_train = newk_index.ravel()\n",
    "        indices_test = k_indices[i]\n",
    "\n",
    "        # Train data at each iteration \"i\" of the loop\n",
    "        x_train = x[indices_train]\n",
    "        y_train = y[indices_train]\n",
    "\n",
    "        # Validate the data at each iteration \"i\" of the loop\n",
    "        x_test = x[indices_test]\n",
    "        y_test = y[indices_test]\n",
    "\n",
    "        # Prepare data (Standardisation and offset)\n",
    "        training_tx, testing_tx = prepare_data(x_train, x_test, flag_add_offset, flag_standardize, flag_remove_outliers, degree)\n",
    "        \n",
    "        # create initial w for methods using it\n",
    "        initial_w = np.zeros(training_tx.shape[1])\n",
    "\n",
    "        if flag_method == 0:\n",
    "            # Use linear regression (full gradient descent)\n",
    "            weight, _ = least_squares_GD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 1:\n",
    "            # Use linear regression (stochastic gradient descent)\n",
    "            weight, _ = least_squares_SGD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 2:\n",
    "            # Use least squares method\n",
    "            weight, _ = least_squares(y_train, training_tx)\n",
    "            \n",
    "        if flag_method == 3:\n",
    "            # Use ridge regression\n",
    "            weight, _ = ridge_regression(y_train, training_tx, lambda_)\n",
    "            \n",
    "        if flag_method == 4:\n",
    "            # Use logistic regression\n",
    "            weight, _ = logistic_regression(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            \n",
    "        if flag_method == 5:\n",
    "            # Use regularized logistic regression\n",
    "            weight, _ = reg_logistic_regression(y_train, training_tx, initial_w, max_iters, gamma, lambda_)\n",
    "            \n",
    "        loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "        loss_tr = np.sqrt(2 * compute_mse(y_train, training_tx, weight))\n",
    "        \n",
    "        # Append loss of this round to list\n",
    "        mses_tr.append(loss_tr)\n",
    "        mses_te.append(loss_te)\n",
    "        \n",
    "        # calculate accuracy and add it to list\n",
    "        y_pred_tr = predict_labels(weight, training_tx)\n",
    "        y_pred_te = predict_labels(weight, testing_tx)\n",
    "        accuracy_tr.append(np.sum(y_pred_tr == y_train)/len(y_train))\n",
    "        accuracy_te.append(np.sum(y_pred_te == y_test)/len(y_test))\n",
    "\n",
    "\n",
    "    mean_accuracy_tr = np.mean(accuracy_tr)\n",
    "    mean_accuracy_te = np.mean(accuracy_te)\n",
    "    loss_tr = np.mean(mses_tr)\n",
    "    loss_te = np.mean(mses_te)\n",
    "    return loss_tr, loss_te, mean_accuracy_tr, mean_accuracy_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Main\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, please wait\n",
      "Data loaded, continue!!\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "print(\"Loading Data, please wait\")\n",
    "train_y, train_x, ids_train = load_csv_data('data/train.csv')\n",
    "print(\"Data loaded, continue!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMethods mapping\\n0    Linear regression (full gradient descent)\\n1    Linear regression (stochastic gradient descent)\\n2    Least squares method\\n3    Ridge regression\\n4    Logistic regression (stochastic gradient descent)\\n5    Regularized logistic regression (stochastic gradient descent)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Methods mapping\n",
    "0    Linear regression (full gradient descent)\n",
    "1    Linear regression (stochastic gradient descent)\n",
    "2    Least squares method\n",
    "3    Ridge regression\n",
    "4    Logistic regression (stochastic gradient descent)\n",
    "5    Regularized logistic regression (stochastic gradient descent)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for one set of parameters only\n",
    "Get the RMSE for one method with defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6244300482999456\n",
      "iteration\t 100 \tloss:  0.5947400919348644\n",
      "iteration\t 150 \tloss:  0.5820373274626097\n",
      "iteration\t 200 \tloss:  0.5728809514233614\n",
      "iteration\t 250 \tloss:  0.5658750402261199\n",
      "iteration\t 300 \tloss:  0.5604126331420326\n",
      "iteration\t 350 \tloss:  0.5560955680418571\n",
      "iteration\t 400 \tloss:  0.552643123927952\n",
      "iteration\t 450 \tloss:  0.54985257884781\n",
      "iteration\t 500 \tloss:  0.5475749687839848\n",
      "iteration\t 550 \tloss:  0.5456991863830151\n",
      "iteration\t 600 \tloss:  0.5441412705414723\n",
      "iteration\t 650 \tloss:  0.5428370302379633\n",
      "iteration\t 700 \tloss:  0.5417368710804673\n",
      "iteration\t 750 \tloss:  0.5408021107788042\n",
      "iteration\t 800 \tloss:  0.5400023182520437\n",
      "iteration\t 850 \tloss:  0.5393133667423564\n",
      "iteration\t 900 \tloss:  0.5387159919111021\n",
      "iteration\t 950 \tloss:  0.5381947119067703\n",
      "iteration\t 1000 \tloss:  0.5377370102125572\n",
      "iteration\t 1050 \tloss:  0.5373327115312614\n",
      "iteration\t 1100 \tloss:  0.5369735010272224\n",
      "iteration\t 1150 \tloss:  0.5366525511003193\n",
      "iteration\t 1200 \tloss:  0.5363642295634793\n",
      "iteration\t 1250 \tloss:  0.5361038699663656\n",
      "iteration\t 1300 \tloss:  0.5358675897341617\n",
      "iteration\t 1350 \tloss:  0.5356521453604965\n",
      "iteration\t 1400 \tloss:  0.5354548165069808\n",
      "iteration\t 1450 \tloss:  0.5352733127928082\n",
      "iteration\t 1500 \tloss:  0.5351056984970439\n",
      "iteration\t 1550 \tloss:  0.5349503314775735\n",
      "iteration\t 1600 \tloss:  0.5348058134293653\n",
      "iteration\t 1650 \tloss:  0.5346709492289083\n",
      "iteration\t 1700 \tloss:  0.534544713590842\n",
      "iteration\t 1750 \tloss:  0.5344262236328802\n",
      "iteration\t 1800 \tloss:  0.5343147162326709\n",
      "iteration\t 1850 \tloss:  0.5342095292849194\n",
      "iteration\t 1900 \tloss:  0.5341100861435292\n",
      "iteration\t 1950 \tloss:  0.5340158826727895\n",
      "iteration\t 2000 \tloss:  0.5339264764420711\n",
      "iteration\t 2050 \tloss:  0.5338414776864334\n",
      "iteration\t 2100 \tloss:  0.5337605417258888\n",
      "iteration\t 2150 \tloss:  0.5336833625925279\n",
      "iteration\t 2200 \tloss:  0.5336096676602142\n",
      "iteration\t 2250 \tloss:  0.5335392131083484\n",
      "iteration\t 2300 \tloss:  0.5334717800810385\n",
      "iteration\t 2350 \tloss:  0.5334071714272967\n",
      "iteration\t 2400 \tloss:  0.5333452089277051\n",
      "iteration\t 2450 \tloss:  0.5332857309291963\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6245079087298614\n",
      "iteration\t 100 \tloss:  0.5950044158532545\n",
      "iteration\t 150 \tloss:  0.5824356514361658\n",
      "iteration\t 200 \tloss:  0.5733888099619017\n",
      "iteration\t 250 \tloss:  0.566475757295157\n",
      "iteration\t 300 \tloss:  0.5610924159225454\n",
      "iteration\t 350 \tloss:  0.556842633195852\n",
      "iteration\t 400 \tloss:  0.5534473917493694\n",
      "iteration\t 450 \tloss:  0.5507054623527219\n",
      "iteration\t 500 \tloss:  0.5484691876649656\n",
      "iteration\t 550 \tloss:  0.5466285881611451\n",
      "iteration\t 600 \tloss:  0.5451006596395573\n",
      "iteration\t 650 \tloss:  0.5438220131296574\n",
      "iteration\t 700 \tloss:  0.5427437228828286\n",
      "iteration\t 750 \tloss:  0.541827663783649\n",
      "iteration\t 800 \tloss:  0.5410438701007562\n",
      "iteration\t 850 \tloss:  0.5403686051097962\n",
      "iteration\t 900 \tloss:  0.5397829326804836\n",
      "iteration\t 950 \tloss:  0.5392716482523019\n",
      "iteration\t 1000 \tloss:  0.5388224704825799\n",
      "iteration\t 1050 \tloss:  0.5384254242463169\n",
      "iteration\t 1100 \tloss:  0.5380723656537044\n",
      "iteration\t 1150 \tloss:  0.537756613535804\n",
      "iteration\t 1200 \tloss:  0.5374726614861653\n",
      "iteration\t 1250 \tloss:  0.5372159513703941\n",
      "iteration\t 1300 \tloss:  0.5369826941051898\n",
      "iteration\t 1350 \tloss:  0.5367697270501366\n",
      "iteration\t 1400 \tloss:  0.5365743999470054\n",
      "iteration\t 1450 \tloss:  0.536394483255328\n",
      "iteration\t 1500 \tloss:  0.5362280941589935\n",
      "iteration\t 1550 \tloss:  0.5360736365896884\n",
      "iteration\t 1600 \tloss:  0.5359297524236093\n",
      "iteration\t 1650 \tloss:  0.5357952816257426\n",
      "iteration\t 1700 \tloss:  0.535669229590111\n",
      "iteration\t 1750 \tloss:  0.5355507402904602\n",
      "iteration\t 1800 \tloss:  0.5354390741401958\n",
      "iteration\t 1850 \tloss:  0.5353335896824482\n",
      "iteration\t 1900 \tloss:  0.5352337284054912\n",
      "iteration\t 1950 \tloss:  0.5351390021163\n",
      "iteration\t 2000 \tloss:  0.5350489824140587\n",
      "iteration\t 2050 \tloss:  0.5349632918922349\n",
      "iteration\t 2100 \tloss:  0.534881596767212\n",
      "iteration\t 2150 \tloss:  0.5348036006871556\n",
      "iteration\t 2200 \tloss:  0.5347290395196223\n",
      "iteration\t 2250 \tloss:  0.5346576769526622\n",
      "iteration\t 2300 \tloss:  0.5345893007735422\n",
      "iteration\t 2350 \tloss:  0.5345237197131039\n",
      "iteration\t 2400 \tloss:  0.5344607607632528\n",
      "iteration\t 2450 \tloss:  0.5344002668910058\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6247309791715705\n",
      "iteration\t 100 \tloss:  0.5953271991580401\n",
      "iteration\t 150 \tloss:  0.5828083152621322\n",
      "iteration\t 200 \tloss:  0.5737909752899786\n",
      "iteration\t 250 \tloss:  0.5668935926620068\n",
      "iteration\t 300 \tloss:  0.5615180583398545\n",
      "iteration\t 350 \tloss:  0.5572719664531806\n",
      "iteration\t 400 \tloss:  0.5538782857681408\n",
      "iteration\t 450 \tloss:  0.5511367828624677\n",
      "iteration\t 500 \tloss:  0.5489002988530032\n",
      "iteration\t 550 \tloss:  0.5470591107489071\n",
      "iteration\t 600 \tloss:  0.5455303535497444\n",
      "iteration\t 650 \tloss:  0.544250719431864\n",
      "iteration\t 700 \tloss:  0.5431713336290744\n",
      "iteration\t 750 \tloss:  0.5422541051642394\n",
      "iteration\t 800 \tloss:  0.5414690922669682\n",
      "iteration\t 850 \tloss:  0.5407925755769872\n",
      "iteration\t 900 \tloss:  0.5402056318120629\n",
      "iteration\t 950 \tloss:  0.5396930660449047\n",
      "iteration\t 1000 \tloss:  0.5392426042147987\n",
      "iteration\t 1050 \tloss:  0.5388442767266196\n",
      "iteration\t 1100 \tloss:  0.5384899438976256\n",
      "iteration\t 1150 \tloss:  0.538172927757798\n",
      "iteration\t 1200 \tloss:  0.5378877243260117\n",
      "iteration\t 1250 \tloss:  0.5376297772966521\n",
      "iteration\t 1300 \tloss:  0.5373952989536182\n",
      "iteration\t 1350 \tloss:  0.5371811276657905\n",
      "iteration\t 1400 \tloss:  0.5369846139064589\n",
      "iteration\t 1450 \tloss:  0.5368035286511001\n",
      "iteration\t 1500 \tloss:  0.5366359894324235\n",
      "iteration\t 1550 \tloss:  0.5364804004016043\n",
      "iteration\t 1600 \tloss:  0.5363354035544943\n",
      "iteration\t 1650 \tloss:  0.5361998388988971\n",
      "iteration\t 1700 \tloss:  0.5360727118126959\n",
      "iteration\t 1750 \tloss:  0.5359531662083954\n",
      "iteration\t 1800 \tloss:  0.5358404624037323\n",
      "iteration\t 1850 \tloss:  0.5357339588199125\n",
      "iteration\t 1900 \tloss:  0.5356330968032391\n",
      "iteration\t 1950 \tloss:  0.5355373880033458\n",
      "iteration\t 2000 \tloss:  0.535446403850194\n",
      "iteration\t 2050 \tloss:  0.5353597667587239\n",
      "iteration\t 2100 \tloss:  0.5352771427593752\n",
      "iteration\t 2150 \tloss:  0.5351982353083253\n",
      "iteration\t 2200 \tloss:  0.5351227800760945\n",
      "iteration\t 2250 \tloss:  0.5350505405493724\n",
      "iteration\t 2300 \tloss:  0.5349813043102793\n",
      "iteration\t 2350 \tloss:  0.5349148798811343\n",
      "iteration\t 2400 \tloss:  0.5348510940422764\n",
      "iteration\t 2450 \tloss:  0.5347897895463941\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.6243497146737671\n",
      "iteration\t 100 \tloss:  0.5947045624867114\n",
      "iteration\t 150 \tloss:  0.5820445840832458\n",
      "iteration\t 200 \tloss:  0.5729336914987154\n",
      "iteration\t 250 \tloss:  0.5659726072431465\n",
      "iteration\t 300 \tloss:  0.5605513894467062\n",
      "iteration\t 350 \tloss:  0.5562703629960359\n",
      "iteration\t 400 \tloss:  0.5528483246982817\n",
      "iteration\t 450 \tloss:  0.5500827389162513\n",
      "iteration\t 500 \tloss:  0.5478251909566092\n",
      "iteration\t 550 \tloss:  0.545965243339203\n",
      "iteration\t 600 \tloss:  0.5444195831665245\n",
      "iteration\t 650 \tloss:  0.5431245900711621\n",
      "iteration\t 700 \tloss:  0.5420311536764792\n",
      "iteration\t 750 \tloss:  0.5411009974735513\n",
      "iteration\t 800 \tloss:  0.5403040301725206\n",
      "iteration\t 850 \tloss:  0.5396164102391331\n",
      "iteration\t 900 \tloss:  0.5390191135334925\n",
      "iteration\t 950 \tloss:  0.5384968611113214\n",
      "iteration\t 1000 \tloss:  0.5380373083161213\n",
      "iteration\t 1050 \tloss:  0.5376304257332876\n",
      "iteration\t 1100 \tloss:  0.5372680225780271\n",
      "iteration\t 1150 \tloss:  0.5369433768850069\n",
      "iteration\t 1200 \tloss:  0.5366509465178372\n",
      "iteration\t 1250 \tloss:  0.5363861418535779\n",
      "iteration\t 1300 \tloss:  0.5361451458986014\n",
      "iteration\t 1350 \tloss:  0.5359247711437242\n",
      "iteration\t 1400 \tloss:  0.5357223450659364\n",
      "iteration\t 1450 \tloss:  0.5355356181043402\n",
      "iteration\t 1500 \tloss:  0.5353626893687483\n",
      "iteration\t 1550 \tloss:  0.5352019464142068\n",
      "iteration\t 1600 \tloss:  0.5350520162281683\n",
      "iteration\t 1650 \tloss:  0.5349117251970834\n",
      "iteration\t 1700 \tloss:  0.5347800662949409\n",
      "iteration\t 1750 \tloss:  0.5346561721036295\n",
      "iteration\t 1800 \tloss:  0.5345392925602911\n",
      "iteration\t 1850 \tloss:  0.534428776549642\n",
      "iteration\t 1900 \tloss:  0.5343240566341689\n",
      "iteration\t 1950 \tloss:  0.5342246363530853\n",
      "iteration\t 2000 \tloss:  0.5341300796303284\n",
      "iteration\t 2050 \tloss:  0.5340400019189194\n",
      "iteration\t 2100 \tloss:  0.5339540627786306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 2150 \tloss:  0.5338719596397277\n",
      "iteration\t 2200 \tloss:  0.5337934225505472\n",
      "iteration\t 2250 \tloss:  0.5337182097430049\n",
      "iteration\t 2300 \tloss:  0.5336461038796156\n",
      "iteration\t 2350 \tloss:  0.5335769088695523\n",
      "iteration\t 2400 \tloss:  0.533510447160834\n",
      "iteration\t 2450 \tloss:  0.5334465574317039\n",
      "iteration\t 0 \tloss:  0.013862943611198907\n",
      "iteration\t 50 \tloss:  0.624740661707601\n",
      "iteration\t 100 \tloss:  0.5951854945685351\n",
      "iteration\t 150 \tloss:  0.5825106153583675\n",
      "iteration\t 200 \tloss:  0.5733813222837236\n",
      "iteration\t 250 \tloss:  0.5664062266373993\n",
      "iteration\t 300 \tloss:  0.5609753549397323\n",
      "iteration\t 350 \tloss:  0.5566883366555754\n",
      "iteration\t 400 \tloss:  0.5532633068497474\n",
      "iteration\t 450 \tloss:  0.550497078282443\n",
      "iteration\t 500 \tloss:  0.5482406269812558\n",
      "iteration\t 550 \tloss:  0.5463830133339355\n",
      "iteration\t 600 \tloss:  0.5448405513653972\n",
      "iteration\t 650 \tloss:  0.5435493590801674\n",
      "iteration\t 700 \tloss:  0.5424601463867212\n",
      "iteration\t 750 \tloss:  0.5415345129942449\n",
      "iteration\t 800 \tloss:  0.5407422810084608\n",
      "iteration\t 850 \tloss:  0.5400595469412338\n",
      "iteration\t 900 \tloss:  0.5394672412102735\n",
      "iteration\t 950 \tloss:  0.5389500506742064\n",
      "iteration\t 1000 \tloss:  0.5384956042934446\n",
      "iteration\t 1050 \tloss:  0.538093851821072\n",
      "iteration\t 1100 \tloss:  0.5377365856760026\n",
      "iteration\t 1150 \tloss:  0.5374170701027507\n",
      "iteration\t 1200 \tloss:  0.5371297514691772\n",
      "iteration\t 1250 \tloss:  0.5368700304507515\n",
      "iteration\t 1300 \tloss:  0.5366340817886327\n",
      "iteration\t 1350 \tloss:  0.5364187108841877\n",
      "iteration\t 1400 \tloss:  0.5362212391072225\n",
      "iteration\t 1450 \tloss:  0.5360394116253147\n",
      "iteration\t 1500 \tloss:  0.5358713229989519\n",
      "iteration\t 1550 \tloss:  0.5357153568662966\n",
      "iteration\t 1600 \tloss:  0.5355701368577259\n",
      "iteration\t 1650 \tloss:  0.5354344865023213\n",
      "iteration\t 1700 \tloss:  0.5353073963655899\n",
      "iteration\t 1750 \tloss:  0.535187997025981\n",
      "iteration\t 1800 \tloss:  0.5350755367837219\n",
      "iteration\t 1850 \tloss:  0.5349693632187833\n",
      "iteration\t 1900 \tloss:  0.5348689078900332\n",
      "iteration\t 1950 \tloss:  0.5347736736058756\n",
      "iteration\t 2000 \tloss:  0.534683223806222\n",
      "iteration\t 2050 \tloss:  0.5345971736828262\n",
      "iteration\t 2100 \tloss:  0.5345151827347011\n",
      "iteration\t 2150 \tloss:  0.5344369485112399\n",
      "iteration\t 2200 \tloss:  0.5343622013406809\n",
      "iteration\t 2250 \tloss:  0.5342906998779303\n",
      "iteration\t 2300 \tloss:  0.5342222273352435\n",
      "iteration\t 2350 \tloss:  0.534156588283238\n",
      "iteration\t 2400 \tloss:  0.5340936059292635\n",
      "iteration\t 2450 \tloss:  0.5340331197961321\n",
      "For the Degree: 1 cross-validation loss is 1.234623, and accuracy is 0.731468\n"
     ]
    }
   ],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 4;\n",
    "degree = 1\n",
    "lambda_ = 0\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 2500\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "_, loss_te, _, accuracy_te = cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "print(\"For the Degree: %d cross-validation loss is %f, and accuracy is %f\" %(degree, loss_te, accuracy_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for methods without regularisation\n",
    "Test polynominal expansion of different degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 4;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 1000\n",
    "\n",
    "# Define range for the polynomial expansion\n",
    "degree_range = np.arange(1, 3)\n",
    "\n",
    "train_losses = np.zeros(len(degree_range))\n",
    "test_losses = np.zeros(len(degree_range))\n",
    "train_accuracies = np.zeros(len(degree_range))\n",
    "test_accuracies = np.zeros(len(degree_range))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, 0, gamma, max_iters)\n",
    "    print(\"For the Degree: %d , The LOSS is : %f\" %(degree, loss_te))\n",
    "    train_losses[ind_degree] = loss_tr\n",
    "    test_losses[ind_degree] = loss_te\n",
    "    test_accuracies[ind_degree] = accuracy_tr\n",
    "    test_accuracies[ind_degree] = accuracy_te\n",
    "    \n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses), test_losses.shape)\n",
    "print(\"The best degrees are: \", degree_range[best_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies)\n",
    "cross_validation_visualization_degree(degree_range, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for Methods using regularisation\n",
    "Grid search over different degrees of polynominal expansion and for different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 5;\n",
    "\n",
    "# set Gradient descent parameters\n",
    "gamma = 0.1\n",
    "max_iters = 500\n",
    "\n",
    "# Define range for the polynomial expansion and for lambda\n",
    "degree_range = np.arange(1, 2)\n",
    "lambda_range = np.arange(0.0, 10, 3)\n",
    "\n",
    "train_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_losses_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "train_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "test_accuracies_matrix = np.zeros((len(degree_range), len(lambda_range)))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain = remove_invalid(train_x)\n",
    "\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    for ind_lambda_, lambda_ in enumerate(lambda_range):\n",
    "        loss_tr, loss_te , accuracy_tr, accuracy_te= cross_validation(xtrain, ytrain_cross_validation, flag_method, degree, lambda_, gamma, max_iters)\n",
    "        print(\"For the Degree: %d and lambda %.2E, The LOSS is : %f\" %(degree, lambda_, loss_te))\n",
    "        train_losses_matrix[ind_degree, ind_lambda_] = loss_tr\n",
    "        test_losses_matrix[ind_degree, ind_lambda_] = loss_te\n",
    "        train_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_tr\n",
    "        test_accuracies_matrix[ind_degree, ind_lambda_] = accuracy_te\n",
    "\n",
    "print(\"Cross Validation finished!!\")\n",
    "best_value = np.unravel_index(np.argmin(test_losses_matrix), test_losses_matrix.shape)\n",
    "print(best_value)\n",
    "print(\"Best degree: %d, with lambda %f \" %(degree_range[best_value[0]], lambda_range[best_value[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Test accuracy:\")\n",
    "print(test_accuracies_matrix)\n",
    "cross_validation_visualization_lambda(lambda_range, train_losses_matrix[0, :], test_losses_matrix[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
