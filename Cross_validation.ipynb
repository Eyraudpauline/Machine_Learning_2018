{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from custom_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x, y, flag_method, degree, lambda_=0, k_fold=5, seed=143225, gamma=0.001, max_iters=300):\n",
    "    \"Train the model and evaluate loss based on cross validation\"\n",
    "    mses_tr = []\n",
    "    mses_te = []\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed);\n",
    "    for i in range(k_fold):\n",
    "        newk_index = np.delete(k_indices, i, 0)\n",
    "        indices_train = newk_index.ravel()\n",
    "        indices_test = k_indices[i]\n",
    "\n",
    "        # Train data at each iteration \"i\" of the loop\n",
    "        x_train = x[indices_train]\n",
    "        y_train = y[indices_train]\n",
    "\n",
    "        # Validate the data at each iteration \"i\" of the loop\n",
    "        x_test = x[indices_test]\n",
    "        y_test = y[indices_test]\n",
    "\n",
    "        # Building Polynomial base with degree passed\n",
    "        x_train_poly = build_poly(x_train, degree)\n",
    "        x_test_poly = build_poly(x_test, degree)\n",
    "\n",
    "        # Standardizing data\n",
    "        std_training_x, a, b = standardize(x_train_poly)\n",
    "        std_testing_x, c, d = standardize(x_test_poly)\n",
    "\n",
    "        # Getting matrix tX, adding offset value, entire colum of ones[1]\n",
    "        training_tx = np.c_[np.ones(len(y_train)), std_training_x]\n",
    "        testing_tx = np.c_[np.ones(len(y_test)), std_testing_x]\n",
    "        \n",
    "        initial_w = np.zeros((testing_tx.shape[1], 1))\n",
    "\n",
    "        if flag_method == 0:\n",
    "            # Use linear regression (full gradient descent)\n",
    "            weight, loss_tr = least_squares_GD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "            \n",
    "        if flag_method == 1:\n",
    "            # Use linear regression (stochastic gradient descent)\n",
    "            weight, loss_tr = least_squares_SGD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "            \n",
    "        if flag_method == 2:\n",
    "            # Use least squares method\n",
    "            weight, loss_tr = least_squares(y_train, training_tx)\n",
    "            loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "            \n",
    "        if flag_method == 3:\n",
    "            # Use ridge regression\n",
    "            weight, loss_tr = ridge_regression(y_train, training_tx, lambda_)\n",
    "            loss_te = np.sqrt(2 * compute_mse(y_test, testing_tx, weight))\n",
    "            \n",
    "        if flag_method == 4:\n",
    "            # Use logistic regression\n",
    "            weight, loss_tr = logistic_regression_SGD(y_train, training_tx, initial_w, max_iters, gamma)\n",
    "            loss_te = calculate_log_loss(y_test, testing_tx, weight)\n",
    "            \n",
    "        if flag_method == 5:\n",
    "            # Use regularized logistic regression\n",
    "            weight, loss_tr = reg_logistic_regression_SGD(y_train, training_tx, initial_w, max_iters, gamma, lambda_)\n",
    "            loss_te = calculate_log_loss(y_test, testing_tx, weight)\n",
    "        \n",
    "        # Append loss of this round to list\n",
    "        mses_tr.append(loss_tr)\n",
    "        mses_te.append(loss_te)\n",
    "\n",
    "\n",
    "    loss_tr = np.mean(mses_tr)\n",
    "    loss_te = np.mean(mses_te)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Main\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, please wait\n",
      "Data loaded, continue!!\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "print(\"Loading Data, please wait\")\n",
    "train_y, train_x, ids_train = load_csv_data('data/train.csv')\n",
    "print(\"Data loaded, continue!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods mapping\n",
    "0    Linear regressio (full gradient descent)\n",
    "1    Linear regression (stochastic gradient descent)\n",
    "2    Least squares method\n",
    "3    Ridge regression\n",
    "4    Logistic regression (stochastic gradient descent)\n",
    "5    Regularized logistic regression (stochastic gradient descent)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for methods without regularisation\n",
    "Test polynominal expansion of different degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Degree: 1 , The LOSS is : 0.937235\n",
      "For the Degree: 2 , The LOSS is : 2119.333097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5cfc89cf534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_cross_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For the Degree: %d , The LOSS is : %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_degree\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-663df09860c5>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(x, y, flag_method, degree, lambda_, k_fold, seed, gamma, max_iters)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Building Polynomial base with degree passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx_train_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx_test_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning_2018/custom_helpers.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Chose learnig method to use (see mapping above)\n",
    "flag_method = 2;\n",
    "\n",
    "# Define range for the polynomial expansion\n",
    "degree_range = np.arange(1, 7)\n",
    "train_losses = np.zeros(len(degree_range))\n",
    "test_losses = np.zeros(len(degree_range))\n",
    "\n",
    "# Preparing data for cross validation\n",
    "ytrain_cross_validation = train_y.copy()\n",
    "xtrain=remove_invalid(train_x)\n",
    "for ind_degree, degree in enumerate(degree_range):\n",
    "    loss_tr, loss_te = cross_validation(xtrain, ytrain_cross_validation, flag_method, degree)\n",
    "    print(\"For the Degree: %d , The LOSS is : %f\" %(degree, loss_te))\n",
    "    train_losses[ind_degree] = loss_tr\n",
    "    test_losses[ind_degree] = loss_te\n",
    "print(\"The Least Square Cross Validation finished!!\")\n",
    "test_losses_abs = np.absolute(test_losses)\n",
    "best_value = np.unravel_index(np.argmin(test_losses_abs), test_losses.shape)\n",
    "print(\"The best degrees are: \", degree_range[best_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for Methods using regularisation\n",
    "Grid search over different degrees of polynominal expansion and for different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_cross_validation_lambdas(ytrain, xtrain, degree,k_fold):\n",
    "    seed = 1\n",
    "    lambda_range = np.arange(-0.005, 0.1, 0.005)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(ytrain, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation: DONE\n",
    "    for lambda_ in lambda_range:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for index in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge_regression(ytrain, xtrain, k_indices, index, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        print(\"For the Lambda: %f , The LOSS is : %f\" %(lambda_,loss_te))\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    # ***************************************************\n",
    "    print(\"The Ridge Regression Cross Validation finished!!\")\n",
    "    #cross_validation_visualization(lambda_range, train_losses, test_losses)\n",
    "    rmse_te_abs = np.absolute(rmse_te)\n",
    "    best_value = np.unravel_index(np.argmin(rmse_te_abs), rmse_te_abs.shape)\n",
    "    print(\"Best lambda is :\", lambda_range[best_value])\n",
    "    return lambda_range[best_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: Implement plots for grid search (lambdas & poly degrees)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_cross_validation_lambdas_plot(ytrain, xtrain, degree,k_fold):\n",
    "    seed = 1\n",
    "    lambda_range = np.arange(-0.005, 0.1, 0.005)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(ytrain, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation: DONE\n",
    "    for lambda_ in lambda_range:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for index in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge_regression(ytrain, xtrain, k_indices, index, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        print(\"For the Lambda: %f , The LOSS is : %f\" %(lambda_,loss_te))\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "    # ***************************************************\n",
    "    print(\"The Ridge Regression Cross Validation finished!!\")\n",
    "    #cross_validation_visualization(lambda_range, train_losses, test_losses)\n",
    "    rmse_te_abs = np.absolute(rmse_te)\n",
    "    best_value = np.unravel_index(np.argmin(rmse_te_abs), rmse_te_abs.shape)\n",
    "    print(\"Best lambda is :\", lambda_range[best_value])\n",
    "    return lambda_range[best_value], rmse_tr, rmse_te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
