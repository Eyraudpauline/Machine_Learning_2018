{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from methods_implementation import *\n",
    "from cross_validation import *\n",
    "from custom_helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, please wait\n",
      "Data loaded, continue!!\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "print(\"Loading Data, please wait\")\n",
    "testing_y, testing_x, ids_test = load_csv_data('data/test.csv')\n",
    "training_y, training_x, ids_train = load_csv_data('data/train.csv')\n",
    "print(\"Data loaded, continue!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  UNCOMENT THE FOLLOWING CODE TO TEST LEAST SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the dateset, we found that the Column[22] PRI_jet_num dataset is categorical with Four categories defined.column_jet_nb = 22\n",
    "# pred_y = []\n",
    "# ids_pred_y = []\n",
    "# column_categorical=22\n",
    "# # In the paper where describes the differente features of the data, explains that different columns are invalid values\n",
    "# # depending on the value of the categorical feature, so we can delete those values for the 4 different trainings\n",
    "\n",
    "# #The undefined features, with first vector for the categorical value of 0, and so on.\n",
    "# undefined_features = [[4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29], [4, 5, 6, 12, 22, 26, 27, 28], [22], [22]]\n",
    "\n",
    "# #We will have a for loop with 4 values for the 4 categorical training\n",
    "# print(\"Starting the categorical training!!!!\\n\")\n",
    "# for nb_jets in range(0, 4):\n",
    "#     print(\"\\nCLEANING DATA STAGE!!!!!!!!!!!!!\\n\")\n",
    "#     print(\"The training for categorical value %d\" %nb_jets)\n",
    "#     # We will separate the data according to the value of the categorical values for each loop in our cicle\n",
    "#     test_x = testing_x[testing_x[:, column_categorical] == nb_jets]\n",
    "#     test_y = testing_y[testing_x[:, column_categorical] == nb_jets]\n",
    "#     id_test = ids_test[testing_x[:, column_categorical] == nb_jets]\n",
    "#     train_x = training_x[training_x[:, column_categorical] == nb_jets]\n",
    "#     train_y = training_y[training_x[:, column_categorical] == nb_jets]\n",
    "#     id_train = ids_train[training_x[:, column_categorical] == nb_jets]\n",
    "#     #Now we will remove the undefined features depending on the categorical value\n",
    "#     print(\"Removing undefined features for categorical value\", nb_jets)\n",
    "#     test_x = np.delete(test_x, undefined_features[nb_jets], axis=1)\n",
    "#     train_x = np.delete(train_x, undefined_features[nb_jets], axis=1)\n",
    "#     #Now we will remove the unvalid value of -999 by replacing the value with the mean of the column\n",
    "#     print(\"Removing the invalid value -999\")\n",
    "#     train_x = remove_invalid(train_x)\n",
    "#     test_x = remove_invalid(test_x)\n",
    "#     #Now we will replace the ourliers with the most common element of each column\n",
    "#     print(\"Replacing Outliers.\")\n",
    "#     train_x = remove_outliers(train_x)\n",
    "#     test_x = remove_outliers(test_x)\n",
    "#     #Now we will build the polynomial vectors to fit the data\n",
    "#     print(\"DATA CLEAN!!!!!\\n\")\n",
    "#     print(\"Building Polynomial Vectors with the best_degrees found\")\n",
    "#     degree = 1\n",
    "#     training_poly_x = build_poly(train_x, degree)\n",
    "#     testing_poly_x = build_poly(test_x, degree)\n",
    "#     #Then we need to standarize the data, first the training_x and then the\n",
    "#     # testing_x with the values(mean_train,std_train) found before\n",
    "#     print(\"Standarizing the data\")\n",
    "#     std_training_x, mean_train, std_train = standardize(training_poly_x)\n",
    "#     std_testing_x = standardize_test(testing_poly_x, mean_train, std_train)\n",
    "#     #Now we will add the offset column of ones [1]\n",
    "#     print(\"Adding vector of ones to offset\")\n",
    "#     training_tx = np.c_[np.ones(len(train_y)), std_training_x]\n",
    "#     testing_tx = np.c_[np.ones(len(test_y)), std_testing_x]\n",
    "    \n",
    "#     #We will divide the data in 5 to make cross validation 50000 each kfold\n",
    "#     print(\"Finding the best parameters with cross validation\")\n",
    "#     k_fold = 5\n",
    "#     best_degrees=least_squares_cross_validation_degree(training_y,training_x,k_fold)\n",
    "  \n",
    "    \n",
    "#     print(\"Now we are ready to start training !!\")\n",
    "#     # We will run least squares method\n",
    "#     print(\"Optimization with Least squares and best degree=1\")\n",
    "#     # Getting the weights with the method Least Squares\n",
    "#     weight_tr_LS, loss_tr_LS = least_squares(train_y, training_tx)\n",
    "\n",
    "#     print(\"Training finalized!! now we will prepare the predictions\")\n",
    "#     # Now we get the prediction for y\n",
    "#     testing_y1 = predict_labels(weight_tr_LS, testing_tx)\n",
    "#     pred_y.extend(testing_y1)\n",
    "#     ids_pred_y.extend(id_test)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Done!\")\n",
    "# print(\"Creating submission file.\")\n",
    "# create_csv_submission(ids_pred_y, pred_y, \"AEJ_submit_lest_squares.csv\")\n",
    "# print(\"Created submission file!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  UNCOMENT THE FOLLOWING CODE TO TEST RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the categorical training!!!!\n",
      "\n",
      "\n",
      "CLEANING DATA STAGE!!!!!!!!!!!!!\n",
      "\n",
      "The training for categorical value 0\n",
      "Removing undefined features for categorical value 0\n",
      "Removing the invalid value -999\n",
      "Replacing Outliers.\n",
      "DATA CLEAN!!!!!\n",
      "\n",
      "Building Polynomial Vectors with the best_degrees found\n",
      "Standarizing the data\n",
      "Adding vector of ones to offset\n",
      "Finding the best parameters with cross validation\n",
      "Now we are ready to start training !!\n",
      "Optimization with Ridge Regression and best lambda_\n",
      "Training finalized!! now we will prepare the predictions\n",
      "\n",
      "CLEANING DATA STAGE!!!!!!!!!!!!!\n",
      "\n",
      "The training for categorical value 1\n",
      "Removing undefined features for categorical value 1\n",
      "Removing the invalid value -999\n",
      "Replacing Outliers.\n",
      "DATA CLEAN!!!!!\n",
      "\n",
      "Building Polynomial Vectors with the best_degrees found\n",
      "Standarizing the data\n",
      "Adding vector of ones to offset\n",
      "Finding the best parameters with cross validation\n",
      "Now we are ready to start training !!\n",
      "Optimization with Ridge Regression and best lambda_\n",
      "Training finalized!! now we will prepare the predictions\n",
      "\n",
      "CLEANING DATA STAGE!!!!!!!!!!!!!\n",
      "\n",
      "The training for categorical value 2\n",
      "Removing undefined features for categorical value 2\n",
      "Removing the invalid value -999\n",
      "Replacing Outliers.\n",
      "DATA CLEAN!!!!!\n",
      "\n",
      "Building Polynomial Vectors with the best_degrees found\n",
      "Standarizing the data\n",
      "Adding vector of ones to offset\n",
      "Finding the best parameters with cross validation\n",
      "Now we are ready to start training !!\n",
      "Optimization with Ridge Regression and best lambda_\n",
      "Training finalized!! now we will prepare the predictions\n",
      "\n",
      "CLEANING DATA STAGE!!!!!!!!!!!!!\n",
      "\n",
      "The training for categorical value 3\n",
      "Removing undefined features for categorical value 3\n",
      "Removing the invalid value -999\n",
      "Replacing Outliers.\n",
      "DATA CLEAN!!!!!\n",
      "\n",
      "Building Polynomial Vectors with the best_degrees found\n",
      "Standarizing the data\n",
      "Adding vector of ones to offset\n",
      "Finding the best parameters with cross validation\n",
      "Now we are ready to start training !!\n",
      "Optimization with Ridge Regression and best lambda_\n",
      "Training finalized!! now we will prepare the predictions\n"
     ]
    }
   ],
   "source": [
    "# In the dateset, we found that the Column[22] PRI_jet_num dataset is categorical with Four categories defined.column_jet_nb = 22\n",
    "pred_y = []\n",
    "ids_pred_y = []\n",
    "column_categorical=22\n",
    "# In the paper where describes the differente features of the data, explains that different columns are invalid values\n",
    "# depending on the value of the categorical feature, so we can delete those values for the 4 different trainings\n",
    "\n",
    "#The undefined features, with first vector for the categorical value of 0, and so on.\n",
    "undefined_features = [[4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29], [4, 5, 6, 12, 22, 26, 27, 28], [22], [22]]\n",
    "\n",
    "#We will have a for loop with 4 values for the 4 categorical training\n",
    "print(\"Starting the categorical training!!!!\\n\")\n",
    "for nb_jets in range(0, 4):\n",
    "    print(\"\\nCLEANING DATA STAGE!!!!!!!!!!!!!\\n\")\n",
    "    print(\"The training for categorical value %d\" %nb_jets)\n",
    "    # We will separate the data according to the value of the categorical values for each loop in our cicle\n",
    "    test_x = testing_x[testing_x[:, column_categorical] == nb_jets]\n",
    "    test_y = testing_y[testing_x[:, column_categorical] == nb_jets]\n",
    "    id_test = ids_test[testing_x[:, column_categorical] == nb_jets]\n",
    "    train_x = training_x[training_x[:, column_categorical] == nb_jets]\n",
    "    train_y = training_y[training_x[:, column_categorical] == nb_jets]\n",
    "    id_train = ids_train[training_x[:, column_categorical] == nb_jets]\n",
    "    #Now we will remove the undefined features depending on the categorical value\n",
    "    print(\"Removing undefined features for categorical value\", nb_jets)\n",
    "    test_x = np.delete(test_x, undefined_features[nb_jets], axis=1)\n",
    "    train_x = np.delete(train_x, undefined_features[nb_jets], axis=1)\n",
    "    #Now we will remove the unvalid value of -999 by replacing the value with the mean of the column\n",
    "    print(\"Removing the invalid value -999\")\n",
    "    train_x = remove_invalid(train_x)\n",
    "    test_x = remove_invalid(test_x)\n",
    "    #Now we will replace the ourliers with the most common element of each column\n",
    "    print(\"Replacing Outliers.\")\n",
    "    train_x = remove_outliers(train_x)\n",
    "    test_x = remove_outliers(test_x)\n",
    "    #Now we will build the polynomial vectors to fit the data\n",
    "    print(\"DATA CLEAN!!!!!\\n\")\n",
    "    print(\"Building Polynomial Vectors with the best_degrees found\")\n",
    "    degree = 2\n",
    "    training_poly_x = build_poly(train_x, degree)\n",
    "    testing_poly_x = build_poly(test_x, degree)\n",
    "    #Then we need to standarize the data, first the training_x and then the\n",
    "    # testing_x with the values(mean_train,std_train) found before\n",
    "    print(\"Standarizing the data\")\n",
    "    std_training_x, mean_train, std_train = standardize(training_poly_x)\n",
    "    std_testing_x = standardize_test(testing_poly_x, mean_train, std_train)\n",
    "    #Now we will add the offset column of ones [1]\n",
    "    print(\"Adding vector of ones to offset\")\n",
    "    training_tx = np.c_[np.ones(len(train_y)), std_training_x]\n",
    "    testing_tx = np.c_[np.ones(len(test_y)), std_testing_x]\n",
    "    \n",
    "    #We will divide the data in 5 to make cross validation 50000 each kfold\n",
    "    print(\"Finding the best parameters with cross validation\")\n",
    "    k_fold = 5\n",
    "#     best_lambda=ridge_regression_cross_validation_lambdas(training_y,training_x,degree,k_fold)  \n",
    "    print(\"Now we are ready to start training !!\")\n",
    "    # We will run RIDGRE REGRESSION\n",
    "    print(\"Optimization with Ridge Regression and best lambda_\")\n",
    "    # Getting the weights with the method Ridge Regression\n",
    "    lambda_=0.005\n",
    "    weight_tr_rr, loss_tr_rr = ridge_regression(train_y, training_tx,lambda_)\n",
    "\n",
    "    print(\"Training finalized!! now we will prepare the predictions\")\n",
    "    # Now we get the prediction for y\n",
    "    testing_y1 = predict_labels(weight_tr_rr, testing_tx)\n",
    "    pred_y.extend(testing_y1)\n",
    "    ids_pred_y.extend(id_test)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the categorical training!!!!\n",
      "\n",
      "\n",
      "CLEANING DATA STAGE!!!!!!!!!!!!!\n",
      "\n",
      "The training for categorical value 0\n",
      "Removing undefined features for categorical value 0\n",
      "Removing the invalid value -999\n",
      "Replacing Outliers.\n",
      "DATA CLEAN!!!!!\n",
      "\n",
      "Building Polynomial Vectors with the best_degrees found\n",
      "Standarizing the data\n",
      "Adding vector of ones to offset\n",
      "Optimization with Logistic regression\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-88445148e810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mweight_tr_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finalized!! now we will prepare the predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning_2018/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning_2018/implementations.py\u001b[0m in \u001b[0;36mcalculate_log_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In the dateset, we found that the Column[22] PRI_jet_num dataset is categorical with Four categories defined.column_jet_nb = 22\n",
    "pred_y = []\n",
    "ids_pred_y = []\n",
    "column_categorical=22\n",
    "# In the paper where describes the differente features of the data, explains that different columns are invalid values\n",
    "# depending on the value of the categorical feature, so we can delete those values for the 4 different trainings\n",
    "\n",
    "#The undefined features, with first vector for the categorical value of 0, and so on.\n",
    "undefined_features = [[4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29], [4, 5, 6, 12, 22, 26, 27, 28], [22], [22]]\n",
    "\n",
    "#We will have a for loop with 4 values for the 4 categorical training\n",
    "print(\"Starting the categorical training!!!!\\n\")\n",
    "for nb_jets in range(0, 4):\n",
    "    print(\"\\nCLEANING DATA STAGE!!!!!!!!!!!!!\\n\")\n",
    "    print(\"The training for categorical value %d\" %nb_jets)\n",
    "    # We will separate the data according to the value of the categorical values for each loop in our cicle\n",
    "    test_x = testing_x[testing_x[:, column_categorical] == nb_jets]\n",
    "    test_y = testing_y[testing_x[:, column_categorical] == nb_jets]\n",
    "    id_test = ids_test[testing_x[:, column_categorical] == nb_jets]\n",
    "    train_x = training_x[training_x[:, column_categorical] == nb_jets]\n",
    "    train_y = training_y[training_x[:, column_categorical] == nb_jets]\n",
    "    id_train = ids_train[training_x[:, column_categorical] == nb_jets]\n",
    "    #Now we will remove the undefined features depending on the categorical value\n",
    "    print(\"Removing undefined features for categorical value\", nb_jets)\n",
    "    test_x = np.delete(test_x, undefined_features[nb_jets], axis=1)\n",
    "    train_x = np.delete(train_x, undefined_features[nb_jets], axis=1)\n",
    "    #Now we will remove the unvalid value of -999 by replacing the value with the mean of the column\n",
    "    print(\"Removing the invalid value -999\")\n",
    "    train_x = remove_invalid(train_x)\n",
    "    test_x = remove_invalid(test_x)\n",
    "    #Now we will replace the ourliers with the most common element of each column\n",
    "    print(\"Replacing Outliers.\")\n",
    "    train_x = remove_outliers(train_x)\n",
    "    test_x = remove_outliers(test_x)\n",
    "    #Now we will build the polynomial vectors to fit the data\n",
    "    print(\"DATA CLEAN!!!!!\\n\")\n",
    "    print(\"Building Polynomial Vectors with the best_degrees found\")\n",
    "    degree = 2\n",
    "    training_poly_x = build_poly(train_x, degree)\n",
    "    testing_poly_x = build_poly(test_x, degree)\n",
    "    #Then we need to standarize the data, first the training_x and then the\n",
    "    # testing_x with the values(mean_train,std_train) found before\n",
    "    print(\"Standarizing the data\")\n",
    "    std_training_x, mean_train, std_train = standardize(training_poly_x)\n",
    "    std_testing_x = standardize_test(testing_poly_x, mean_train, std_train)\n",
    "    #Now we will add the offset column of ones [1]\n",
    "    print(\"Adding vector of ones to offset\")\n",
    "    training_tx = np.c_[np.ones(len(train_y)), std_training_x]\n",
    "    testing_tx = np.c_[np.ones(len(test_y)), std_testing_x]\n",
    "    \n",
    "    print(\"Optimization with Logistic regression\")\n",
    "    # Getting the weights with the method Logistic regression\n",
    "    max_iters = 100000\n",
    "    gamma = 0.01\n",
    "    initial_w = np.zeros((training_tx.shape[1], 1))\n",
    "    weight_tr_lr, loss_tr_lr = logistic_regression(train_y, training_tx, initial_w, max_iters, gamma)\n",
    "\n",
    "    print(\"Training finalized!! now we will prepare the predictions\")\n",
    "    # Now we get the prediction for y\n",
    "    testing_y1 = predict_labels(weight_tr_lr, testing_tx)\n",
    "    pred_y.extend(testing_y1)\n",
    "    ids_pred_y.extend(id_test)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Creating submission file.\n",
      "Created submission file!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")\n",
    "print(\"Creating submission file.\")\n",
    "create_csv_submission(ids_pred_y, pred_y, \"AEJ_submit_ridge_regression.csv\")\n",
    "print(\"Created submission file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
